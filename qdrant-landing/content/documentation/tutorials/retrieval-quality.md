---
title: Measure retrieval quality
weight: 21
---

# Measure retrieval quality

| Time: 30 min | Level: Intermediate |  |    |
|--------------|---------------------|--|----|

In this tutorial, you will:

1. Load and prepare a dataset of arXiv titles with pre-computed embeddings.
2. Create a collection in Qdrant and index the training data.
3. Implement a function to measure retrieval quality using precision@k.
4. Compare the results of ANN search with exact search.
5. Tune HNSW parameters to improve retrieval quality.
6. Analyze the results and explore the trade-offs between precision and performance.

By the end of this tutorial, you'll have a practical understanding of how to measure and improve retrieval quality in Qdrant.

### Step 0: Understanding the Impact of Search Algorithms on Your Retrieval Quality

Semantic search pipelines rely heavily on the quality of the embeddings they use. If your model cannot properly represent input data, similar objects might be far apart in the vector space, leading to poor search results. Another critical component that affects search quality is the Approximate Nearest Neighbor (ANN) algorithm itself.

Qdrant harnesses the power of ANN algor "ithms to handle massive datasets efficiently. As your dataset grows, exact k-Nearest Neighbors (kNN) search starts to suffer in performance due to the increased computational time required to compute distances between the query and every point in the dataset. This makes exact kNN impractical for large-scale applications. ANN algorithms like HNSW provide a significant advantage by approximating the nearest neighbors, allowing you to perform fast and scalable searches over large volumes of data.

While ANN algorithms are faster, they may return approximate results, which can impact search quality. It's important to measure the retrieval quality of the ANN algorithm compared to the kNN algorithm to ensure it approximates the exact search effectively.


### Step 1: Load and Prepare the Dataset

First, we need to load a dataset and prepare it for evaluation. We'll use the `Qdrant/arxiv-titles-instructorxl-embeddings` dataset from the Hugging Face Hub.

```python
# Install required libraries
pip install qdrant-client datasets

from datasets import load_dataset

# Load dataset
dataset = load_dataset(
    "Qdrant/arxiv-titles-instructorxl-embeddings", split="train", streaming=True
)

# Split data into training and testing sets
dataset_iterator = iter(dataset)
train_dataset = [next(dataset_iterator) for _ in range(50000)]
test_dataset = [next(dataset_iterator) for _ in range(5000)]
```

This code loads the dataset and splits it into two parts: 50,000 items for training and 5,000 items for testing. This separation will allow us to build the search index with training data and evaluate its quality using the test data.

### Step 2: Create a Collection in Qdrant

Next, we need to create a collection in Qdrant to store the vectors.

```python
from qdrant_client import QdrantClient, models

# Set up Qdrant client
client = QdrantClient("http://localhost:6333")

# Create a collection in Qdrant
client.create_collection(
    collection_name="arxiv-titles-instructorxl-embeddings",
    vectors_config=models.VectorParams(
        size=768,  # Size of the embeddings generated by the InstructorXL model
        distance=models.Distance.COSINE,
    ),
)
```

This code creates a collection in Qdrant. The `size` parameter represents the dimensionality of the embedding vectors, and the `distance` parameter specifies the distance function to use (e.g., cosine). Using the correct distance function is important for achieving good retrieval quality.

### Step 3: Index the Training Data

Once the collection is set up, we need to upload the training data into Qdrant for indexing.

```python
# Upload training data to Qdrant
client.upload_points(  # Available as of qdrant-client v1.7.1
    collection_name="arxiv-titles-instructorxl-embeddings",
    points=[
        models.PointStruct(
            id=item["id"],
            vector=item["vector"],
            payload=item,
        )
        for item in train_dataset
    ]
)

# Wait for indexing to complete
while True:
    collection_info = client.get_collection(collection_name="arxiv-titles-instructorxl-embeddings")
    if collection_info.status == models.CollectionStatus.GREEN:
        break
```

This code uploads the training dataset to Qdrant, creating an index that can be used for approximate nearest neighbor (ANN) searches. The status check ensures that indexing is complete before moving forward.

### Step 4: Measure Retrieval Quality

To assess retrieval quality, we need to compare the results of an ANN search with the results of an exact search. This comparison will help determine how well the ANN algorithm approximates the true nearest neighbors.

The function `avg_precision_at_k(k)` calculates the average precision at k by comparing the ANN results against the exact search results for each item in the test dataset. Precision at k measures how many of the top k results from ANN match the exact search results.

```python
def avg_precision_at_k(k: int):
    precisions = []
    for item in test_dataset:
        # Perform ANN search
        ann_result = client.query_points(
            collection_name="arxiv-titles-instructorxl-embeddings",
            query=item["vector"],
            limit=k,
        ).points
    
        # Perform exact search
        knn_result = client.query_points(
            collection_name="arxiv-titles-instructorxl-embeddings",
            query=item["vector"],
            limit=k,
            search_params=models.SearchParams(
                exact=True,  # Enables exact search mode
            ),
        ).points

        # Calculate precision@k
        ann_ids = set(point.id for point in ann_result)
        knn_ids = set(point.id for point in knn_result)
        precision = len(ann_ids.intersection(knn_ids)) / k
        precisions.append(precision)
    
    return sum(precisions) / len(precisions)
```

### Step 5: Measure Retrieval Quality

```python
print(f"Initial avg(precision@5) = {avg_precision_at_k(k=5)}")
```

This step measures the initial retrieval quality before any tuning of the HNSW(Hierarchical Navigable Small World) parameters. HNSW is a graph-based indexing algorithm that builds a multi-layer navigation structure. The upper layers are sparse with nodes far apart, while lower layers are denser with closer nodes. The search starts from the top layer, finds the closest node to the target, then moves down to denser layers, iteratively approaching the target position.

In order to improve performance, HNSW limits the maximum degree of nodes on each layer of the graph to m. There are two types of parameters that can be tuned:

Index-time parameters:
- `m`: This parameter limits the maximum number of connections (degree) per node in each layer of the HNSW graph. A higher value for `m` allows more connections between nodes, potentially improving search accuracy but requiring more memory and indexing time. The default value for `m` is 16.
- `ef_construct`: This parameter defines the search range during index construction. A higher value of `ef_construct` means a wider search range during indexing, resulting in a higher quality graph structure. However, this increases the indexing time. The default value for `ef_construct` is 100.

Search-time parameter:
- `ef`: (also known as `efSearch`): This parameter controls the search range when looking for nearest neighbors during queries. A higher value of `ef` widens the search range, increasing the likelihood of finding true nearest neighbors at the cost of higher search latency. The default value depends on the value of `ef_construct`.

We'll use the default m and ef_construct as a baseline and then tweak the parameters to see how it affects the precision of the search. Later, we will double these values and observe their impact on retrieval quality.

Qdrant allows us to easily compare the performance between exact and approximate searches. For smaller datasets (e.g., up to 20,000 documents), exact search can be practical, but as the dataset scales, ANN algorithms like HNSW become necessary to handle the increased data volume efficiently.

### Step 6: Tune HNSW Parameters for Better Precision

The HNSW algorithm used in Qdrant can be fine-tuned for better precision by adjusting parameters like `m` and `ef_construct`.

```python
# Tune HNSW parameters for improved precision
client.update_collection(
    collection_name="arxiv-titles-instructorxl-embeddings",
    hnsw_config=models.HnswConfigDiff(
        m=32,          # Increase from the default 16 to 32
        ef_construct=200,  # Increase from the default 100 to 200
    )
)

# Wait for re-indexing to complete
while True:
    collection_info = client.get_collection(collection_name="arxiv-titles-instructorxl-embeddings")
    if collection_info.status == models.CollectionStatus.GREEN:
        break

# Measure new precision
print(f"New avg(precision@5) = {avg_precision_at_k(k=5)}")
```

By increasing `m` and `ef_construct`, you're allowing for a more connected graph and a more exhaustive search during indexing, which should lead to higher precision. After tuning, it's important to measure the precision again to verify improvements.

### How to Select HNSW Parameters
- If you require higher precision, increase `m` and `ef_construct` while considering the increased memory usage and indexing time.
- If memory and indexing time are critical constraints, tune the parameters incrementally to find the right balance.
- Consider adjusting `ef` (also known as `efSearch`), which controls the number of neighbors evaluated during the search. A higher value may increase precision but also increases latency.

### Measuring Search Quality in the WebUI

If you prefer a visual interface, you can also measure search quality in the Qdrant WebUI. To do this, first go to [Qdrant Cloud](https://cloud.qdrant.io/) and select a collection. Then, navigate to the Search Quality tab. 


![Search Quality Report in Qdrant WebUI](/articles_data/retrieval-quality/user_clean.png)


From here you can run a simple search Quality Report on your collection. Qdrant will select a random sample of 100 points for you and run an exact search and an ANN search. It will then compare the results and calculate the precision@10.

![Search Quality Report in Qdrant WebUI](/articles_data/retrieval-quality/clean_precision_ran.png)

You can also run a more comprehensive Search Quality Report by clicking on the `Advanced Mod` button. This will open an interface where you can specify parameters like ef and m and evaluate the Search Quality in the webui.

![Search Quality Report in Qdrant WebUI](/articles_data/retrieval-quality/clean_advanced.png)

### Step 7: Analyze Results and Conclusion

Finally, analyze the results:

- Compare the initial and new precision scores to determine the impact of parameter changes.
- Assess whether the improvements justify the increased resource usage.

By following these steps, you can effectively measure and improve the retrieval quality of your Qdrant-based semantic search system. Qdrant's ANN capabilities provide the ability to efficiently handle massive datasets while maintaining acceptable levels of accuracy.

### Additional Considerations

- **Embeddings Quality Is Crucial**: While tuning ANN parameters can improve retrieval quality, the quality of the embeddings is a major factor.
- **Exact Search as Baseline**: Use exact search as a baseline to compare with ANN to separate the impact of embedding quality from the search methods.
- **Monitor Performance**: Continuously monitor retrieval quality and adjust parameters accordingly to optimize both performance and accuracy.

By leveraging Qdrant's ANN capabilities, you can build a robust and scalable semantic search system that delivers high-quality results even at the scale of billions of vectors.
