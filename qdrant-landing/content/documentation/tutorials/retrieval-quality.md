---
title: Measure retrieval quality
weight: 21
---

# Measure retrieval quality

| Time: 30 min | Level: Intermediate |  |    |
|--------------|---------------------|--|----|

In this tutorial, you will:

1. Load and prepare a dataset of arXiv titles with pre-computed embeddings.
2. Create a collection in Qdrant and index the training data.
3. Implement a function to measure retrieval quality using precision@k.
4. Compare the results of ANN search with exact search.
5. Tune HNSW parameters to improve retrieval quality.
6. Analyze the results and explore the trade-offs between precision and performance.

By the end of this tutorial, you'll have a practical understanding of how to measure and improve retrieval quality in Qdrant.

### Step 0: Understanding the Impact of Search Algorithms on Your Retrieval Quality

Semantic search pipelines rely heavily on the quality of the embeddings they use. If your model cannot properly represent input data, similar objects might be far apart in the vector space, leading to poor search results. Another critical component that affects search quality is the Approximate Nearest Neighbor (ANN) algorithm itself.

Qdrant harnesses the power of ANN algorithms to handle massive datasets efficiently. As your dataset grows, exact k-Nearest Neighbors (kNN) search starts to suffer in performance due to the increased computational time required to compute distances between the query and every point in the dataset. This makes exact kNN impractical for large-scale applications. ANN algorithms like HNSW provide a significant advantage by approximating the nearest neighbors, allowing you to perform fast and scalable searches over large volumes of data.

While ANN algorithms are faster, they may return approximate results, which can impact search quality. It's important to measure the retrieval quality of the ANN algorithm compared to the kNN algorithm to ensure it approximates the exact search effectively.


### Step 1: Load and Prepare the Dataset

First, we need to load a dataset and prepare it for evaluation. We'll use the `Qdrant/arxiv-titles-instructorxl-embeddings` dataset from the Hugging Face Hub.

```python
# Install required libraries
pip install qdrant-client datasets

from datasets import load_dataset

# Load dataset
dataset = load_dataset(
    "Qdrant/arxiv-titles-instructorxl-embeddings", split="train", streaming=True
)

# Split data into training and testing sets
dataset_iterator = iter(dataset)
train_dataset = [next(dataset_iterator) for _ in range(50000)]
test_dataset = [next(dataset_iterator) for _ in range(5000)]
```

This code loads the dataset and splits it into two parts: 50,000 items for training and 5,000 items for testing. This separation will allow us to build the search index with training data and evaluate its quality using the test data.

### Step 2: Create a Collection in Qdrant

Next, we need to create a collection in Qdrant to store the vectors.

```python
from qdrant_client import QdrantClient, models

# Set up Qdrant client
client = QdrantClient("http://localhost:6333")

# Create a collection in Qdrant
client.create_collection(
    collection_name="arxiv-titles-instructorxl-embeddings",
    vectors_config=models.VectorParams(
        size=768,  # Size of the embeddings generated by the InstructorXL model
        distance=models.Distance.COSINE,
    ),
)
```

This code creates a collection in Qdrant. The `size` parameter represents the dimensionality of the embedding vectors, and the `distance` parameter specifies the distance function to use (e.g., cosine). Using the correct distance function is important for achieving good retrieval quality.

### Step 3: Index the Training Data

Once the collection is set up, we need to upload the training data into Qdrant for indexing.

```python
# Upload training data to Qdrant
client.upload_points(  # Available as of qdrant-client v1.7.1
    collection_name="arxiv-titles-instructorxl-embeddings",
    points=[
        models.PointStruct(
            id=item["id"],
            vector=item["vector"],
            payload=item,
        )
        for item in train_dataset
    ]
)

# Wait for indexing to complete
while True:
    collection_info = client.get_collection(collection_name="arxiv-titles-instructorxl-embeddings")
    if collection_info.status == models.CollectionStatus.GREEN:
        break
```

This code uploads the training dataset to Qdrant, creating an index that can be used for approximate nearest neighbor (ANN) searches. The status check ensures that indexing is complete before moving forward.

### Step 4: Measure Retrieval Quality

To assess retrieval quality, we need to compare the results of an ANN search with the results of an exact search. This comparison will help determine how well the ANN algorithm approximates the true nearest neighbors.

The function `avg_precision_at_k(k)` calculates the average precision at k by comparing the ANN results against the exact search results for each item in the test dataset. Precision at k measures how many of the top k results from ANN match the exact search results.

```python
def avg_precision_at_k(k: int):
    precisions = []
    for item in test_dataset:
        # Perform ANN search
        ann_result = client.query_points(
            collection_name="arxiv-titles-instructorxl-embeddings",
            query=item["vector"],
            limit=k,
        ).points
    
        # Perform exact search
        knn_result = client.query_points(
            collection_name="arxiv-titles-instructorxl-embeddings",
            query=item["vector"],
            limit=k,
            search_params=models.SearchParams(
                exact=True,  # Enables exact search mode
            ),
        ).points

        # Calculate precision@k
        ann_ids = set(point.id for point in ann_result)
        knn_ids = set(point.id for point in knn_result)
        precision = len(ann_ids.intersection(knn_ids)) / k
        precisions.append(precision)
    
    return sum(precisions) / len(precisions)
```

### Step 5: Measure Retrieval Quality

```python
print(f"Initial avg(precision@5) = {avg_precision_at_k(k=5)}")
```

This step measures the initial retrieval quality before any tuning of the HNSW parameters. The HNSW (Hierarchical Navigable Small World) algorithm has two key parameters that influence search performance and quality:

- **m**: This parameter determines the maximum number of connections per node in the HNSW graph. A higher value for `m` increases the connectivity of the graph, potentially improving search accuracy at the cost of increased memory usage and indexing time. The default value for `m` is 16.
- **ef_construct**: This parameter controls the size of the dynamic candidate list during index construction. A higher value of `ef_construct` leads to a more exhaustive search during the indexing phase, resulting in a higher quality graph and improved search accuracy. However, this comes at the cost of longer indexing times. The default value for `ef_construct` is 100.

We will use the untuned HNSW as the baseline to compare how changes affect the precision of the search. Initially, we will use the default values of `m` (16) and `ef_construct` (100) for the HNSW algorithm. Later, we will double these values to observe their impact on retrieval quality.

Qdrant allows us to easily compare the performance between exact and approximate searches. For smaller datasets (e.g., up to 20,000 documents), exact search can be practical, but as the dataset scales, ANN algorithms like HNSW become necessary to handle the increased data volume efficiently.

### Step 6: Tune HNSW Parameters for Better Precision

The HNSW algorithm used in Qdrant can be fine-tuned for better precision by adjusting parameters like `m` and `ef_construct`.

```python
# Tune HNSW parameters for improved precision
client.update_collection(
    collection_name="arxiv-titles-instructorxl-embeddings",
    hnsw_config=models.HnswConfigDiff(
        m=32,          # Increase from the default 16 to 32
        ef_construct=200,  # Increase from the default 100 to 200
    )
)

# Wait for re-indexing to complete
while True:
    collection_info = client.get_collection(collection_name="arxiv-titles-instructorxl-embeddings")
    if collection_info.status == models.CollectionStatus.GREEN:
        break

# Measure new precision
print(f"New avg(precision@5) = {avg_precision_at_k(k=5)}")
```

By increasing `m` and `ef_construct`, you're allowing for a more connected graph and a more exhaustive search during indexing, which should lead to higher precision. After tuning, it's important to measure the precision again to verify improvements.

### How to Select HNSW Parameters

- If you require higher precision, increase `m` and `ef_construct` while considering the increased memory usage and indexing time.
- If memory and indexing time are critical constraints, tune the parameters incrementally to find the right balance.

### Measuring Search Quality in the WebUI

If you prefer a visual interface, you can also measure search quality in the Qdrant WebUI. To do this, first go to [Qdrant Cloud](https://cloud.qdrant.io/) and select a collection. Then, navigate to the Search Quality tab. 


![Search Quality Report in Qdrant WebUI](/img/retrieval-quality/user_clean.png)


From here you can run a simple search Quality Report on your collection. Qdrant will select a random sample of 100 points for you and run an exact search and an ANN search. It will then compare the results and calculate the precision@10.

![Search Quality Report in Qdrant WebUI](/img/retrieval-quality/clean_precision_ran.png)

You can also run a more comprehensive Search Quality Report by clicking on the `Advanced Mod` button. This will open an interface where you can specify parameters like ef and m and evaluate the Search Quality in the webui.

![Search Quality Report in Qdrant WebUI](/img/retrieval-quality/clean_advanced.png)

### Step 7: Analyze Results and Conclusion

Finally, analyze the results:

- Compare the initial and new precision scores to determine the impact of parameter changes.
- Assess whether the improvements justify the increased resource usage.

By following these steps, you can effectively measure and improve the retrieval quality of your Qdrant-based semantic search system. Qdrant's ANN capabilities provide the ability to efficiently handle massive datasets while maintaining acceptable levels of accuracy.

### Additional Considerations

- **Embeddings Quality Is Crucial**: While tuning ANN parameters can improve retrieval quality, the quality of the embeddings is a major factor.
- **Exact Search as Baseline**: Use exact search as a baseline to compare with ANN to separate the impact of embedding quality from the search methods.
- **Monitor Performance**: Continuously monitor retrieval quality and adjust parameters accordingly to optimize both performance and accuracy.

By leveraging Qdrant's ANN capabilities, you can build a robust and scalable semantic search system that delivers high-quality results even at the scale of billions of vectors.
