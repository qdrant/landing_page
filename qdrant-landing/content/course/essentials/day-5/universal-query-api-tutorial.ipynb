{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Universal Query API Tutorial\n",
        "\n",
        "Learn to build hybrid search systems using Qdrant's Universal Query API with real data from Hugging Face. We'll demonstrate 6 different retrieval patterns using FastEmbed's dense, sparse, and ColBERT models.\n",
        "\n",
        "## What You'll Learn\n",
        "\n",
        "The Universal Query API enables complex search pipelines in a single request. Traditional approaches require multiple API calls and client-side result merging. This tutorial shows you how to build sophisticated search systems with declarative queries.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "%pip install qdrant-client fastembed datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from qdrant_client import QdrantClient, models\n",
        "from fastembed import TextEmbedding, SparseTextEmbedding, LateInteractionTextEmbedding\n",
        "from datasets import load_dataset\n",
        "\n",
        "print(\"Dependencies loaded\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Setup Qdrant Connection\n",
        "\n",
        "Configure your Qdrant Cloud credentials.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure Qdrant Cloud\n",
        "QDRANT_URL = \"https://your-cluster-url.cloud.qdrant.io\"\n",
        "QDRANT_API_KEY = \"your-api-key\"\n",
        "\n",
        "client = QdrantClient(url=QDRANT_URL, api_key=QDRANT_API_KEY)\n",
        "\n",
        "# Test connection\n",
        "try:\n",
        "    collections = client.get_collections()\n",
        "    print(f\"Connected to Qdrant. Collections: {len(collections.collections)}\")\n",
        "except Exception as e:\n",
        "    print(f\"Connection failed: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Load Dataset and Initialize Models\n",
        "\n",
        "Load SQuAD dataset and initialize FastEmbed models for dense, sparse, and ColBERT embeddings.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load SQuAD dataset (100 examples)\n",
        "dataset = load_dataset(\"squad\", split=\"validation[:100]\")\n",
        "print(f\"Loaded {len(dataset)} examples\")\n",
        "\n",
        "# Initialize FastEmbed models\n",
        "dense_model = TextEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
        "sparse_model = SparseTextEmbedding(model_name=\"Qdrant/bm25\")\n",
        "colbert_model = LateInteractionTextEmbedding(model_name=\"jinaai/jina-colbert-v2\")\n",
        "print(\"FastEmbed models loaded: BGE + BM25 + Jina-ColBERT-v2\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Create Collection\n",
        "\n",
        "Set up a multi-vector collection supporting dense, sparse, and ColBERT vectors.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "collection_name = \"squad_universal_query\"\n",
        "\n",
        "# Remove existing collection\n",
        "try:\n",
        "    client.delete_collection(collection_name)\n",
        "    print(\"Deleted existing collection\")\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# Create collection with all vector types\n",
        "client.create_collection(\n",
        "    collection_name=collection_name,\n",
        "    vectors_config={\n",
        "        \"dense\": models.VectorParams(size=384, distance=models.Distance.COSINE),\n",
        "        \"colbert\": models.VectorParams(\n",
        "            size=128, \n",
        "            distance=models.Distance.COSINE,\n",
        "            multivector_config=models.MultiVectorConfig(\n",
        "                comparator=models.MultiVectorComparator.MAX_SIM\n",
        "            )\n",
        "        )\n",
        "    },\n",
        "    sparse_vectors_config={\n",
        "        \"sparse\": models.SparseVectorParams()\n",
        "    }\n",
        ")\n",
        "\n",
        "print(f\"Created collection: {collection_name}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Generate Embeddings and Upload Data\n",
        "\n",
        "Process the dataset and generate all three vector types using FastEmbed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare texts for batch processing\n",
        "texts = []\n",
        "for item in dataset:\n",
        "    text = f\"{item['title']} {item['context']}\"\n",
        "    texts.append(text)\n",
        "\n",
        "print(\"Generating embeddings...\")\n",
        "\n",
        "# Generate all embeddings in batch\n",
        "dense_embeddings = list(dense_model.embed(texts))\n",
        "sparse_embeddings = list(sparse_model.embed(texts))\n",
        "colbert_embeddings = list(colbert_model.embed(texts))\n",
        "\n",
        "# Prepare points for upload\n",
        "points = []\n",
        "for i, item in enumerate(dataset):\n",
        "    # Convert sparse embedding to dict format\n",
        "    sparse_vector = {\n",
        "        int(idx): float(value) \n",
        "        for idx, value in zip(sparse_embeddings[i].indices, sparse_embeddings[i].values)\n",
        "    }\n",
        "    \n",
        "    point = models.PointStruct(\n",
        "        id=i,\n",
        "        vector={\n",
        "            \"dense\": dense_embeddings[i],\n",
        "            \"colbert\": colbert_embeddings[i]\n",
        "        },\n",
        "        sparse_vector={\n",
        "            \"sparse\": models.SparseVector(\n",
        "                indices=list(sparse_vector.keys()),\n",
        "                values=list(sparse_vector.values())\n",
        "            )\n",
        "        },\n",
        "        payload={\n",
        "            \"title\": item['title'],\n",
        "            \"context\": item['context'][:500],\n",
        "            \"question\": item['question']\n",
        "        }\n",
        "    )\n",
        "    points.append(point)\n",
        "\n",
        "# Upload to Qdrant\n",
        "client.upsert(collection_name=collection_name, points=points)\n",
        "print(f\"Uploaded {len(points)} points with dense, sparse, and ColBERT vectors\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Helper Function for Query Processing\n",
        "\n",
        "Create a helper to generate query vectors for all patterns.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_query_vectors(query_text):\n",
        "    \"\"\"Generate all query vector types\"\"\"\n",
        "    dense = list(dense_model.embed([query_text]))[0]\n",
        "    \n",
        "    sparse_embedding = list(sparse_model.embed([query_text]))[0]\n",
        "    sparse = {\n",
        "        int(idx): float(value) \n",
        "        for idx, value in zip(sparse_embedding.indices, sparse_embedding.values)\n",
        "    }\n",
        "    \n",
        "    colbert = list(colbert_model.embed([query_text]))[0]\n",
        "    \n",
        "    return dense, sparse, colbert\n",
        "\n",
        "# Test query\n",
        "query = \"What is the capital of France?\"\n",
        "query_dense, query_sparse, query_colbert = generate_query_vectors(query)\n",
        "print(f\"Query: '{query}'\")\n",
        "print(f\"Vectors: Dense({len(query_dense)}), Sparse({len(query_sparse)}), ColBERT({len(query_colbert)} tokens)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Pattern 1: Hybrid Search with RRF\n",
        "\n",
        "Combine dense and sparse search using Reciprocal Rank Fusion.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=== Pattern 1: Dense + Sparse Hybrid (RRF) ===\")\n",
        "\n",
        "hybrid_results = client.query_points(\n",
        "    collection_name=collection_name,\n",
        "    prefetch=[\n",
        "        models.Prefetch(query=query_dense, using=\"dense\", limit=20),\n",
        "        models.Prefetch(\n",
        "            query=models.SparseVector(\n",
        "                indices=list(query_sparse.keys()),\n",
        "                values=list(query_sparse.values())\n",
        "            ),\n",
        "            using=\"sparse\", limit=20\n",
        "        )\n",
        "    ],\n",
        "    query=models.FusionQuery(fusion=models.Fusion.RRF),\n",
        "    limit=3\n",
        ")\n",
        "\n",
        "for i, result in enumerate(hybrid_results.points, 1):\n",
        "    print(f\"{i}. {result.payload['title']} (score: {result.score:.3f})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Pattern 2: Dense Recall + ColBERT Reranking\n",
        "\n",
        "Use dense search for broad recall, then ColBERT for precision reranking.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n=== Pattern 2: Dense Recall + ColBERT Reranking ===\")\n",
        "\n",
        "rerank_results = client.query_points(\n",
        "    collection_name=collection_name,\n",
        "    prefetch=[\n",
        "        models.Prefetch(query=query_dense, using=\"dense\", limit=50)\n",
        "    ],\n",
        "    query=query_colbert,\n",
        "    using=\"colbert\",\n",
        "    limit=3\n",
        ")\n",
        "\n",
        "for i, result in enumerate(rerank_results.points, 1):\n",
        "    print(f\"{i}. {result.payload['title']} (score: {result.score:.3f})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Pattern 3: Triple Vector Hybrid\n",
        "\n",
        "Combine all three vector types (dense + sparse + ColBERT) with RRF fusion.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n=== Pattern 3: Triple Vector Hybrid (RRF) ===\")\n",
        "\n",
        "triple_results = client.query_points(\n",
        "    collection_name=collection_name,\n",
        "    prefetch=[\n",
        "        models.Prefetch(query=query_dense, using=\"dense\", limit=15),\n",
        "        models.Prefetch(\n",
        "            query=models.SparseVector(\n",
        "                indices=list(query_sparse.keys()),\n",
        "                values=list(query_sparse.values())\n",
        "            ),\n",
        "            using=\"sparse\", limit=15\n",
        "        ),\n",
        "        models.Prefetch(query=query_colbert, using=\"colbert\", limit=15)\n",
        "    ],\n",
        "    query=models.FusionQuery(fusion=models.Fusion.RRF),\n",
        "    limit=3\n",
        ")\n",
        "\n",
        "for i, result in enumerate(triple_results.points, 1):\n",
        "    print(f\"{i}. {result.payload['title']} (score: {result.score:.3f})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Pattern 4: DBSF Fusion\n",
        "\n",
        "Use Distribution-Based Score Fusion instead of RRF for different score normalization.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n=== Pattern 4: DBSF Fusion ===\")\n",
        "\n",
        "dbsf_results = client.query_points(\n",
        "    collection_name=collection_name,\n",
        "    prefetch=[\n",
        "        models.Prefetch(query=query_dense, using=\"dense\", limit=20),\n",
        "        models.Prefetch(\n",
        "            query=models.SparseVector(\n",
        "                indices=list(query_sparse.keys()),\n",
        "                values=list(query_sparse.values())\n",
        "            ),\n",
        "            using=\"sparse\", limit=20\n",
        "        )\n",
        "    ],\n",
        "    query=models.FusionQuery(fusion=models.Fusion.DBSF),\n",
        "    limit=3\n",
        ")\n",
        "\n",
        "for i, result in enumerate(dbsf_results.points, 1):\n",
        "    print(f\"{i}. {result.payload['title']} (score: {result.score:.3f})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Pattern 5: Multi-Stage Filtering\n",
        "\n",
        "Apply early filters during prefetch and late filters after fusion.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n=== Pattern 5: Multi-Stage Filtering ===\")\n",
        "\n",
        "# Early filter: Basic constraints during prefetch\n",
        "early_filter = models.Filter(\n",
        "    must=[models.FieldCondition(key=\"title\", match=models.MatchText(text=\"University\"))]\n",
        ")\n",
        "\n",
        "# Late filter: Final constraints after fusion\n",
        "late_filter = models.Filter(\n",
        "    must=[models.FieldCondition(key=\"title\", match=models.MatchText(text=\"of\"))]\n",
        ")\n",
        "\n",
        "filtered_results = client.query_points(\n",
        "    collection_name=collection_name,\n",
        "    prefetch=[\n",
        "        models.Prefetch(query=query_dense, using=\"dense\", limit=50, filter=early_filter),\n",
        "        models.Prefetch(\n",
        "            query=models.SparseVector(\n",
        "                indices=list(query_sparse.keys()),\n",
        "                values=list(query_sparse.values())\n",
        "            ),\n",
        "            using=\"sparse\", limit=50, filter=early_filter\n",
        "        )\n",
        "    ],\n",
        "    query=models.FusionQuery(fusion=models.Fusion.RRF),\n",
        "    filter=late_filter,\n",
        "    limit=3\n",
        ")\n",
        "\n",
        "for i, result in enumerate(filtered_results.points, 1):\n",
        "    print(f\"{i}. {result.payload['title']} (score: {result.score:.3f})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Pattern 6: Complex Pipeline\n",
        "\n",
        "Multiple prefetches with different strategies and final ColBERT reranking.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n=== Pattern 6: Complex Pipeline ===\")\n",
        "\n",
        "complex_results = client.query_points(\n",
        "    collection_name=collection_name,\n",
        "    prefetch=[\n",
        "        # High recall dense search\n",
        "        models.Prefetch(query=query_dense, using=\"dense\", limit=50),\n",
        "        \n",
        "        # Filtered sparse search\n",
        "        models.Prefetch(\n",
        "            query=models.SparseVector(\n",
        "                indices=list(query_sparse.keys()),\n",
        "                values=list(query_sparse.values())\n",
        "            ),\n",
        "            using=\"sparse\", \n",
        "            limit=30,\n",
        "            filter=early_filter\n",
        "        ),\n",
        "        \n",
        "        # ColBERT search with filter\n",
        "        models.Prefetch(query=query_colbert, using=\"colbert\", limit=20, filter=early_filter)\n",
        "    ],\n",
        "    # Final ColBERT reranking\n",
        "    query=query_colbert,\n",
        "    using=\"colbert\",\n",
        "    limit=3\n",
        ")\n",
        "\n",
        "for i, result in enumerate(complex_results.points, 1):\n",
        "    print(f\"{i}. {result.payload['title']} (score: {result.score:.3f})\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Compare All Patterns\n",
        "\n",
        "Test different patterns on the same query to see performance differences.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_query = \"What is machine learning?\"\n",
        "print(f\"\\nComparing patterns for: '{test_query}'\")\n",
        "\n",
        "q_dense, q_sparse, q_colbert = generate_query_vectors(test_query)\n",
        "\n",
        "patterns = {\n",
        "    \"Dense + Sparse (RRF)\": lambda: client.query_points(\n",
        "        collection_name=collection_name,\n",
        "        prefetch=[\n",
        "            models.Prefetch(query=q_dense, using=\"dense\", limit=20),\n",
        "            models.Prefetch(query=models.SparseVector(\n",
        "                indices=list(q_sparse.keys()), values=list(q_sparse.values())\n",
        "            ), using=\"sparse\", limit=20)\n",
        "        ],\n",
        "        query=models.FusionQuery(fusion=models.Fusion.RRF), limit=2\n",
        "    ),\n",
        "    \n",
        "    \"Triple Vector (RRF)\": lambda: client.query_points(\n",
        "        collection_name=collection_name,\n",
        "        prefetch=[\n",
        "            models.Prefetch(query=q_dense, using=\"dense\", limit=15),\n",
        "            models.Prefetch(query=models.SparseVector(\n",
        "                indices=list(q_sparse.keys()), values=list(q_sparse.values())\n",
        "            ), using=\"sparse\", limit=15),\n",
        "            models.Prefetch(query=q_colbert, using=\"colbert\", limit=15)\n",
        "        ],\n",
        "        query=models.FusionQuery(fusion=models.Fusion.RRF), limit=2\n",
        "    ),\n",
        "    \n",
        "    \"Dense → ColBERT\": lambda: client.query_points(\n",
        "        collection_name=collection_name,\n",
        "        prefetch=[models.Prefetch(query=q_dense, using=\"dense\", limit=50)],\n",
        "        query=q_colbert, using=\"colbert\", limit=2\n",
        "    ),\n",
        "    \n",
        "    \"ColBERT Only\": lambda: client.query_points(\n",
        "        collection_name=collection_name, query=q_colbert, using=\"colbert\", limit=2\n",
        "    )\n",
        "}\n",
        "\n",
        "for pattern_name, pattern_func in patterns.items():\n",
        "    print(f\"\\n{pattern_name}:\")\n",
        "    results = pattern_func()\n",
        "    for i, result in enumerate(results.points, 1):\n",
        "        print(f\"  {i}. {result.payload['title']} ({result.score:.3f})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Summary\n",
        "\n",
        "You've explored 6 Universal Query API patterns using native FastEmbed models:\n",
        "\n",
        "**Vector Types:**\n",
        "- **Dense**: BAAI/bge-small-en-v1.5 (384-dim)\n",
        "- **Sparse**: Qdrant/bm25 (native BM25)\n",
        "- **ColBERT**: jinaai/jina-colbert-v2 (128-dim late interaction)\n",
        "\n",
        "**Patterns Demonstrated:**\n",
        "1. **Hybrid RRF** - Dense + sparse fusion\n",
        "2. **Dense → ColBERT** - Broad recall + precision reranking\n",
        "3. **Triple Vector** - All three types with RRF\n",
        "4. **DBSF Fusion** - Alternative fusion method\n",
        "5. **Multi-Stage Filtering** - Early + late constraints\n",
        "6. **Complex Pipeline** - Multiple strategies combined\n",
        "\n",
        "The Universal Query API handles complex search orchestration server-side, enabling sophisticated retrieval with simple declarative queries.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional cleanup\n",
        "# client.delete_collection(collection_name)\n",
        "\n",
        "print(\"Tutorial complete!\")\n",
        "print(\"Next: Experiment with your own datasets and query patterns\")\n",
        "print(\"Docs: https://qdrant.tech/documentation/\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
