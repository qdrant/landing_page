---
title: Embed faster. Query faster. Go hybrid or multimodal.
cards:
- id: 0
  icon:
    src: /img/cloud-inference-features/ai.svg
    alt: AI
  title: Vector search with built-in embeddings
  description: Generate embeddings inside the network of your Qdrant Cloud cluster. No separate model server or pipeline needed.
- id: 1
  icon:
    src: /img/cloud-inference-features/bars-growth.svg
    alt: Bars growth
  title: In-cluster inference, lower latency
  description: Generate embeddings and run search in-region on AWS, Azure, or GCP (US only). No external hops, no extra egress. Ideal for real-time apps that canâ€™t afford delays or data transfer overhead.
- id: 2
  icon:
    src: /img/cloud-inference-features/cloud-data.svg
    alt: Cloud data
  title: Supports Dense, Sparse & Image Models
  description: Build vector search the way you need. Use dense models like all-MiniLM-L6-v2 for fast semantic match, sparse models like splade-pp-en-v1 or bm25 for keyword recall, or CLIP-style models for image and text. Need Hybrid and/or multimodal search? Covered.
sitemapExclude: true
---
