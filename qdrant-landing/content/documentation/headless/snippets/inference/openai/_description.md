This code snippet demonstrates how to use the OpenAI API for inference on Qdrant Cloud. The example first creates a collection that supports vectors with 512 dimensions. Next, a point is inserted, but instead of providing an explicit vector, the example request includes text along with the name of an OpenAI model. When the model name is prepended with `openai/`, the Qdrant Cloud Inference proxy will use the OpenAI API to infer embeddings out of the provided text and store the resulting vector. The request also shows how to pass OpenAI-specific parameters to the API. In this case, the request passes the OpenAI API key and the `dimensions` parameter. Finally, the example shows how to use the OpenAI API for query-time inference. Instead of supplying an explicit query vector, the query includes text and name of an OpenAI model, as well as an OpenAI API key and the OpenAI-specific `dimensions` parameter. When the model name is prepended with `openai/`, the Qdrant Cloud Inference proxy will use the OpenAI API to infer embeddings out of the provided text and search with the resulting vector.