---
title: "BM42: New baseline for hybrid search"
short_description: "Introducing next evolutionary step in lexical search."
description: "Introducing BM42 - a new sparse embedding approach, which combine benefits of exact keyword search and intelligence of transformers"
social_preview_image: /articles_data/bm42/social-preview.jpg
preview_dir: /articles_data/bm42/preview
weight: -140
author: Andrey Vasnetsov
date: 2024-06-01T13:00:00+03:00  # ToDo change date
draft: false
keywords:
  - hybrid search
  - sparse embeddings
  - bm25
---

For the last 40 years, BM25 has been the de-facto standard for search engines. 
It is a simple, yet powerful algorithm that has been used in many search engines, including Google, Bing, and Yahoo.

It seemed that with the advent of vector search, its influence should have diminished, but it only partially did.
Current state-of-the-art approach to retrieval nowadays tries to incorporate BM25 along with embeddings into a hybrid search system.

However, the use-case of text retrieval has significantly shifted since the introduction of RAG.
Many assumptions, on which BM25 was built, are no longer valid. 

For example, the the typical length of the document and the query, as used in web-search, have little to do with the typical length of the document and the query in the RAG systems.


In this article we will try to understand which strong sides of BM25 made it relevant for so many years, why current alternatives were struggling to replace it, and how we can make a next step in the evolution of lexical search.

## Why BM25 was relevant

To understand why BM25 stayed relevant for so long, we need to look into how it works.

Famous BM25 formula is defined as:

$$
\text{score}(D,Q) = \sum_{i=1}^{N} \text{IDF}(q_i) \cdot \frac{f(q_i, D) \cdot (k_1 + 1)}{f(q_i, D) + k_1 \cdot \left(1 - b + b \cdot \frac{|D|}{\text{avgdl}}\right)}
$$

Let's dumb it down a bit, to get some intuition behind it.

- The $score(D, Q)$ - means, that we compute the score for each pair of document $D$ and query $Q$.

- The $\sum_{i=1}^{N}$ - means, that each of $N$ terms in the query contributes to the final score as a part of the sum.

- The $\text{IDF}(q_i)$ - is the inverse document frequency. Means, that the more rare the term $q_i$ is, the more it contributes to the score. Simplified formula for this is

$$
\text{IDF}(q_i) = \frac{\text{Number of documents}}{\text{Number of documents with } q_i}
$$

It should be fair to say, that the `IDF` is the most important part of the BM25 formula.
It is responsible for selecting the most important terms in the query, relative to the specific document collection.
So intuitively, we can interpret the `IDF` as **term importance within the corpora**.

That explains why BM25 is so good at handling queries, which dense embeddings consider out-of-domain.

The last component of the formula can be intuitively interpreted as **term importance within the document**.
It might look a bit complicated, so let's break it down.

$$
\text{Term importance in document }(q_i) =  \color{red}\frac{f(q_i, D)\color{black} \cdot \color{blue}(k_1 + 1) \color{black} }{\color{red}f(q_i, D)\color{black} + \color{blue}k_1\color{black} \cdot \left(1 - \color{blue}b\color{black} + \color{blue}b\color{black} \cdot \frac{|D|}{\text{avgdl}}\right)}
$$

- The $\color{red}f(q_i, D)\color{black}$ - is the frequency of the term $q_i$ in the document $D$. Or in other words, the number of times the term $q_i$ appears in the document $D$.
- The $\color{blue}k_1\color{black}$ and $\color{blue}b\color{black}$ are the hyperparameters of the BM25 formula. In most implementations, they are constants set to $k_1=1.5$ and $b=0.75$. Those constants define relative implications of the term frequency and the document length in the formula.
- The $\frac{|D|}{\text{avgdl}}$ - is the relative length of the document $D$ compared to the average document length in the corpora. The intuition befind this part is following: if the token is found in the smaller document, it is more likely that this token is important for this document.

#### Will BM25 term importance in the document work for RAG?

As we can see, the *term importance in the document* heavily depends on statistics within the document. And statistics works well if the document is long enough.
So it works well for searcing web-pages, books, articles, etc.

However, would it work as well for modern search applications, such as RAG? Let's see.

The typical length of the document in RAG is much shorter than in the web-search. In fact, even if we are working with the web-pages and articles, we prefer to split them into chunks, so that
a) Dense models can handle them and
b) We can pin-point the exact part of the document, which is relevant to the query

As a result, document size in RAG is small and fixed.

That effectively renders the term importance in the document part of the BM25 formula useless. 
The term frequency in the document is always 0 or 1, and the relative length of the document is always 1.

So, the only part of the BM25 formula, which is still relevant for RAG, is `IDF`. Let's see how we can leverage it.

## Why SPLADE is not always the answer

Before discussing the new approach we propose, let's take a look at the current state-of-the-art alternative to BM25 - SPLADE.

The idea behind SPLADE is interesting - what if we let smart end-to-end trained model to generate bag-of-words representation of the text for us?
It will assign all the weights to the tokens, so that we won't need to bother about statistics and hyperparameters.
The documents are then represented as a sparse embedding, where each token is represented as an element of the sparse vector.

And it works in academic benchmarks. There are many papers, which report that SPLADE outperforms BM25 in terms of retrieval quality.
This performance, however, comes at a cost.

* **Inappropriate Tokenizer**: In order to incorporate transformers for this task, SPLADE models require to use standard transformer tokenizer. This kind of tokenizers are not designed for retrival tasks. for example, if the word in not in (quite limiter) vocabulary, it will be either split into subwords or replaced with `[UNK]` token. This behavior works well for language modeling, but completely destructive for retrieval tasks.

* **Expensive Token Expansion**: In order to compensate the tokenization issues, SPLADE uses *token expansion* technique. It means, that for each token in the query, we generate a set of similar tokens. There are a few problems with this approach:
  - It is computationally and memory expensive. For each token in the document we need to generate more values, which increases both the size of the storage and the time of the retrieval.
  - It is not always clear, where to stop with the token expansion. The more tokens we generate, the more likely we are to get the relevant one. But at the same time, the more tokens we generate, the more likely we are to get the irrelevant results.
  - Token expansion dilutes the interpretability of the search. We can't say anymore, which tokens were actually used in the document, and which were generated by the token expansion.

* **Domain and Language Dependency**: SPLADE models are trained on the specific corpora. It means, that they are not always generalizable to the new or rare domains. As it doesn't use any statistics from the corpora, it is not able to adapt to the new domain without fine-tuning.

* **Inference Time**: On top of that, currently available SPLADE models are quite big and slow. They usually require GPU to make the inference in reasonable time.

At Qdrant we acknowledge the aforementioned problems and were looking for the solution. 
Our idea was to combine the best of both worlds - the simplicity and interpretability of BM25 and the intelligence of transformers, while avoiding the pitfalls of SPLADE.

And here is what we came up with.

## The best of both worlds 

We already discussed, that the most important part of the BM25 formula is the `IDF`. In fact it is so imporant, that we decided to move its calculation into the Qdrant engine itself.
Check out our latest release notes: **TODO LINK HERE**. This type of separation allows streaming updates of the sparse embeddings, while keeping the `IDF` calculation up-to-date.

As for the second part of the formula, *term importance within the document* needs to be rethought.

Since we can't rely on the statistics within the document, we can try to use semantics of the document instead.
And semantics is what transformers are good at. We only need to solve two problems:

- How to extract importance information from the transformer?
- How to avoid the tokenization issues?


### Attention is all you need

Transformer models, even those used to generate embeddings, generate a bunch of different outputs.
Some of those outputs are used for generating embeddings. 

Others are used to solve other kind of tasks, such as classification, text generation, etc.

The one particularlly interesting output for us is the attention matrix.

{{< figure src="/articles_data/bm42/attention-matrix.png" alt="Attention matrix" caption="Attention matrix" width="60%" >}}

The attention matrix is a square matrix, where each row and column corresponds to the token in the input sequence.
It represents the importance of each token in the input sequence for each other.

The classical transformer models are trained to predict masked tokens in the context, so the attention defines which of the context tokens have the most influence on the masked token.

Apart from the regular text tokens, the transformer model also has special token called `[CLS]`. This token is used to represent the whole sequence in the classification tasks. And this is exactly what we need.

By looking at the attention row for the `[CLS]` token, we can get the importance of each token in the document for the whole document.


ToDo: Example here


Note, that classical transformers have multiple attention heads, so we can get multiple importance vectors for the same document. The simplest way to combine them is to simply average them.

Those averaged attention vectors is the importance information we were looking for.
And the best part of it is, that you can get it from any transformer model, without any additional training.
So naturally, BM42 supports any natural language as long as you have a transformer model for it.

In our implementation, we use the `sentence-transformers/all-MiniLM-L6-v2` model, which gives a huge boost in the inference speed compared to the SPLADE models.


### BPE retokenization

The final piece of the puzzle we need to solve is the tokenization issue. Indeed, in order to get attention vectors, we need to use native transformer tokenization.
But this tokenization is not suitable for the retrieval tasks. What can we do about it?

Actually, the solution we came up with is quite simple. We reverse the tokenization process after we get the attention vectors.

Transformers use Byte-Pair Encoding (BPE) tokenization. In case it sees the word, which is not in the vocabulary, it splits it into subwords.

it looks like this:

```text
"unbelievable" -> ["un", "##believ", "##able"]
```

What we can do is to merge the subwords back into the words. Luckily, the subwords are marked with the `##` prefix, so we can easily detect them.
Since the attention weights are normalized, we can simply sum the attention weights of the subwords to get the attention weight of the word.

After that, we can apply the same traditional NLP techniques, as

- Removing of the stop-words
- Removing of the punctuation
- Lemmatization

In this way we significantly reduce the number of tokens, and therefore minimize the memory footprint of the sparse embeddings. At the same time we don't compromise the ability to match (almost) exact tokens.


## Practical examples


| Trait                   | BM25         | SPLADE       | BM42         |
|-------------------------|--------------|--------------|--------------|
| Interpretability        | High ✅      | Ok 🆗  ,      | High ✅      |
| Document Inference speed| Very high ✅ | Slow 🐌      | High ✅      |
| Query Inference speed   | Very high ✅ | Slow 🐌      | Very high ✅ |
| Memory footprint        | Low ✅       | High ❌      | Low ✅       |
| In-domain accuracy      | Ok 🆗        | High ✅      | High ✅      |
| Out-of-domain accuracy  | Ok 🆗        | Low ❌       | Ok 🆗        |
| Small documents accuracy| Low ❌       | High ✅      | High ✅      |
| Large documents accuracy| High ✅      | Low ❌       | Ok 🆗        |
| Unknown tokens handling | Yes ✅       | Bad ❌       | Yes ✅       |
| Multi-lingual support   | Yes ✅       | No ❌        | Yes ✅       |
| Best Match              | Yes ✅       | No ❌        | Yes ✅       |


ToDO: Example of Bm42 inference with FastEmbed

ToDo: Benchmark results

## Stay curious

Meaning all of the above advantages, we have to say that BM42 is not a silver bullet.
For large documents without chunks, BM25 might still be a better choice.

There might be a better way to extract the importance information from the transformer, there might be a better way to weight IDF and attention scores together.

Hey, we are not a model training company, we are building the search engine and we are giving you the tools to experiment with.
And we believe that sparse vectors is at exactly the right level of abstraction be both powerful and flexible.

We are looking forward to see what you can build with Qdrant. Stay curious!

