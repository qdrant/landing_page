# https://qdrant.tech/ llms.txt
## Overall Summary
> Qdrant is a cutting-edge platform focused on delivering exceptional performance and efficiency in vector similarity search. As a robust vector database, it specializes in managing, searching, and retrieving high-dimensional vector data, essential for enhancing AI applications, machine learning, and modern search engines. With a suite of powerful features such as state-of-the-art hybrid search capabilities, retrieval-augmented generation (RAG) applications, and dense and sparse vector support, Qdrant stands out as an industry leader. Its offerings include managed cloud services, enabling users to harness the robust functionality of Qdrant without the burden of maintaining infrastructure. The platform supports advanced data security measures and seamless integrations with popular platforms and frameworks, catering to diverse data handling and analytic needs. Additionally, Qdrant offers comprehensive solutions for complex searching requirements through its innovative Query API and multivector representations, allowing for precise matching and enhanced retrieval quality. With its commitment to open-source principles and continuous innovation, Qdrant tailors solutions to meet both small-scale projects and enterprise-level demands efficiently, helping organizations unlock profound insights from their unstructured data and optimize their AI capabilities.

## Page Links
- [Concepts](https://qdrant.tech/documentation/concepts/): Qdrant's glossary introduces key AI concepts, such as Collections (sets of searchable points), Payloads (associated vector data), and Points (vector records). It also covers tools like Search, Explore, Filtering, Hybrid Queries, and processes like Indexing, Optimization, and Snapshots for efficient storage and retrieval in vector databases.
- [Collections](https://qdrant.tech/documentation/concepts/collections/): A collection in Qdrant is a structured set of points (vectors) that support searching, with specific dimensionality and distance metrics like Dot product, Cosine similarity, and Euclidean distance. Collections can be customized with various configurations (e.g., multitenancy, on-disk storage, sparse/dense vectors, and indexing options), allowing flexibility in structure, storage, and performance tuning, including features like multiple named vectors and uint8 embeddings.
- [Points](https://qdrant.tech/documentation/concepts/points/): Qdrant uses "points" as its core data structure, which consist of vectors and optional payloads, and these are searchable within collections via vector similarity. It supports dense, sparse, and named vectors, allows batch uploads with idempotent APIs, and provides methods for modifying points, including updating and deleting vectors or payloads, while ensuring data consistency with a write-ahead-log mechanism.
- [Vectors](https://qdrant.tech/documentation/concepts/vectors/): Qdrant's vector search engine uses vectors (or embeddings) to represent object similarity in vector space, with dense vectors (fixed-length numerical arrays) as the most common type, while also supporting sparse vectors, multivectors, and named vectors for diverse use cases. By offering options like Float32, Float16, and Uint8 data types, Qdrant optimizes memory use and precision, enabling tasks like similarity search, clustering, and cross-modal representation of data.
- [Payload](https://qdrant.tech/documentation/concepts/payload/): Qdrant enables storing additional metadata, called "payloads," alongside vectors in JSON format and supports various data types like integers, floats, keywords, geo-coordinates, and datetimes. It offers flexible methods for payload management, including setting, overwriting, clearing, deleting, and indexing payloads, which enhance customization and search performance in dynamic datasets.
- [Search](https://qdrant.tech/documentation/concepts/search/): Similarity search utilizes vector-based methods to find objects close in meaning or attributes (like texts, images, or music) in vector space, with Qdrant's `Query API` supporting various search types such as k-NN, filtering, recommendations, and hybrid queries. Key features include customizable metrics (e.g., cosine similarity), filtering by conditions like scores or payloads, efficient approximate and exact search options, and capabilities for handling both dense and sparse vectors, with batch search available for multiple queries in one request.
- [Explore](https://qdrant.tech/documentation/concepts/explore/): Qdrant offers advanced APIs for data exploration, including the **Recommendation API** and **Discovery API**, which support flexible search strategies like average vector, best score, and sum scores, allowing for recommendations based on positive/negative examples or dissimilarity to outliers. These tools, along with multi-vector support, batch processing, and inter-collection lookup, enable powerful use cases such as recommendations, data cleaning, and embedding-agnostic searches optimized by strategies tailored to specific needs and performance trade-offs.
- [Hybrid Queries #required](https://qdrant.tech/documentation/concepts/hybrid-queries/): Qdrant's `Query API` enables advanced search techniques like hybrid and multi-stage queries by combining multiple queries or stages for more precise results. Key features include prefetching for nested queries, result fusion methods (`rrf` and `dbsf`), multi-stage re-scoring, diversity-focused Maximal Marginal Relevance (MMR), and customizable score boosting to incorporate business logic.
- [Filtering](https://qdrant.tech/documentation/concepts/filtering/): Qdrant enables advanced filtering for vector search by allowing the use of logical clauses like `AND`, `OR`, and `NOT`, alongside conditions such as `Match`, `Match Any`, `Match Except`, and nested filters, to refine searches based on payload attributes or point IDs. These filters facilitate complex queries addressing both business-specific requirements, like availability or pricing, and structured JSON data, such as nested fields or arrays.
- [Optimizer](https://qdrant.tech/documentation/concepts/optimizer/): Qdrant employs three key optimizers to enhance data storage and retrieval efficiency: Vacuum Optimizer removes accumulated deleted records to free memory and maintain performance, Merge Optimizer consolidates small segments to improve search efficiency while adhering to size thresholds, and Indexing Optimizer dynamically enables indexes and memmap storage based on data size, optimizing resource usage. All optimizer behaviors are configurable through a configuration file or dynamically adjustable at the collection level.
- [Storage](https://qdrant.tech/documentation/concepts/storage/): Qdrant stores data in segments, which handle vector and payload storage along with indexing, and can be either appendable (supporting additions, deletions, and queries) or non-appendable (read-only). Storage options include in-memory (high speed but RAM-intensive) and memmap (disk-based with flexible memory use), with configurable thresholds for optimization, while payload storage can be in-memory for speed or on-disk for lower RAM usage, supported by indexing to reduce latency during queries.
- [Indexing](https://qdrant.tech/documentation/concepts/indexing/): Qdrant combines vector and traditional indexes to optimize vector search with filters. It offers payload indexes for various data types, parameterized and on-disk options for performance and memory efficiency, and specialized indexing (tenant and principal indexes) to enhance multi-tenant and time-based searches.
- [Snapshots](https://qdrant.tech/documentation/concepts/snapshots/): Snapshots in Qdrant are `tar` archive files capturing the data and configuration of a specific collection on a specific node, requiring separate snapshots for each node in a distributed setup. They facilitate data archiving, replication, and recovery, with multiple restoration methods available, such as via URL, file upload, or start-up parameter, and support full storage snapshots for single-node deployments.
- [Local Quickstart](https://qdrant.tech/documentation/quickstart/): To get started with Qdrant locally, install and run its Docker container, then use the supported client libraries like Python, JavaScript, or others to initialize a connection, create a collection, and add vectors with associated payloads. Ensure security measures are in place as Qdrant runs without encryption or authentication by default.
- [Distance-based data exploration](https://qdrant.tech/articles/distance-based-exploration/): To uncover hidden structures in unstructured high-dimensional datasets, tools like Qdrant’s Distance Matrix API simplify distance calculations, which can then be used with methods such as UMAP for dimensionality reduction or KMeans for clustering. These approaches enable visualizing data relationships and patterns effectively, supporting deeper data exploration without requiring computationally intensive operations.
- [Modern Sparse Neural Retrieval: From Theory to Practice](https://qdrant.tech/articles/modern-sparse-neural-retrieval/): The article explores modern sparse neural retrieval methods that aim to combine the precision of keyword-based approaches like BM25 with the semantic understanding of dense retrievers. It compares key models like DeepCT, DeepImpact, and TILDEv2, detailing their evolution, strengths, and limitations, while providing practical guidance on using SPLADE++ in Qdrant for optimized retrieval solutions.
- [Qdrant Summer of Code 2024 - ONNX Cross Encoders in Python](https://qdrant.tech/articles/cross-encoder-integration-gsoc/): Huong (Celine) Hoang's Summer of Code 2024 internship at Qdrant focused on integrating cross-encoders into the lightweight FastEmbed library to enhance re-ranking capabilities for more context-aware search applications. Through overcoming challenges like tokenization, ONNX model integration, and testing, she successfully expanded FastEmbed's functionality while ensuring user-friendliness and scalability, with future improvements aimed at model support and optimization.
- [An Introduction to Vector Databases](https://qdrant.tech/articles/what-is-a-vector-database/): A **vector database** is a specialized system designed to manage unstructured data by converting it into vectors—numerical representations that capture context and semantics—enabling advanced similarity searches and analysis that traditional databases cannot perform. Unlike conventional structured databases, vector databases excel in applications such as recommendation systems, anomaly detection, and semantic search, leveraging metadata and filtering for more refined results.
- [What is Vector Quantization?](https://qdrant.tech/articles/what-is-vector-quantization/): Vector quantization is a data compression technique that reduces memory usage for high-dimensional data while retaining essential information, enabling faster and more efficient storage and search operations, particularly in large datasets. Methods like Scalar and Binary Quantization significantly shrink vector sizes (e.g., Binary Quantization reduces memory by 32x), with Binary Quantization providing the most substantial speed gains for distance computations using optimized binary operations like XOR and Popcount.
- [Vector Search Resource Optimization Guide](https://qdrant.tech/articles/vector-search-resource-optimization/): This guide provides strategies for optimizing vector database performance and resource efficiency, focusing on techniques like indexing (using HNSW), parameter tuning for balancing speed and accuracy, and data compression via quantization (e.g., reducing 32-bit float vectors to 8-bit representations). These methods aim to help users scale applications cost-effectively while tailoring performance to specific use cases.
- [A Complete Guide to Filtering in Vector Search](https://qdrant.tech/articles/vector-search-filtering/): Qdrant's filtered vector search combines semantic similarity and metadata constraints to improve search precision and efficiency, particularly in applications like e-commerce. By using methods like pre-filtering, post-filtering, and Qdrant's novel filterable HNSW index, it addresses inefficiencies in traditional search approaches, balancing accuracy and computational resources for datasets of varying sizes and metadata cardinality.
- [Qdrant Internals: Immutable Data Structures](https://qdrant.tech/articles/immutable-data-structures/): The article discusses the trade-offs and performance considerations of different data structures based on hardware efficiency, highlighting that no single structure is universally optimal. It emphasizes the benefits of immutability, such as optimized memory allocation and faster access, and introduces advanced techniques like minimal perfect hash functions to reduce hash collisions and improve performance in real-world systems like vector databases (e.g., Qdrant).
- [miniCOIL: on the Road to Usable Sparse Neural Retrieval](https://qdrant.tech/articles/minicoil/): Sparse neural retrieval bridges the gap between term-based and dense retrieval by combining lightweight, explainable methods like BM25 with the ability to capture word meanings, but prior models faced challenges in domain adaptability and efficiency. To address this, Qdrant developed **miniCOIL**, a sparse neural retriever inspired by Contextualized Inverted Lists (COIL), designed to produce lightweight, semantically-aware representations while maintaining robust out-of-domain performance.
- [Relevance Feedback in Informational Retrieval](https://qdrant.tech/articles/search-feedback-loop/): A well-defined search query significantly aids information retrieval, but users often struggle to articulate precise queries, making relevance feedback a critical tool for refining results iteratively. Despite decades of research, integrating relevance feedback into modern vector search systems remains underutilized due to challenges like user reluctance, computational costs, and the early-stage development of neural search techniques, prompting a need to explore approaches like query and scoring refinement for better adoption of these methods.
- [Built for Vector Search](https://qdrant.tech/articles/dedicated-vector-search/): Vector databases are specialized tools designed for efficient high-dimensional data searches, where general-purpose databases struggle with scalability and performance due to vectors' heavy storage and computational demands. With BASE-oriented architectures prioritizing availability and scalability, vector databases leverage fixed-size embeddings for fast indexing and retrieval, making them ideal for applications requiring high-throughput, low-latency search, unlike ACID-compliant systems built for strict consistency.
- [Any* Embedding Model Can Become a Late Interaction Model... If You Give It a Chance!](https://qdrant.tech/articles/late-interaction-models/): Qdrant 1.10's multi-vector support allows traditional dense embedding models to be adapted for late interaction retrieval by leveraging output token embeddings, achieving competitive or superior performance compared to specialized models like ColBERT while maintaining efficiency. Experimental results show that using output token embeddings enhances retrieval quality across multiple datasets, suggesting significant optimization potential for advanced search systems.
- [Optimizing Memory for Bulk Uploads](https://qdrant.tech/articles/indexing-optimization/): Efficient memory management during bulk ingestion in Qdrant can be achieved by temporarily disabling dense vector indexing (`m=0`), deferring HNSW graph construction, and enabling on-disk storage (`on_disk: true`) for vectors and indexes to reduce RAM usage. These practices help avoid out-of-memory errors and maintain performance, particularly in large-scale datasets or memory-constrained environments.
- [Introducing Gridstore: Qdrant's Custom Key-Value Store](https://qdrant.tech/articles/gridstore-key-value-storage/): Qdrant built its own storage engine, Gridstore, after encountering limitations with RocksDB, such as compaction-induced latency and configuration complexity, as well as a need for tailored features like efficient handling of sequential keys and variable-sized data. Gridstore's architecture consists of three layers—Data, Mask, and Gaps—that optimize fast retrieval, space reuse, and scalable updates, while ensuring data integrity through a Write-Ahead Log for crash recovery.
- [What is Agentic RAG? Building Agents with Qdrant](https://qdrant.tech/articles/agentic-rag/): Standard Retrieval-Augmented Generation (RAG) follows a linear flow of retrieving documents and generating responses, while Agentic RAG integrates agents, which use large language models (LLMs) to make decisions and take non-linear, multi-step actions, such as querying databases, improving queries, or self-correcting. While tools like LangGraph facilitate building such agentic systems by enabling cyclic workflows and decision-making, their practical applications often depend on use cases, with limitations like higher cost and latency compared to simpler RAG systems.
- [Hybrid Search Revamped - Building with Qdrant's Query API](https://qdrant.tech/articles/hybrid-search/): Qdrant 1.10 introduces a powerful new Query API that simplifies building hybrid search systems by enabling server-side integration of dense, sparse, and multivector search strategies, as well as supporting complex search pipelines and late interaction models like ColBERT for efficient reranking. Unlike simple linear combinations of scores, Qdrant employs advanced methods such as Reciprocal Rank Fusion and layered reranking to improve retrieval quality, emphasizing data-driven experimentation and effectiveness evaluation.
- [What is RAG: Understanding Retrieval-Augmented Generation](https://qdrant.tech/articles/what-is-rag-in-ai/): Retrieval-augmented generation (RAG) enhances Large Language Models (LLMs) by integrating external information retrieval, enabling them to access and generate responses based on relevant data from a vector database rather than relying solely on pre-trained knowledge. The RAG architecture includes a "retriever" using vector similarity techniques (dense or sparse) to identify relevant data and a "generator" to create accurate, non-hallucinated outputs, with hybrid search approaches often improving results further.
- [BM42: New Baseline for Hybrid Search](https://qdrant.tech/articles/bm42/): BM25 has remained a robust and widely used search algorithm due to its simplicity and focus on term importance within a corpus (via IDF), though its limitations in handling modern Retrieval-Augmented Generation (RAG) use cases with shorter, fixed-length documents have reduced its effectiveness. Despite alternatives like SPLADE showing promise, challenges such as inefficient token expansion, domain dependency, and high computational costs persist, motivating efforts like Qdrant's approach to combine BM25's interpretability with transformer-based semantic insights.
- [Qdrant 1.8.0: Enhanced Search Capabilities for Better Results](https://qdrant.tech/articles/qdrant-1.8.x/): Qdrant 1.8.0 introduces significant performance enhancements, including up to 16x faster hybrid search with sparse vectors, dynamic CPU resource allocation for optimized indexing, and improved text data indexing that reduces RAM usage by 10%. Additional features include ordering search results by metadata, datetime payload support, collection existence checks, and enhanced nested field modification capabilities, offering a more efficient and scalable search experience.
- [Optimizing RAG Through an Evaluation-Based Methodology](https://qdrant.tech/articles/rapid-rag-optimization-with-qdrant-and-quotient/): AI, particularly through Retrieval Augmented Generation (RAG) methods that integrate vector databases like Qdrant and evaluation tools like Quotient, significantly improves knowledge management by enhancing the accuracy, relevance, and reliability of generated responses in organizational contexts. Key findings show that optimizing document retrieval strategies, adapting to query needs, and careful evaluation of LLM outputs are essential to reducing hallucinations and ensuring effective RAG solutions.
- [Is RAG Dead? The Role of Vector Databases in Vector Search | Qdrant](https://qdrant.tech/articles/rag-is-dead/): Despite advances like larger LLM context windows (e.g., Gemini 1.5’s 10M tokens), Retrieval Augmented Generation (RAG) and vector databases remain essential for efficiency, accuracy, and cost-effectiveness, especially in enterprise applications. Relying solely on increased context is computationally expensive and less precise, whereas vector search offers faster, cheaper, and more targeted information retrieval, making it indispensable for scalable AI systems.
- [Optimizing OpenAI Embeddings: Enhance Efficiency with Qdrant's Binary Quantization](https://qdrant.tech/articles/binary-quantization-openai/): OpenAI Ada-003 embeddings provide advanced performance for NLP tasks but are large and resource-intensive; the article details how Qdrant's Binary Quantization addresses this by significantly reducing storage requirements and accelerating real-time search while maintaining strong accuracy. Experimental results demonstrate that features like rescoring further boost accuracy—especially for higher-dimensional models—making binary quantization a practical solution for efficient and scalable use of OpenAI embeddings in real-world applications.
- [How to Implement Multitenancy and Custom Sharding in Qdrant](https://qdrant.tech/articles/multitenancy/): Qdrant enables scalable machine learning deployments by supporting multitenancy and custom sharding, allowing data from multiple customers to be efficiently isolated and managed within a single cluster. These features improve performance, reduce costs, enable precise data placement (such as by region or time), and provide multiple levels of data isolation, making Qdrant suitable for complex, compliance-sensitive, and large-scale applications.
- [Data Privacy with Qdrant: Implementing Role-Based Access Control (RBAC)" #required](https://qdrant.tech/articles/data-privacy/): Vector databases often contain sensitive proprietary data and are subject to strict compliance regulations, making robust security measures—such as encryption, role-based access control, and deployment flexibility—essential to prevent data breaches and meet legal requirements. Qdrant addresses these needs by offering features like API key and JWT-based authentication, TLS encryption, and granular access control, enabling enterprises to build secure, compliant, and data-sovereign vector search solutions.
- [Discovery needs context](https://qdrant.tech/articles/discovery-search/): Discovery search in Qdrant 1.7 enables more controlled exploration of vector spaces by allowing users to define contextual boundaries using pairs of positive and negative vectors, either with or without a specific target point. This approach supports advanced search and filtering applications—like refining image or recommendation results—by partitioning the search space and incorporating positive/negative feedback dynamically.
- [What are Vector Embeddings? - Revolutionize Your Search Experience](https://qdrant.tech/articles/what-are-embeddings/): Embeddings are numerical vector representations created by deep learning models to capture the meaning and context of complex data like text, images, or audio, enabling efficient processing, search, and personalized recommendations by measuring semantic similarity within high-dimensional spaces. Advanced models such as BERT and GPT generate context-aware embeddings for nuanced understanding, and these vectors can be integrated with APIs for applications like search, recommendations, and retrieval augmented generation.
- [What is a Sparse Vector? How to Achieve Vector-based Hybrid Search](https://qdrant.tech/articles/sparse-vectors/): Sparse vectors represent documents by assigning non-zero weights only to select tokens (words or subwords), making them efficient and interpretable for tasks like search and ranking, whereas dense vectors assign non-zero values to all elements, capturing more nuanced relationships but at a higher computational cost. Methods like SPLADE use neural networks to improve upon traditional sparse models like BM25, enabling better handling of specialized terms and large datasets, though sparse vectors still struggle with capturing deeper semantic relationships compared to dense embeddings.
- [Qdrant 1.7.0 has just landed!](https://qdrant.tech/articles/qdrant-1.7.x/): Qdrant 1.7.0 introduces major new features including support for sparse vectors for keyword-based search, a Discovery API for advanced vector exploration, user-defined sharding for more flexible data organization, and snapshot-based shard transfer for efficient cluster scaling. The release also includes various minor improvements and bug fixes to enhance overall performance.
- [Deliver Better Recommendations with Qdrant’s new API](https://qdrant.tech/articles/new-recommendation-api/): Qdrant 1.6 introduces a more flexible Recommendation API, enabling users to mix IDs and vector embeddings as positive and negative examples, and to choose between the traditional `average_vector` and the new `best_score` strategies for tailored recommendations. The update enhances semantic and recommendation search capabilities by allowing more control over how vectors are combined and how candidate points are evaluated during approximate nearest neighbor searches using the HNSW graph structure.
- [Vector Search as a dedicated service](https://qdrant.tech/articles/dedicated-service/): The article discusses the debate over whether to use specialized vector databases or existing database plugins for storing embeddings in LLM applications, highlighting that vector databases (better termed "search engines") are architecturally distinct for scalability and search speed. It argues for dedicated vector solutions over integrated plugins, debunking misconceptions about data duplication, synchronization complexity, costs, and maintenance, and emphasizing the flexibility, performance, and manageability advantages of specialized vector search engines.
- [FastEmbed: Qdrant's Efficient Python Library for Embedding Generation](https://qdrant.tech/articles/fastembed/): FastEmbed is a Python library designed to simplify and accelerate the creation of text embeddings by providing easy-to-use workflows, quantized models, minimal dependencies, and ONNXRuntime integration, making it faster and more resource-efficient than traditional frameworks like PyTorch Transformers. It supports a select set of high-quality, quantized transformer models optimized for CPU usage, offering significant speed and installation advantages for production NLP applications.
- [Google Summer of Code 2023 - Polygon Geo Filter for Qdrant Vector Database](https://qdrant.tech/articles/geo-polygon-filter-gsoc/): As a Google Summer of Code 2023 participant, Zein Wen worked on enhancing Qdrant's geo filtering capabilities by adding a Geo Polygon Filter, allowing users to refine vector database queries with complex geographic boundaries for greater flexibility and real-world applicability. The project involved addressing technical challenges in geometry computation and API design, focusing on efficiency, usability, and user experience, while providing valuable personal growth and open-source collaboration experience.
- [Binary Quantization - Vector Search, 40x Faster](https://qdrant.tech/articles/binary-quantization/): Qdrant’s new binary quantization (BQ) feature significantly reduces memory usage and boosts retrieval speeds—up to 40x—by converting high-dimensional vectors into binary embeddings, with recall accuracy customizable during search. While BQ is ideal for large vectors (over 1024 dimensions), its compression is most effective at scale, and it supports a flexible trade-off between speed and accuracy with minimal user intervention.
- [Food Discovery Demo](https://qdrant.tech/articles/food-discovery-demo/): Qdrant's open-source Food Discovery Demo uses semantic image search powered by the CLIP model and the Qdrant vector database to help users explore food options even without explicit queries, offering features like random dish suggestions, text-based search, and personalized recommendations through user feedback (including negative-only feedback). The system's architecture includes a FastAPI backend, React frontend, and supports diverse search modes on a dataset of over 2M dishes, with CLIP embeddings enabling effective cross-modal search and feedback-driven result refinement.
- [Google Summer of Code 2023 - Web UI for Visualization and Exploration](https://qdrant.tech/articles/web-ui-gsoc/): Kartik Gupta participated in Google Summer of Code 2023, where he developed a user-friendly web-based UI for Qdrant, a vector search engine, completing six major milestones including UI design, data exploration, and advanced query features while overcoming technical and project management challenges. His experience deepened his skills in vector data visualization, optimization techniques like web workers, and collaborative problem-solving, and highlighted areas for future improvement such as enhanced autocomplete and expanded data visualization.
- [Qdrant Summer of Code 2024 - WASM based Dimension Reduction](https://qdrant.tech/articles/dimension-reduction-qsoc/): Jishan Bhattacharya interned at Qdrant, focusing on optimizing high-dimensional vector visualization by rewriting the t-SNE algorithm in Rust with WebAssembly, introducing multi-threading, and implementing the Barnes-Hut approximation for significant speed improvements. Key challenges included handling large datasets, optimizing data transfer and rendering, and future improvements were identified, such as faster payload parsing, more efficient data handling, and adopting a WebGL-based chart library.
- [Semantic Search As You Type](https://qdrant.tech/articles/search-as-you-type/): The author optimized semantic search-as-you-type for their website by switching from a Python-based Qdrant implementation to one in Rust, using ONNX Runtime for embeddings and a prefix cache to greatly accelerate short queries—achieving more than a 10x speedup in some cases. Further quality improvements include prioritized, parallelized searches for exact matches and semantic relevance, with batch requests for efficiency and deduplication, resulting in a faster and more responsive search experience.
- [Vector Similarity: Going Beyond Full-Text Search | Qdrant](https://qdrant.tech/articles/vector-similarity-beyond-search/): Vector similarity search enables advanced data exploration beyond traditional full-text search by allowing semantic comparisons, cross-modal retrieval, dissimilarity and diversity searches, outlier detection, and enhanced recommendations, functions not possible or limited in keyword-based search. Vector databases leverage these capabilities to unlock new insights from unstructured data, though they require dedicated design patterns and should be used distinctly from full-text search for optimal results.
- [Serverless Semantic Search](https://qdrant.tech/articles/serverless/): This guide demonstrates how to create a free, non-commercial semantic search engine for your website or app by combining Rust, AWS Lambda, an embedding provider (like Cohere), and a Qdrant instance, with step-by-step setup instructions for each tool and service. It covers deploying a Rust-based Lambda function, safely managing API keys for embedding services, and integrating all components to achieve semantic search functionality.
- [Introducing Qdrant 1.3.0](https://qdrant.tech/articles/qdrant-1.3.x/): Qdrant 1.3.0 introduces major updates including an asynchronous I/O interface for faster disk operations, oversampling for improved quantization accuracy, a grouping API for cross-collection lookups, a new Web UI dashboard, and support for specifying a temporary directory for snapshots. These enhancements focus on boosting performance, storage efficiency, and user experience, while addressing key user feature requests.
- [Qdrant under the hood: io_uring](https://qdrant.tech/articles/io_uring/): Qdrant version 1.3.0 adds an io_uring-based async storage backend on Linux, significantly improving disk IO throughput and reducing CPU overhead compared to traditional mmap-based IO, especially for large datasets and quantization workloads. Benchmark results show a dramatic reduction in processing time (e.g., from 43s to 12s) and higher IOPS using io_uring, demonstrating its effectiveness for high-performance, IO-bound scenarios on modern Linux systems.
- [Product Quantization in Vector Search | Qdrant](https://qdrant.tech/articles/product-quantization/): Product Quantization, introduced in Qdrant 1.2.0, is a customizable technique that significantly reduces memory usage in vector search by dividing vectors into chunks, clustering them, and storing centroid IDs, achieving higher compression rates than Scalar Quantization at the cost of some search precision and potentially slower indexing/search times. It is particularly beneficial for low-RAM environments or high-dimensional vectors, but Scalar Quantization may be preferable when accuracy and speed are more critical.
- [Scalar Quantization: Background, Practices & More | Qdrant](https://qdrant.tech/articles/scalar-quantization/): Scalar quantization compresses high-dimensional vector embeddings by converting float32 values to int8, significantly reducing memory usage (by up to 75%) and improving performance with minimal loss in search precision. Benchmarks show that scalar quantization in Qdrant yields reduced search latency and indexing time, making it highly effective for large-scale vector databases.
- [On Unstructured Data, Vector Databases, New AI Age, and Our Seed Round.](https://qdrant.tech/articles/seed-round/): Qdrant, an open-source vector database company, has raised $7.5M in seed funding led by Unusual Ventures to advance its high-performance solutions for managing unstructured data and powering AI applications. Their focus is on making billion-scale vector search affordable and accessible, leveraging innovations like Scalar Quantization and a robust technical stack to serve modern AI use cases, with strong support from the developer community.
- [Using LangChain for Question Answering with Qdrant](https://qdrant.tech/articles/langchain-integration/): LangChain's integration with Qdrant enables streamlined question answering by combining semantic search—using vector embeddings stored in Qdrant—with large language models that generate answers from retrieved context, all implemented in just a few lines of code. This approach leverages two models (one for embeddings, one for language generation) and simplifies complex application pipelines, making it easy to build, extend, and maintain QA systems.
- [Minimal RAM you need to serve a million vectors](https://qdrant.tech/articles/memory-consumption/): Measuring memory usage with tools like `htop` can be inaccurate due to memory pre-allocation, unfreed deallocations, shared memory in forked processes, and disk cache, so the best way to determine actual RAM requirements is to experimentally limit process memory and observe functionality. Benchmarks with Qdrant show that, depending on configuration, serving 1 million vectors may require as little as 1.2GB RAM in-memory, or down to 135MB using memory-mapped files—with the trade-off of slower search speed as memory allocation decreases.
- [Question Answering as a Service with Cohere and Qdrant](https://qdrant.tech/articles/qa-with-cohere-and-qdrant/): Bi-encoders efficiently power semantic Question Answering systems by using the same neural model to generate vector embeddings for both questions and answers, enabling similarity-based retrieval via tools like Cohere’s co.embed API and Qdrant’s vector database without maintaining your own infrastructure. This setup streamlines deploying scalable semantic search, and can be readily implemented using public datasets (like pubmed_qa) and SaaS solutions for embedding generation and vector storage.
- [Introducing Qdrant 1.2.x](https://qdrant.tech/articles/qdrant-1.2.x/): Qdrant 1.2 introduces significant new features including Product Quantization for up to 64x memory reduction, optional named vectors, server-side grouping requests, and advanced nested filters, alongside important changes like recovery mode, appendable mmap, and enhanced security with API key and TLS support. The update also encourages user feedback and detailed documentation is available for all these enhancements.
- [Finding errors in datasets with Similarity Search](https://qdrant.tech/articles/dataset-quality/): Modern applications rely on accurate data categorization, but manual and model-based approaches to labeling are prone to costly errors that affect user experience and business outcomes. Distance-based methods like similarity and diversity search, which use embeddings to identify outliers or misplaced items, can help improve dataset quality, especially in contexts like online furniture marketplaces, though combining multiple techniques may be necessary for best results.
- [Q&A with Similarity Learning](https://qdrant.tech/articles/faq-question-answering/): Traditional classification methods for question-answering systems require extensive data labeling and frequent retraining, making them inefficient for dynamic tasks like customer support automation. Instead, similarity learning with embeddings—further improved through fine-tuning using frameworks like Quaterion—offers a more flexible and scalable solution by matching questions and answers based on vector similarity rather than fixed classes.
- [Why Rust?](https://qdrant.tech/articles/why-rust/): Qdrant is built in Rust due to its superior memory safety, performance, and control compared to languages like Java, Scala, Python, and Go, resulting in efficient, robust, and maintainable cloud services. Rust's strict type system minimizes bugs, its growing ecosystem and community support innovation, and its adoption by major tech companies further validates Qdrant’s technology choice.
- [Layer Recycling and Fine-tuning Efficiency](https://qdrant.tech/articles/embedding-recycler/): Layer recycling, which involves caching outputs from frozen layers to avoid redundant computation during training and inference, can achieve significant speedups (~83%) with minimal performance loss, but its effectiveness and optimal percentage of layers to recycle are task-dependent. Experiments with Quaterion show that recycling 50% of layers often closely matches full fine-tuning performance, though results vary with task type and dataset size.
- [Fine Tuning Similar Cars Search](https://qdrant.tech/articles/cars-recognition/): Supervised classification is not suitable for tasks where class categories vary, are incomplete, or cannot be fully enumerated, which can be addressed by similarity learning, though it introduces challenges like larger batch sizes, complex loss functions, and different training/inference architectures. The Quaterion framework, built on PyTorch Lightning, facilitates similarity learning with modules for model training and inference, built-in loss functions, dataset handling, and an efficient workflow, as exemplified in a tutorial using the Stanford Cars dataset with novel-class generalization.
- [Metric Learning Tips & Tricks](https://qdrant.tech/articles/metric-learning-tips/): The article explains how metric learning, specifically the representation-based approach, can be used to train object matching models without labeled data by leveraging relative similarities and self-supervised datasets, enabling more scalable and flexible solutions than traditional classification—especially when dealing with a large or growing number of classes. Key methods include using embeddings, vector databases, and hard negative mining to effectively match objects (e.g., job positions and candidates) in production environments without manual labeling.
- [Metric Learning for Anomaly Detection](https://qdrant.tech/articles/detecting-coffee-anomalies/): Anomaly detection is challenging due to scarce and evolving data, making traditional supervised approaches costly and hard to maintain; however, a metric learning approach leveraging autoencoders and as few as 200 labeled samples achieved performance comparable to supervised classification trained on ~30,000 samples, offering a more efficient and scalable solution. This method allows for easier adaptation, reduced labeling requirements, and practical production deployment using vector search engines.
- [Triplet Loss - Advanced Intro](https://qdrant.tech/articles/triplet-loss/): Triplet Loss, introduced in the FaceNet paper, is a supervised metric learning loss that ensures an anchor sample is closer to a positive (same label) than to a negative (different label) by at least a margin, and is more tolerant of intra-class variance compared to Contrastive Loss. Efficient training with Triplet Loss relies on strategies like online triplet mining and computational tricks such as distance matrices and masking to select useful triplets and speed up calculations.
- [Neural Search 101: A Complete Guide and Step-by-Step Tutorial](https://qdrant.tech/articles/neural-search-tutorial/): Neural search uses neural networks to generate vector embeddings of queries and documents, enabling searches based on meaning rather than just keywords, which improves search performance for fuzzy or imprecise queries such as images, audio, or long text. The guide explains how to build a neural search system step-by-step, using pre-trained models for embedding (like sentence-transformers), and a vector search engine (like Qdrant) to store and retrieve relevant items.
- [Filtrable HNSW](https://qdrant.tech/articles/filtrable-hnsw/): Standard vector similarity libraries like Annoy, FAISS, and NMSLib offer fast approximate nearest neighbor search but have limited options for incorporating constraints during search. By modifying the HNSW algorithm to apply filters—such as categorical, numerical, or geographical constraints—during the search phase, it is possible to maintain graph connectivity and efficient search performance, with specific strategies suggested for each filtering type.
- [Introducing Qdrant 0.11](https://qdrant.tech/articles/qdrant-0-11-release/): Qdrant v0.11 introduces key features such as replication support for high availability and scalability, an administration API to control write operations, and an exact search parameter for improved validation, while maintaining backward compatibility for single node deployments but not for distributed setups. These enhancements focus on performance, reliability, and management capabilities.
- [Qdrant 0.10 released](https://qdrant.tech/articles/qdrant-0-10-release/): Qdrant 0.10 introduces major performance improvements and key features including support for storing multiple vectors per object in a single collection, batch vector search via a single API call, built-in ARM support with optimized Docker images, and new full-text filtering capabilities alongside keyword filters. These enhancements simplify multi-vector management, reduce network overhead, broaden platform compatibility, and improve search flexibility.
- [Vector Search in constant time](https://qdrant.tech/articles/quantum-quantization/): Quantum quantization leverages quantum computing to convert float32 vectors into entangled qbit vectors, enabling highly efficient vector search in artificial neural networks. By applying Grover's algorithm and a technique called transposition, this approach allows vector search on arbitrarily large databases to be performed in constant time.
- [How to choose an embedding model](https://qdrant.tech/articles/how-to-choose-an-embedding-model/): Choosing the right embedding model for vector search depends on multiple factors including search quality, language coverage, resource requirements, tokenizer compatibility, model size, sequence length, and support for optimization techniques, with no universal best model for all scenarios. The article emphasizes the importance of evaluating models on domain-specific data and tasks, understanding tokenizer effects, and building relevant ground truth datasets to make informed decisions.
- [Vector Search in Production](https://qdrant.tech/articles/vector-search-production/): Running vector search in production requires careful management of system resources (especially memory), optimizing indexing and search parameters, and leveraging strategies like data quantization to maintain high performance and reliability under real-world loads. Default configurations are often inadequate, so tuning settings, indexing relevant metadata, balancing speed and accuracy, and properly offloading or compressing data are essential for a resilient and efficient search infrastructure.
- [Semantic Cache: Accelerating AI with Lightning-Fast Data Retrieval](https://qdrant.tech/articles/semantic-cache-ai-data-retrieval/): Semantic cache is an AI optimization technique that stores answers to semantically similar queries—rather than just exact matches—to improve retrieval speed and reduce computational costs, especially in applications like Retrieval-Augmented Generation (RAG) systems. By leveraging vector databases like Qdrant, semantic cache retrieves responses for questions with similar meaning (even if phrased differently), providing scalability and cost-efficiency for question-answering and chatbot applications.
- [Full-text filter and index are already available!](https://qdrant.tech/articles/qdrant-introduces-full-text-filters-and-indexes/): Qdrant, an efficient vector database, now supports full-text filters in addition to keyword filtering, enabling more advanced search capabilities that can be combined with other filter types. Using a full-text index on filtered fields significantly improves query performance, offers customizable text tokenization options, and allows case-insensitive searches, making frequent queries more efficient.
- [Optimizing Semantic Search by Managing Multiple Vectors](https://qdrant.tech/articles/storing-multiple-vectors-per-object-in-qdrant/): Qdrant 0.10 introduced support for storing multiple vectors per object within a single collection, enabling efficient semantic search across various data types (e.g., image and text) without duplicating payloads. By configuring named vectors and using pretrained models to generate embeddings, users can flexibly search using different vector types for richer, more customizable search results.
- [Mastering Batch Search for Vector Optimization](https://qdrant.tech/articles/batch-vector-search-with-qdrant/): Qdrant 0.10.0 introduces a batch search feature that allows multiple vector searches in a single API call, greatly reducing network overhead and simplifying application code. Benchmarks showed that using batch search and multiprocessing together can speed up vector search operations by over 30% compared to sequential requests.
- [Vultr and Qdrant Hybrid Cloud Support Next-Gen AI Projects](https://qdrant.tech/blog/hybrid-cloud-vultr/): Qdrant and Vultr have partnered to offer a flexible, scalable, and secure deployment of Qdrant Hybrid Cloud on Vultr's global infrastructure, enabling rapid and customizable development of AI and ML applications with seamless vector search and centralized management. This integration streamlines deployment, optimizes performance and costs, and ensures data privacy while supporting global reach and compliance.
- [STACKIT and Qdrant Hybrid Cloud for Best Data Privacy](https://qdrant.tech/blog/hybrid-cloud-stackit/): Qdrant and STACKIT have launched Qdrant Hybrid Cloud, enabling developers to deploy a fully managed, scalable vector database on German data centers via STACKIT, ensuring full data privacy, GDPR compliance, and seamless integration. This solution empowers businesses to build secure, AI-driven applications—such as contract management platforms—while maintaining complete control over their sensitive data.
- [Qdrant Hybrid Cloud and Scaleway Empower GenAI](https://qdrant.tech/blog/hybrid-cloud-scaleway/): Qdrant and Scaleway have partnered to launch Qdrant Hybrid Cloud, a fully managed, scalable vector database that enables developers—especially in startups and Europe—to easily deploy advanced AI applications like RAG on Scaleway's secure and sustainable infrastructure while maintaining data sovereignty. This integration offers rapid setup, intuitive management, and AI-focused resources, making scalable vector search and modern AI workloads accessible and efficient for a broad range of users.
- [Red Hat OpenShift and Qdrant Hybrid Cloud Offer Seamless and Scalable AI](https://qdrant.tech/blog/hybrid-cloud-red-hat-openshift/): Qdrant has partnered with Red Hat to make its Hybrid Cloud vector database available on Red Hat OpenShift, allowing enterprises to easily deploy, scale, and securely manage AI-powered vector search workloads across hybrid cloud environments while maintaining full data sovereignty. This integration leverages OpenShift’s scalability, automation, and security features to simplify AI infrastructure for use cases like retrieval augmented generation and recommendation systems.
- [Qdrant and OVHcloud Bring Vector Search to All Enterprises](https://qdrant.tech/blog/hybrid-cloud-ovhcloud/): Qdrant Hybrid Cloud is now available as a fully managed vector database on OVHcloud, enabling European businesses to deploy AI-driven solutions quickly and securely within their existing, GDPR-compliant infrastructure while maintaining data sovereignty. The seamless integration offers simple setup, strong security, open ecosystem compatibility, and cost-efficient performance, with a tutorial provided to demonstrate building scalable recommendation systems entirely within OVHcloud.
- [New RAG Horizons with Qdrant Hybrid Cloud and LlamaIndex](https://qdrant.tech/blog/hybrid-cloud-llamaindex/): LlamaIndex and Qdrant have partnered to launch Qdrant Hybrid Cloud, enabling engineers and scientists to securely and flexibly develop, scale, and deploy advanced GenAI applications using Kubernetes-based architecture and robust vector search capabilities. This integration allows seamless, open-source, and scalable deployments, supports advanced hybrid semantic search, and includes comprehensive resources such as tutorials for building context-augmented AI solutions like document retrieval systems.
- [Developing Advanced RAG Systems with Qdrant Hybrid Cloud and LangChain](https://qdrant.tech/blog/hybrid-cloud-langchain/): LangChain and Qdrant have launched Qdrant Hybrid Cloud, a Kubernetes-based solution that enables secure, scalable deployment of GenAI applications with advanced vector search, seamless integration, and robust RAG (Retrieval-Augmented Generation) capabilities in any environment. This collaboration empowers organizations to efficiently build and deploy AI products, including complex QA systems, with open-source compatibility, enterprise features, and comprehensive documentation and tutorials.
- [Cutting-Edge GenAI with Jina AI and Qdrant Hybrid Cloud](https://qdrant.tech/blog/hybrid-cloud-jinaai/): Qdrant and Jina AI have partnered to launch Qdrant Hybrid Cloud, a Kubernetes-native solution that integrates Jina AI's advanced embedding models and APIs to enable rapid, flexible, and secure deployment of scalable Generative AI applications, particularly for Retrieval Augmented Generation (RAG) use cases. This collaboration provides seamless deployment, scalable and secure vector search, and cost efficiency, with comprehensive tutorials and documentation to help users quickly build and deploy AI-powered solutions.
- [Qdrant Hybrid Cloud and Haystack for Enterprise RAG](https://qdrant.tech/blog/hybrid-cloud-haystack/): Qdrant and Haystack have expanded their integration with the launch of Qdrant Hybrid Cloud, enabling developers to easily deploy production-ready, customizable Retrieval-Augmented Generation (RAG) AI applications with full data control in any environment. Their open-source, modular tools—now optimized with Haystack 2.0 and features like Hayhooks—simplify building, customizing, and deploying enterprise-grade AI solutions, with comprehensive tutorials and documentation available for rapid adoption.
- [Qdrant Hybrid Cloud and DigitalOcean for Scalable and Secure AI Solutions](https://qdrant.tech/blog/hybrid-cloud-digitalocean/): DigitalOcean and Qdrant have integrated to offer a managed vector database solution—Qdrant Hybrid Cloud—on DigitalOcean Kubernetes, enabling developers to efficiently deploy advanced AI applications while maintaining data privacy and control within their own infrastructure. This integration streamlines deployment, enhances flexibility, and makes it easy to build scalable AI-powered services such as semantic search and recommendation systems.
- [Enhance AI Data Sovereignty with Aleph Alpha and Qdrant Hybrid Cloud](https://qdrant.tech/blog/hybrid-cloud-aleph-alpha/): Aleph Alpha and Qdrant have partnered to offer a hybrid cloud vector database solution that prioritizes data sovereignty, security, and compliance—particularly for European enterprises—enabling organizations to securely deploy scalable, AI-powered applications within their own infrastructure. Their joint platform integrates advanced AI models, seamless scalability, and regional regulatory alignment, with resources and tutorials available to help businesses build secure, region-specific AI systems.
- [Elevate Your Data With Airbyte and Qdrant Hybrid Cloud](https://qdrant.tech/blog/hybrid-cloud-airbyte/): Airbyte and Qdrant have partnered to launch Qdrant Hybrid Cloud, the first managed vector database deployable on any environment, integrating Airbyte's data ingestion capabilities with Qdrant’s advanced indexing and search for scalable, private, and flexible AI data infrastructure. This collaboration enables seamless, up-to-date data pipelines for advanced GenAI and Retrieval Augmented Generation (RAG) applications, offering enterprise users flexible deployment, cost control, and open-source compatibility.
- [How PortfolioMind Delivered Real-Time Crypto Intelligence with Qdrant](https://qdrant.tech/blog/case-study-portfolio-mind/): PortfolioMind leveraged Qdrant’s multivector search capabilities to transform real-time user interactions and diverse crypto data into dynamic, personalized research insights, resulting in a 70% reduction in query latency, a 58% increase in interaction relevance, and a 22% improvement in user retention. Future enhancements include mapping cross-user interests, tracking changes in curiosity over time, and optimizing onboarding for new users.
- [Qdrant Edge: Vector Search for Embedded AI](https://qdrant.tech/blog/qdrant-edge/): Qdrant Edge is a lightweight, embedded vector search engine designed for on-device AI applications, enabling low-latency, multimodal search and retrieval in resource-constrained or offline environments such as robotics, mobile devices, and IoT agents. Now in private beta, it offers core Qdrant features re-architected for edge deployments, supporting local inference and decision-making where traditional cloud-based vector stores are unsuitable.
- [Qdrant for Research: The Story Behind ETH & Stanford’s MIRIAD Dataset](https://qdrant.tech/blog/miriad-qdrant/): Researchers from ETH Zurich and Stanford released MIRIAD, an open-source dataset containing 5.8 million high-quality, literature-grounded medical question-answer pairs designed to improve the reliability and reduce hallucinations in medical AI applications by serving as a structured knowledge base for Retrieval Augmented Generation (RAG). Built from 900,000 curated medical papers using a multi-stage filtering process and leveraging Qdrant for scalable retrieval, MIRIAD demonstrated significant improvements in benchmark accuracy and hallucination detection, and is freely available for the community with interactive exploration tools.
- [Qdrant 1.15 - Smarter Quantization & better Text Filtering](https://qdrant.tech/blog/qdrant-1.15.x/): Qdrant 1.15.0 introduces new 1.5- and 2-bit quantization modes and asymmetric quantization for improved vector compression and search accuracy, along with major text index upgrades including multilingual tokenization, configurable stopwords, stemming, and phrase matching. Additional optimizations include HNSW healing for faster index rebuilding and migration to Gridstore for increased data ingestion speed.
- [Qdrant joins AI Agent category on AWS Marketplace to accelerate Agentic AI development](https://qdrant.tech/blog/ai-agents-aws-marketplace/): Qdrant’s high-performance vector search engine is now available in the new AWS Marketplace AI Agents and Tools category, enabling customers to easily discover, purchase, and deploy enterprise AI agent solutions—such as real-time, step-aware reasoning and context retrieval—directly within their AWS environments. This integration streamlines procurement, enhances agent performance with advanced vector-native capabilities, and offers both managed SaaS and hybrid deployment options for scalability and compliance.
- [How &AI scaled global legal retrieval with Qdrant](https://qdrant.tech/blog/case-study-and-ai/): &AI uses Qdrant’s scalable vector database to power fast, accurate semantic search across billions of global patent documents, enabling legal professionals to efficiently invalidate patents and streamline litigation workflows. By focusing on retrieval over generation and leveraging cloud-native, multi-tenant infrastructure, &AI dramatically reduced DevOps overhead while delivering sub-second search performance and robust data privacy.
- [Introducing Qdrant Cloud Inference](https://qdrant.tech/blog/qdrant-cloud-inference-launch/): Qdrant Cloud Inference lets users generate, store, and index embeddings for text and images in a single API call within the Qdrant Cloud environment, eliminating the need for separate services and reducing complexity, latency, and costs for AI applications such as RAG, hybrid, and multimodal search. It supports multiple curated models, offers generous free monthly token allowances to paid users, is live on major cloud providers in US regions, and enables simplified, faster development with unified billing and easy activation.
- [Announcing Vector Space Day 2025 in Berlin](https://qdrant.tech/blog/vector-space-day-2025/): Vector Space Day 2025, hosted by Qdrant on September 26 in Berlin, is a full-day event for engineers, researchers, and AI builders to explore advancements in vector search, RAG pipelines, AI memory, and context engineering through talks, panels, workshops, and networking with industry leaders and emerging voices. The event features sessions on topics like vector databases, semantic search, agentic AI, and includes an after-party, with tickets available for €50 and participation from major organizations such as AWS and Microsoft.
- [How Pento modeled aesthetic taste with Qdrant](https://qdrant.tech/blog/case-study-pento/): The article describes a novel art recommendation system that uses Qdrant’s vector search and clustering (via HDBSCAN) to model individuals' evolving, multi-faceted artistic tastes, representing each user as a dynamic set of weighted clusters built from their artwork interactions. By embedding artworks and user preferences in semantic space, scoring clusters by recency and strength, and prioritizing the most representative clusters, the system connects users with others who share specific, nuanced tastes rather than simply suggesting popular items.
- [How Alhena AI unified its AI stack and improved ecommerce conversions with Qdrant](https://qdrant.tech/blog/case-study-alhena/): Alhena AI unified its previously fragmented vector search infrastructure by migrating to Qdrant Cloud, consolidating multiple backends into a single scalable, high-performance system. This enabled faster onboarding, reduced retrieval latency by 90%, improved ecommerce recommendations and conversions, simplified global deployments, and supported rapid scaling and innovation through features like multitenancy and hybrid search.
- [How GoodData turbocharged AI analytics with Qdrant](https://qdrant.tech/blog/case-study-gooddata/): GoodData has evolved from a traditional BI platform into an AI-powered, API-first analytics solution focused on accelerating insights and providing real-time, conversational data experiences, leveraging Qdrant’s scalable vector database for high-performance, multilingual semantic search and embedding updates. This transition enables fast, efficient AI-driven analytics at enterprise scale, positioning GoodData as a foundation for next-generation, embedded AI applications and advanced personalization features.
- [The Hitchhiker's Guide to Vector Search](https://qdrant.tech/blog/hitchhikers-guide/): Clelia, an Open Source Engineer at LlamaIndex, shares practical tips for building effective Retrieval Augmented Generation (RAG) pipelines, emphasizing the importance of clean text extraction, meaningful chunking, careful selection of dense and sparse embeddings (including hybrid search), and optimization strategies like semantic caching and binary quantization. The post draws from her hands-on experience to help improve the reliability, efficiency, and scalability of vector search-powered applications.
- [How FAZ unlocked 75 years of journalism with Qdrant](https://qdrant.tech/blog/case-study-faz/): Frankfurter Allgemeine Zeitung (FAZ) built a powerful semantic search system leveraging Qdrant and Azure OpenAI embeddings to make its 75-year archive of journalism deeply searchable, with robust support for rich metadata, rapid updates, and sub-second performance at scale. FAZ is now evolving toward a hybrid search architecture combining semantic, symbolic, and filtered queries to further enhance editorial research and user experience.
- [GraphRAG: How Lettria Unlocked 20% Accuracy Gains with Qdrant and Neo4j](https://qdrant.tech/blog/case-study-lettria-v2/): Lettria achieved a 20% increase in document retrieval accuracy for regulated industries by integrating Qdrant's vector search with Neo4j's knowledge graphs in a hybrid system, enabling more precise and explainable outputs than traditional vector-only RAG methods. Their solution addressed complex data ingestion, ontology generation, and transactional syncing challenges, ensuring reliability and consistency across both systems.
- [Vector Data Migration Tool](https://qdrant.tech/blog/beta-database-migration-tool/): Qdrant has released a beta Vector Data Migration Tool that enables easy, live batch migration of data between Qdrant instances or from other vector database providers with a single command, without needing node-specific snapshots. The tool supports flexible configuration changes during migration and can run as a container on any machine with access to the source and target databases.
- [How Lawme Scaled AI Legal Assistants and Significantly Cut Costs with Qdrant](https://qdrant.tech/blog/case-study-lawme/): Lawme.ai, a LegalTech startup, replaced their initial PGVector solution with Qdrant's vector database to better manage AI-driven legal workflows, resulting in a 75% reduction in infrastructure costs, faster query performance at scale, and enhanced compliance with strict data residency requirements. This migration enabled Lawme to confidently expand their AI legal assistant services while maintaining high security, operational simplicity, and meeting demanding client expectations.
- [How ConvoSearch Boosted Revenue for D2C Brands with Qdrant](https://qdrant.tech/blog/case-study-convosearch/): ConvoSearch, an AI-powered recommendation engine for e-commerce brands, switched from Pinecone to Qdrant for faster, more customizable vector search and robust metadata handling, reducing query latency from 50–100ms to 10ms and enabling real-time, hyper-personalized recommendations. This transition led to immediate business results, including a median 23–24% revenue uplift across clients and a 60% increase for The Closet Lover, while improving scalability and customer engagement.
- [LegalTech Builder's Guide: Navigating Strategic Decisions with Vector Search](https://qdrant.tech/blog/legal-tech-builders-guide/): LegalTech applications require highly accurate, nuanced search capabilities that traditional keyword methods cannot deliver, making vector search solutions like Qdrant essential for precision, scalability, and cost-efficiency. Qdrant addresses complex legal search needs with features like filterable HNSW, hybrid search, token-level reranking, domain-specific ranking logic, efficient scaling via GPU and quantization, and flexible deployment options for secure and responsive LegalTech development.
- [Qdrant Achieves SOC 2 Type II and HIPAA Certifications](https://qdrant.tech/blog/soc-2-type-ii-hipaa/): Qdrant has renewed its SOC 2 Type II certification and achieved HIPAA compliance, demonstrating a strong commitment to enterprise-grade security, confidentiality, and regulatory standards. These certifications, alongside advanced features like Single Sign-On, RBAC, and real-time monitoring, ensure the secure management of sensitive data for enterprises, including those in healthcare.
- [​​Introducing the Official Qdrant Node for n8n](https://qdrant.tech/blog/n8n-node/): Qdrant has released an official, fully-featured node for n8n, allowing users to integrate Qdrant’s advanced semantic and hybrid search capabilities directly into their n8n workflows without needing HTTP nodes, available for both cloud and self-hosted instances from version 1.94.0. This integration streamlines building robust solutions—such as RAG systems and agentic pipelines—by supporting features like batch upserts, hybrid queries, reranking, and customizable rescoring, with easy installation and ample community resources.
- [Qdrant + DataTalks.Club: Free 10-Week Course on LLM Applications](https://qdrant.tech/blog/datatalks-course/): A free 10-week online course, created in partnership with Alexey Grigorev and DataTalks.Club, teaches you how to build AI systems that answer questions about your knowledge base using LLMs, RAG, and vector search, with hands-on guidance from Qdrant experts. The course covers foundational and advanced topics, including semantic similarity search, hybrid and multi-vector search, reranking, and evaluation, and is open to everyone.
- [How Qovery Accelerated Developer Autonomy with Qdrant](https://qdrant.tech/blog/case-study-qovery/): Qovery integrated Qdrant’s high-performance vector database to power its AI-driven DevOps Copilot, enabling developers to autonomously manage complex infrastructure tasks in real time with minimal reliance on specialized DevOps expertise. This seamless integration accelerated deployment speeds, reduced operational overhead, and allowed Qovery to scale its services efficiently while focusing on continuous innovation.
- [How Tripadvisor Drives 2 to 3x More Revenue with Qdrant-Powered AI](https://qdrant.tech/blog/case-study-tripadvisor/): Tripadvisor is transforming its platform by leveraging Qdrant’s vector database and generative AI to turn its vast, unstructured user data into highly personalized, conversational travel experiences, such as the AI-powered Trip Planner. This shift has led to a 2–3x increase in revenue from users engaging with the AI features, and Tripadvisor plans to expand vector search and AI integration across its services.
- [Precision at Scale: How Aracor Accelerated Legal Due Diligence with Hybrid Vector Search](https://qdrant.tech/blog/case-study-aracor/): Aracor leveraged Qdrant’s scalable vector search technology to automate and dramatically accelerate legal due diligence workflows, achieving 90% faster processing, 70% reduced document turnaround time, and higher accuracy in legal data handling. Qdrant’s open-source, hybrid search capabilities enabled Aracor to efficiently process vast legal documents, deliver precise results, and continually innovate for complex, high-volume legal tasks.
- [How Garden Scaled Patent Intelligence with Qdrant](https://qdrant.tech/blog/case-study-garden-intel/): Garden, a startup revolutionizing patent analysis with large-scale AI, adopted Qdrant’s filterable vector search to efficiently manage and search hundreds of millions of patent vectors, enabling sub-100ms query latency, 10× cost reduction, and expansion of their addressable patent corpus from 20M to over 200M patents. This upgrade unlocked new business opportunities, such as high-confidence infringement detection, allowing clients to receive comprehensive analyses in minutes.
- [Exploring Qdrant Cloud Just Got Easier](https://qdrant.tech/blog/product-ui-changes/): Qdrant Cloud has introduced major user experience improvements, including streamlined login, effortless cluster creation with a free tier, enhanced cluster management tools (such as real-time monitoring, easy scaling, and improved UI), and a revamped Get Started page with comprehensive guides, tutorials, and community support to help users build and deploy vector search applications more easily. These updates aim to make the entire process—from initial setup to production deployment—simpler and more accessible for developers and teams.
- [How Pariti Doubled Its Fill Rate with Qdrant](https://qdrant.tech/blog/case-study-pariti/): Pariti transformed its talent-matching process by implementing Qdrant’s vector search, reducing résumé vetting time by 70%, boosting fill rates from 20% to 48%, and quadrupling candidate interview success—all while supporting rapid scalability and zero downtime. The new system enabled instant, accurate candidate shortlists from a 70,000-strong database, freeing analysts for higher-value work and positioning Pariti for further growth and client-facing innovation.
- [How Dust Scaled to 5,000+ Data Sources with Qdrant](https://qdrant.tech/blog/case-study-dust-v2/): Dust scaled its AI platform to handle over 5,000 data sources by migrating to Qdrant, leveraging features like multi-tenant collections and scalar quantization to dramatically improve search latency, reduce memory usage, and increase architectural flexibility. This overhaul resulted in a faster, more reliable, and cost-efficient system, enabling Dust’s AI agents to deliver consistently high performance even as data complexity and customer demand grew.
- [How SayOne Enhanced Government AI Services with Qdrant](https://qdrant.tech/blog/case-study-sayone/): SayOne switched from Pinecone to Qdrant for its government AI projects due to Qdrant’s superior performance, lower latency, easier deployment, better data sovereignty, and advanced capabilities, which proved crucial for meeting strict government requirements. As a result, SayOne achieved faster, more secure, and scalable AI solutions, leading to improved productivity and successful deployments across global public sector clients.
- [Beyond Multimodal Vectors: Hotel Search With Superlinked and Qdrant](https://qdrant.tech/blog/superlinked-multimodal-search/): Superlinked and Qdrant enable intelligent, multimodal search by converting natural language queries into specialized vector embeddings across different data types (e.g., text, price, rating), allowing for precise, customizable results in complex domains like hotel search. By leveraging distinct vector spaces and automatic query weighting, users can interact naturally while the system dynamically interprets preferences and efficiently retrieves relevant results.
- [Qdrant 1.14 - Reranking Support & Extensive Resource Optimizations](https://qdrant.tech/blog/qdrant-1.14.x/): Qdrant 1.14.0 introduces a Score-Boosting Reranker, enabling flexible result ranking by combining vector-based similarity with business logic (like metadata, recency, or location), along with incremental HNSW indexing for faster updates, optimized batch search via parallel processing, and improved CPU/disk resource and memory usage for large datasets. These updates make vector search more customized, efficient, and scalable for diverse real-world applications.
- [Pathwork Optimizes Life Insurance Underwriting with Precision Vector Search](https://qdrant.tech/blog/case-study-pathwork/): Pathwork revolutionized life insurance underwriting by implementing an AI-powered tool with Qdrant's vector search, leading to major accuracy gains (halving mean squared error) and faster processing (cutting latency from 9 to 2 seconds), which resulted in rapid user growth and significant improvements in underwriting precision and efficiency. Future plans include deeper carrier integration and continued advancements in accuracy and scalability, positioning Pathwork as an industry leader.
- [How Lyzr Supercharged AI Agent Performance with Qdrant](https://qdrant.tech/blog/case-study-lyzr/): Lyzr improved the performance, scalability, and cost-efficiency of its AI agent platform by switching from Weaviate and Pinecone to Qdrant, achieving over 90% faster query times, 2x faster data ingestion, and a 30% reduction in infrastructure costs while maintaining stability under heavy, real-world loads. This upgrade enabled Lyzr to handle increased agent concurrency and growing data demands, resulting in more accurate, low-latency retrieval for clients like NTT Data and NPD.
- [How Mixpeek Uses Qdrant for Efficient Multimodal Feature Stores](https://qdrant.tech/blog/case-study-mixpeek/): Mixpeek, a multimodal data processing platform, switched from MongoDB and Postgres to Qdrant as its feature store to support advanced hybrid retrieval, multi-vector indexing, and late interaction techniques, resulting in 40% faster query times and 80% less retriever code. Qdrant's vector-specialized capabilities streamlined feature extraction, improved scalability, and enabled efficient taxonomy, clustering, and multimodal retrieval for Mixpeek’s workflows.
- [Satellite Vector Broadcasting: Near-Zero Latency Retrieval from Space](https://qdrant.tech/blog/satellite-vector-broadcasting/): Qdrant has launched Satellite Vector Broadcasting, a groundbreaking system that uses a proprietary CubeSat constellation and inter-satellite laser relays to deliver near-zero latency vector search by processing queries in space rather than on Earth. This technology enables ultra-fast retrieval—experimental CubeSat swarms achieve 4 ms average latency—and introduces innovative features like Broadcast-to-Index Protocol, dynamic clustering, and plans for planetary-scale search expansion.
- [HubSpot & Qdrant: Scaling an Intelligent AI Assistant](https://qdrant.tech/blog/case-study-hubspot/): HubSpot selected Qdrant as its vector database to power Breeze AI, enabling highly personalized, real-time responses and scalable, efficient retrieval of relevant data to meet growing user demands. This integration has accelerated development, improved customer engagement, and positioned HubSpot’s AI infrastructure for future advancements and continued scalability.
- [Vibe Coding RAG with our MCP server](https://qdrant.tech/blog/webinar-vibe-coding-rag/): The webinar tested popular AI coding assistants—Cursor, GitHub Copilot, Aider, and Claude Code—by using “vibe coding,” a development approach that relies on AI for implementation while developers focus on desired outcomes and context. By integrating these tools with the Model Context Protocol and a Qdrant semantic memory server, the session demonstrated the potential and limitations of AI agents in handling complex tasks like Retrieval Augmented Generation, showing that context-aware coding is increasingly feasible though tool selection depends on specific needs.
- [How Deutsche Telekom Built a Multi-Agent Enterprise Platform Leveraging Qdrant](https://qdrant.tech/blog/case-study-deutsche-telekom/): Deutsche Telekom developed LMOS, an open-source, scalable multi-agent AI PaaS, to efficiently power sales and customer service chatbots across ten European countries, addressing complex challenges in data segregation, context sharing, and agent collaboration. To ensure seamless integration, high performance, and operational simplicity, they chose Qdrant as their vector database, enabling over 2 million AI-driven conversations and setting a new standard for enterprise AI deployment.
- [Introducing Qdrant Cloud’s New Enterprise-Ready Vector Search](https://qdrant.tech/blog/enterprise-vector-search/): Qdrant Cloud introduces enterprise-grade features—including a Cloud API with Terraform support, Cloud RBAC, Single Sign-On (SSO), granular Database API Keys, and advanced monitoring integrations—to help developers securely manage, scale, and monitor AI workloads with fine-grained access control and real-time observability. These enhancements simplify authentication and management, enforce security policies, and provide deep visibility into database performance, making Qdrant Cloud ready for compliant, large-scale AI applications.
- [Metadata automation and optimization - Reece Griffiths | Vector Space Talks](https://qdrant.tech/blog/metadata-deasy-labs/): Metadata is crucial for optimizing vector search and retrieval-augmented generation, enabling improved segmentation, enrichment, filtering, and access control, which dramatically boosts retrieval accuracy. Reece Griffiths, CEO of Deasy Labs, highlights how automating and scaling metadata extraction and classification through LLMs can bridge the gap to high-performance search systems.
- [How to Build Intelligent Agentic RAG with CrewAI and Qdrant](https://qdrant.tech/blog/webinar-crewai-qdrant-obsidian/): The live session demonstrated how to integrate CrewAI, Qdrant, and Obsidian to create an agentic RAG (Retrieval-Augmented Generation) system for semi-automating email responses by connecting to Gmail and using Obsidian notes as an up-to-date knowledge base. The system leverages CrewAI’s multi-agent framework and custom Qdrant integration to manage entity and short-term memory, automate knowledge updates, and generate contextually relevant email drafts.
- [Qdrant 1.13 - GPU Indexing, Strict Mode & New Storage Engine](https://qdrant.tech/blog/qdrant-1.13.x/): Qdrant 1.13.0 introduces GPU-accelerated, vendor-agnostic HNSW indexing for dramatically faster vector search on any major GPU, along with Strict Mode for operational control that enforces limits on resource usage and prevents system overload in multi-tenant or distributed environments. Additional features include HNSW graph compression, named vector filtering, and custom storage for efficient handling of payloads and sparse vectors.
- [Voiceflow & Qdrant: Powering No-Code AI Agent Creation with Scalable Vector Search](https://qdrant.tech/blog/case-study-voiceflow/): Voiceflow enables enterprises to build AI agents through a no-code, customizable platform, and after evaluating vector databases for their managed RAG solution, they switched from Pinecone to Qdrant due to its advanced scaling, security, metadata tagging, and responsive support. Leveraging Qdrant's features—such as quantization, robust infrastructure, and efficient metadata management—Voiceflow achieved enhanced scalability, performance, and operational efficiency, and plans to continue expanding filtering and customization capabilities for its users.
- [Building a Facial Recognition System with Qdrant](https://qdrant.tech/blog/facial-recognition/): The Twin Celebrity app matches users to their celebrity look-alike by analyzing selfies with a ResNet-based FaceNet model, storing facial embeddings in the Qdrant vector database, and delivering results via a Streamlit interface. The project demonstrates a scalable embedding search workflow using tools like ZenML for pipelines and MTCNN for face alignment, with practical deployment and optimization tips for developers.
- [Optimizing ColPali for Retrieval at Scale, 13x Faster Results](https://qdrant.tech/blog/colpali-qdrant-optimization/): ColPali, a precise document retrieval tool for visually rich PDFs, faced major computational challenges when scaling due to the generation of over a thousand vectors per page; this was overcome by introducing a two-stage retrieval process using row-wise pooling to reduce vectors followed by reranking, resulting in a 13x speedup with minimal loss in precision (NDCG@20 = 0.952, Recall@20 = 0.917) when using mean pooling. The experiment confirms that mean pooling effectively maintains ColPali’s accuracy while drastically improving efficiency, and further optimizations—like column pooling or quantization—are being explored.
- [Best Practices in RAG Evaluation: A Comprehensive Guide](https://qdrant.tech/blog/rag-evaluation-guide/): This guide covers how to evaluate RAG (Retrieval-Augmented Generation) systems for accuracy and quality by assessing search precision, recall, contextual relevance, and response accuracy, and highlights the importance of using proper data ingestion, embedding models, and optimized retrieval strategies. It also introduces three popular evaluation frameworks—Ragas, Quotient AI, and Arize Phoenix—to measure and visualize key system metrics, and discusses common issues that affect RAG performance and their solutions.
- [Empowering QA.tech’s Testing Agents with Real-Time Precision and Scale](https://qdrant.tech/blog/case-study-qatech/): QA.tech, specializing in AI-driven automated web application testing, developed browser-based testing agents that require fast, real-time analysis and decision-making supported by custom embeddings. After hitting scalability and performance limits with pgvector, QA.tech adopted Qdrant for its efficient, batchable, and multi-embedding vector search capabilities, enabling responsive high-velocity action handling and reliable AI agent performance.
- [Advanced Retrieval with ColPali & Qdrant Vector Database](https://qdrant.tech/blog/qdrant-colpali/): ColPali is a multimodal document retrieval approach that leverages Vision Language Models (VLMs) to generate multi-vector embeddings directly from document images, effectively capturing both visual and textual information and outperforming traditional text/OCR-based methods, particularly on complex, visually rich documents. Integrated with Qdrant and optimized using Binary Quantization, ColPali enables significantly faster and more efficient search without compromising accuracy, as demonstrated on challenging datasets.
- [How Sprinklr Leverages Qdrant to Enhance AI-Driven Customer Experience Solutions](https://qdrant.tech/blog/case-study-sprinklr/): Sprinklr, a leader in unified customer experience management, adopted Qdrant as its vector database to power AI-driven applications such as FAQ bots and advanced analytics, citing Qdrant's speed, developer-friendly customization, cost efficiency, and superior write and latency performance over alternatives like Elasticsearch. This transition led to a 30% reduction in retrieval infrastructure costs, improved developer efficiency, and enabled scalable, real-time AI capabilities across Sprinklr’s product suites.
- [Qdrant 1.12 - Distance Matrix, Facet Counting & On-Disk Indexing](https://qdrant.tech/blog/qdrant-1.12.x/): Qdrant 1.12.0 introduces a Distance Matrix API for efficient pairwise vector distance calculations, a GUI for visual data exploration, a Faceting API for dynamic metadata aggregation, and support for storing both text and geo indexes on disk to improve memory efficiency. These features enhance clustering/clustering tasks, enable flexible field-based aggregation, offer improved dataset visualization, and reduce in-memory resource requirements.
- [New DeepLearning.AI Course on Retrieval Optimization: From Tokenization to Vector Quantization](https://qdrant.tech/blog/qdrant-deeplearning-ai-course/): DeepLearning.AI, in collaboration with Qdrant, has launched a free, beginner-friendly, one-hour online course led by Kacper Łukawski, focused on retrieval optimization techniques such as tokenization, vector quantization, and enhancing vector search in applications. The course is designed for anyone with basic Python knowledge and aims to equip learners with practical skills to build and optimize Retrieval-Augmented Generation (RAG) applications.
- [Introducing Qdrant for Startups](https://qdrant.tech/blog/qdrant-for-startups-launch/): Qdrant for Startups is a new initiative providing early-stage AI startups with discounted Qdrant Cloud services, technical guidance, co-marketing opportunities, partner tool discounts (such as Hugging Face and LlamaIndex), and community support to help them scale AI-driven products efficiently. Eligible pre-seed to Series A startups can apply online, with selection based on innovation potential and alignment with Qdrant’s capabilities.
- [Qdrant and Shakudo: Secure & Performant Vector Search in VPC Environments](https://qdrant.tech/blog/case-study-shakudo/): Qdrant has partnered with Shakudo to offer Qdrant Hybrid Cloud as a fully managed, high-performance vector database within Shakudo’s virtual private cloud (VPC) deployments, enabling enterprises to maintain data sovereignty and privacy while achieving scalable, low-latency vector search for AI applications. This integration delivers seamless compatibility with existing data stacks, Kubernetes-based automation, and robust security, making it ideal for enterprises needing efficient, compliant, and flexible AI infrastructure.
- [Data-Driven RAG Evaluation: Testing Qdrant Apps with Relari AI](https://qdrant.tech/blog/qdrant-relari/): Qdrant and Relari have partnered to simplify the evaluation of Retrieval-Augmented Generation (RAG) systems by combining Qdrant’s vector database capabilities with Relari’s experiment and metrics tools, enabling fast, data-driven, and iterative RAG performance assessments using methods like Top-K Parameter Optimization and Auto Prompt Optimization. This approach streamlines development by allowing developers to easily benchmark and improve their RAG applications with reliable metrics and real-world datasets, as demonstrated with a GitLab legal policies case study.
- [Nyris & Qdrant: How Vectors are the Future of Visual Search](https://qdrant.tech/blog/case-study-nyris/): Nyris, founded in 2015, delivers advanced visual search technology for enterprises—enabling rapid and accurate identification of products and spare parts via images, synthetic data, and CAD-generated visuals, with prominent clients including IKEA and Trumpf. To achieve high-performance, scalable, and accurate vector search, Nyris selected Qdrant as their dedicated vector search engine for its speed, flexibility, cost-effectiveness, data sovereignty, and innovative features, positioning themselves at the forefront of next-generation product search.
- [Kern AI & Qdrant: Precision AI Solutions for Finance and Insurance](https://qdrant.tech/blog/case-study-kern/): Kern AI developed a low-code, data-centric AI platform that helps financial and insurance companies, such as Markel Insurance SE, drastically reduce customer service response times by leveraging advanced vector search and integration of large language models; using Qdrant's open-source vector database, Kern AI achieved <1% hallucination rates and streamlined claims management and support workflows. The company plans to further expand its use of Qdrant to enhance data handling and chatbot accuracy across the industry.
- [Qdrant 1.11 - The Vector Stronghold: Optimizing Data Structures for Scale and Efficiency](https://qdrant.tech/blog/qdrant-1.11.x/): Qdrant 1.11.0 introduces major improvements in memory usage and segment optimization, including defragmented multitenant storage, on-disk payload indexing for less frequently used data, and new query features such as GroupBy, random sampling, and hybrid search fusion. The release also offers enhanced web UI tools for search evaluation and graph exploration, providing better scalability and efficiency for large, multi-tenant datasets.
- [Kairoswealth & Qdrant: Transforming Wealth Management with AI-Driven Insights and Scalable Vector Search](https://qdrant.tech/blog/case-study-kairoswealth/): Kairoswealth, a comprehensive wealth management platform, adopted Qdrant as its vector database to overcome challenges with scalability, performance, and memory efficiency in use cases such as product recommendations and regulatory report generation. The switch enabled consistent high performance, reduced infrastructure costs, access to advanced features, and strong support, positioning Kairoswealth to drive further innovation with AI in the financial sector.
- [Qdrant 1.10 - Universal Query, Built-in IDF & ColBERT Support](https://qdrant.tech/blog/qdrant-1.10.x/): Qdrant 1.10.0 introduces a universal Query API that unifies all search types (including Hybrid Search and ColBERT multivector support) into a single endpoint, allowing users to flexibly combine query strategies with simple parameters. The update also adds built-in IDF support for search and indexing, automating TF-IDF/BM25 calculations within the engine and removing the need for external computation or updates when documents change.
- [Community Highlights #1](https://qdrant.tech/blog/community-highlights-1/): The first edition of Community Highlights showcases outstanding projects and guides on vector search technologies, celebrates Pavan Kumar as Creator of the Month for his multiple contributions, and recognizes the top three most active community members. Community members are also invited to join the upcoming Office Hours on Discord for learning and networking.
- [Response to CVE-2024-3829: Arbitrary file upload vulnerability](https://qdrant.tech/blog/cve-2024-3829-response/): A security vulnerability (CVE-2024-3829) affecting Qdrant versions before v1.9.0 allows attackers to upload arbitrary files that could enable remote code execution, but is mitigated in cloud deployments due to read-only filesystem and authentication. Users should confirm their Qdrant version and upgrade to at least v1.9.0 following appropriate installation or update procedures, though no action is required for cloud users.
- [Qdrant Attains SOC 2 Type II Audit Report](https://qdrant.tech/blog/qdrant-soc2-type2-audit/): Qdrant has successfully completed the SOC 2 Type II audit, demonstrating effective security, availability, and confidentiality controls for customer data from January 1 to April 7, 2024, with no exceptions noted. The company will continue annual audits to maintain compliance and underscores its commitment to top-tier data protection.
- [Introducing Qdrant Stars: Join Our Ambassador Program!](https://qdrant.tech/blog/qdrant-stars-announcement/): Qdrant has launched "Qdrant Stars," an ambassador program to honor and support impactful users advancing AI and vector search through projects, educational efforts, and community engagement. The inaugural group includes diverse experts—such as Robert Caulk, Joshua Mo, Nicholas Khami, Owen Colegrove, Kameshwara Pavan Kumar Mantha, and Niranjan Akella—who contribute through innovative applications, content creation, and fostering community collaboration.
- [Intel’s New CPU Powers Faster Vector Search](https://qdrant.tech/blog/qdrant-cpu-intel-benchmark/): Intel’s 5th generation Xeon processors, particularly "Emerald Rapids," significantly boost vector search performance and energy efficiency compared to previous models, making them ideal for enterprise-scale AI/ML workloads with vector databases like Qdrant. Qdrant, optimized for these new CPUs, enables efficient, large-scale semantic search for modern AI applications and recommends the latest Intel CPUs for cost-effective, high-performance deployments.
- [QSoC 2024: Announcing Our Interns!](https://qdrant.tech/blog/qsoc24-interns-announcement/): Qdrant has selected Jishan Bhattacharya and Celine Hoang as interns for its inaugural Summer of Code program; Jishan will develop a WASM-based dimension reduction visualization in Rust, while Celine will port advanced ranking models to ONNX in Python. These projects aim to improve Qdrant's data visualization capabilities and model support for applications like recommendation engines and search.
- [Are You Vendor Locked?](https://qdrant.tech/blog/are-you-vendor-locked/): Vendor lock-in—particularly around expensive AI hardware like GPUs—is a growing concern as cloud costs and feature differentiation complicate infrastructure decisions; to remain flexible and manage costs, businesses should embrace cloud-agnostic solutions like Kubernetes and consider hybrid cloud options, such as Qdrant Hybrid Cloud, which enable seamless migration and integration across cloud providers. Ultimately, balancing speed, cost, and flexibility is essential, and leveraging open and standardized tools can help organizations avoid costly dependencies.
- [Visua and Qdrant: Vector Search in Computer Vision](https://qdrant.tech/blog/case-study-visua/): VISUA, a leader in computer vision data analysis, adopted Qdrant's vector database to scale and automate its quality control and anomaly detection processes, achieving 40x faster query speeds and 10x greater scalability in data review and reinforcement learning. This integration not only improved operational efficiency but also enabled VISUA to explore new applications such as content moderation and expanded copyright infringement detection.
- [Qdrant 1.9.0 - Heighten Your Security With Role-Based Access Control Support](https://qdrant.tech/blog/qdrant-1.9.x/): Qdrant 1.9.0 introduces key enterprise features such as granular access control via JSON Web Tokens, much faster shard transfer for improved recovery, and native support for uint8 embeddings for significant memory and performance gains. Additional improvements include better write performance, enhanced sparse vector search, and various optimizations for more stable, secure, and efficient large-scale deployments.
- [Qdrant's Trusted Partners for Hybrid Cloud Deployment](https://qdrant.tech/blog/hybrid-cloud-launch-partners/): Qdrant Hybrid Cloud is a managed vector database that can be deployed in any environment—including cloud, on-premise, or edge—providing developers with seamless integration into modern AI stacks, full data sovereignty, and support from major industry partners. Comprehensive tutorials and documentation demonstrate its flexibility in building secure, production-ready AI applications across diverse platforms and use cases.
- [Qdrant Hybrid Cloud: the First Managed Vector Database You Can Run Anywhere](https://qdrant.tech/blog/hybrid-cloud/): Qdrant has launched Qdrant Hybrid Cloud, the first managed, Kubernetes-native vector database that can be deployed across any environment (cloud, on-premise, or edge), offering organizations unparalleled data sovereignty, privacy, and deployment flexibility. Through partnerships with major cloud providers and AI tool leaders, plus features like database isolation, scalable and secure architecture, and effortless setup, Qdrant Hybrid Cloud empowers businesses to easily build and scale AI applications with full control over their data.
- [Advancements and Challenges in RAG Systems - Syed Asad | Vector Space Talks](https://qdrant.tech/blog/rag-advancements-challenges/): Syed Asad, an AI/ML expert at Kiwi Tech, highlights that while many vector databases are scalable, their lack of user-friendliness sets them back, which is why he prefers Qdrant for vector search operations. In his podcast episode, Syed discusses advancements and challenges in AI, particularly in retrieval-augmented generation (RAG) systems, and shares insights on topics like multimodal AI, semantic job matching, privacy concerns, and building engaging, multimedia-rich applications.
- [Building Search/RAG for an OpenAPI spec - Nick Khami | Vector Space Talks](https://qdrant.tech/blog/building-search-rag-open-api/): Nick Khami, founder of Trieve, emphasized how Trieve and Qdrant make building precise search and recommendation systems over Open API specs simple and efficient, highlighting the advantages of Qdrant's group-based system. The episode explores key topics such as leveraging Open API specs, simplifying vector search infrastructure, and enhancing search relevance and analytics for diverse applications.
- [Iveta Lohovska on Gen AI and Vector Search | Qdrant](https://qdrant.tech/blog/gen-ai-and-vector-search/): Iveta Lohovska, Chief Technologist at HPE, highlights the importance of trustworthiness, transparency, and explainability in generative AI and vector search, especially for high-stakes sectors like government and energy, emphasizing rigorous data sourcing, source traceability, and data privacy. She also discusses the challenges of enterprise adoption, the need for on-premises control, and the evolving landscape of open-source and specialized AI model development.
- [Teaching Vector Databases at Scale - Alfredo Deza | Vector Space Talks](https://qdrant.tech/blog/teaching-vector-db-at-scale/): Alfredo Deza, a developer advocate and professor, chooses Qdrant for teaching vector databases because it is easy to set up and straightforward to use, which simplifies both teaching and student learning of complex topics. Drawing on his background as an Olympic athlete, Alfredo emphasizes the value of consistency and simplicity in both education and technology, advocating for engaging materials and up-to-date platforms to enhance the AI learning experience.
- [How to meow on the long tail with Cheshire Cat AI? - Piero and Nicola |](https://qdrant.tech/blog/meow-with-cheshire-cat/): Cheshire Cat AI, an open-source framework with strong community support, primarily uses Qdrant as its default vector database in multiple forms (file, container, cloud), leveraging features like quantization to enhance search accuracy and performance while optimizing memory usage. Founders Piero Savastano and Nicola Procopio highlighted Cheshire Cat’s plugin ecosystem, growing international adoption, and active Discord community, as well as its future plans for a cloud version and user-generated plugin marketplace.
- [Response to CVE-2024-2221: Arbitrary file upload vulnerability](https://qdrant.tech/blog/cve-2024-2221-response/): A security vulnerability (CVE-2024-2221) affecting Qdrant versions before 1.9.0 allows attackers to upload arbitrary files and potentially execute remote code, but this does not significantly impact cloud deployments due to read-only filesystems and enabled authentication. Users should check their Qdrant version and upgrade to at least v1.9.0 to ensure protection against this vulnerability.
- [Introducing FastLLM: Qdrant’s Revolutionary LLM](https://qdrant.tech/blog/fastllm-announcement/): Qdrant has launched FastLLM (FLLM), a lightweight Language Model optimized for Retrieval Augmented Generation (RAG) applications, boasting a groundbreaking 1 billion token context window and seamless integration with Qdrant for large-scale AI-driven content generation. Featuring a mixture-of-experts architecture with 1 trillion parameters and industry-leading performance benchmarks, FastLLM is now available in Early Access for developers.
- [VirtualBrain: Best RAG to unleash the real power of AI - Guillaume](https://qdrant.tech/blog/virtualbrain-best-rag/): Guillaume Marquis, CTO and Co-Founder of VirtualBrain, highlights the necessity of using a fast, scalable vector database like Qdrant to efficiently handle large-scale document retrieval and AI tasks for commercial proposal drafting. He emphasizes the importance of open source tools, system scalability, precision in search, and user experience in minimizing hallucinations, positioning VirtualBrain as an innovative solution for deep AI-powered data work.
- [Talk with YouTube without paying a cent - Francesco Saverio Zuppichini |](https://qdrant.tech/blog/youtube-without-paying-cent/): Francesco Saverio Zuppichini, a Senior Full Stack Machine Learning Engineer, highlights Qdrant as his favorite vector database because it is private, user-controlled, and easy to run locally, which supports better data privacy and ownership. In a recent podcast episode, he discusses building RAGs from YouTube content, practical coding tips, and criteria for selecting effective software tools.
- [Qdrant is Now Available on Azure Marketplace!](https://qdrant.tech/blog/azure-marketplace/): Qdrant, a high-performance and scalable open-source vector database, is now officially available on Azure Marketplace, enabling seamless integration with Azure’s ecosystem for rapid, enterprise-scale AI applications such as retrieval-augmented generation (RAG) and vector search. This partnership allows users to quickly deploy Qdrant clusters on Azure for efficient handling of billion-vector datasets, low-latency querying, and flexible scaling, all within Microsoft’s trusted cloud infrastructure.
- [Production-scale RAG for Real-Time News Distillation - Robert Caulk |](https://qdrant.tech/blog/real-time-news-distillation-rag/): Robert Caulk, founder of Emergent Methods, discusses how his team leverages open-source tools like Qdrant to model and distill over a million news articles daily, aiming to reduce media bias and improve news awareness through scalable context engineering and advanced semantic search. He highlights the importance of modular, robust infrastructure and startups’ agility in adopting new technologies for more reliable and efficient real-time news distribution.
- [Insight Generation Platform for LifeScience Corporation - Hooman](https://qdrant.tech/blog/insight-generation-platform/): Hooman Sedghamiz, a leader in AI/ML at Bayer AG, discusses the rapid growth in vector databases and highlights a lack of innovation in data pipelines supporting retrieval augmented generation, while emphasizing the importance of real-time, cost-effective, and customized evaluation methods for maintaining high-quality AI chatbot interactions. The episode delves into strategies for ensuring chatbot integrity, efficient evaluation using smaller models, tailored metrics, and advances in large language models, offering insights from Sedghamiz's extensive experience advancing AI in life sciences.
- [The challenges in using LLM-as-a-Judge - Sourabh Agrawal | Vector Space Talks](https://qdrant.tech/blog/llm-as-a-judge/): Sourabh Agrawal emphasizes that using expensive models like GPT-4 for evaluating LLM outputs is not cost-effective, advocating for smaller, cheaper models instead. As CEO of UpTrain AI, he is developing an open-source tool to systematically evaluate, test, and monitor LLM applications by providing automated insights, root-cause analysis, and suggestions for improvement.
- [Vector Search for Content-Based Video Recommendation - Gladys and Samuel](https://qdrant.tech/blog/vector-search-vector-recommendation/): Dailymotion's Machine Learning Engineers chose Qdrant as their vector search engine for its ability to handle technical requirements, rapid neighbor search, user-friendly Python API, and cost-effective implementation, helping them efficiently scale their video recommendation system and improve performance with features like handling low-signal content. Additionally, Qdrant’s responsive support and streaming integration enabled Dailymotion to overcome cold start issues, resulting in a threefold increase in click-through rates for new or less-interacted videos.
- [Integrating Qdrant and LangChain for Advanced Vector Similarity Search](https://qdrant.tech/blog/using-qdrant-and-langchain/): Qdrant and LangChain simplify building scalable, production-ready AI applications by combining vector search (RAG) for long-term memory with unified development interfaces, enhancing retrieval accuracy and reducing hallucinations in LLM-based systems. This integration streamlines workflows for use cases like chatbots, recommendation systems, and data analysis, enabling developers to focus on value rather than infrastructure complexity.
- [IrisAgent and Qdrant: Redefining Customer Support with AI](https://qdrant.tech/blog/iris-agent-qdrant/): IrisAgent, founded by Palak Dalal Bhatia, leverages advanced AI and generative AI to automate and optimize customer support by integrating cross-functional data, detecting customer intent, and improving response efficiency. Their adoption of the open-source vector database Qdrant further enhances their AI pipeline’s performance, scalability, and data security, supporting future innovations such as automated knowledge base content generation for improved self-service and customer experience.
- [Dailymotion's Journey to Crafting the Ultimate Content-Driven Video Recommendation Engine with Qdrant Vector Database](https://qdrant.tech/blog/case-study-dailymotion/): Dailymotion improved its video recommendation engine by implementing a content-based system using Qdrant’s vector database, enabling fast, scalable, and diverse recommendations from over 420 million videos. This solution overcame previous limitations—like slow, popularity-biased recommendations—by leveraging advanced vector embeddings and efficient similarity search, resulting in better real-time user experiences across their platforms.
- [Qdrant vs Pinecone: Vector Databases for AI Apps](https://qdrant.tech/blog/comparing-qdrant-vs-pinecone-vector-databases/): Qdrant and Pinecone are leading vector databases designed for storing and searching high-dimensional data critical to modern AI applications, addressing limitations of traditional databases in scalability, performance, and search efficiency. Qdrant, in particular, stands out with its Rust-based architecture, advanced similarity search, flexible deployment, strong security, and integration with popular machine learning frameworks, making it well-suited for enterprise-scale AI workloads.
- [What is Vector Similarity? Understanding its Role in AI Applications.](https://qdrant.tech/blog/what-is-vector-similarity/): Vector similarity enables AI applications to understand and retrieve relevant information from unstructured data (like text, images, or audio) by converting it into high-dimensional vectors and comparing their proximity using metrics like cosine similarity or Euclidean distance. This approach powers advanced search, recommendation systems, image and text analysis, and natural language processing by quantifying how closely related different data points are within vector space.
- [DSPy vs LangChain: A Comprehensive Framework Comparison" #required](https://qdrant.tech/blog/dspy-vs-langchain/): The rise of powerful LLMs and vector stores has led to frameworks like LangChain, which simplify building complex AI applications by providing modular components for model interfacing, retrieval, and composition, supporting rapid prototyping and integration with various data sources and models. However, while LangChain streamlines many aspects, prompt engineering and multi-stage reasoning still require expertise, a challenge recent frameworks like DSPy aim to address by automating prompt optimization.
- [Qdrant Summer of Code 24](https://qdrant.tech/blog/qdrant-summer-of-code-24/): Qdrant, although not accepted into Google Summer of Code 2024, is launching its own Qdrant Summer of Code program following GSoC's timeline and rules, offering stipends for contributors to work on projects in Rust and Python, such as WASM-based visualization, efficient BM25 ranking, ONNX cross encoders, ranking fusion algorithms, and distributed system testing. Interested contributors can apply by email starting March 18th, with full project details available on Qdrant's Notion page.
- [Dust and Qdrant: Using AI to Unlock Company Knowledge and Drive Employee Productivity](https://qdrant.tech/blog/dust-and-qdrant/): Dust, a French company co-founded by ex-OpenAI engineer Stanislas Polu, empowers businesses to build context-aware AI assistants using large language models (LLMs) through retrieval augmented generation (RAG), addressing the challenge of diverse, fluid enterprise use cases and limited tuning data by integrating company knowledge from various SaaS tools. By using Qdrant as their open-source vector database, Dust efficiently scaled their platform, reduced costs by 2x with advanced memory management and quantization, and maintained high performance and accuracy in AI-powered team productivity tools.
- [The Bitter Lesson of Retrieval in Generative Language Model Workflows -](https://qdrant.tech/blog/bitter-lesson-generative-language-model/): Dr. Mikko Lehtimäki, co-founder and chief data scientist at Softlandia, discusses the "bitter lesson" that machine learning approaches leveraging large-scale data and computation eventually outperform hand-crafted solutions, and how this principle shapes the development of Yokot AI's retrieval-augmented generation workflows. He emphasizes the importance of efficient data handling, especially re-ranking, for improving large language model outputs, and shares practical insights from Softlandia’s innovative LLM platform.
- [Indexify Unveiled - Diptanu Gon Choudhury | Vector Space Talks](https://qdrant.tech/blog/indexify-content-extraction-engine/): Diptanu Gon Choudhury, founder of Tensorlake, is developing Indexify, an open-source engine that enables scalable, near-real-time structured extraction from unstructured data to support AI agent-driven workflows and improve knowledge bases. In a recent podcast, he discusses how innovative data infrastructure like Indexify is transforming enterprise AI applications by enabling real-time indexing, efficient agent workflows, and optimized experiences for developers and call centers.
- [Unlocking AI Potential: Insights from Stanislas Polu](https://qdrant.tech/blog/qdrant-x-dust-vector-search/): Stanislas Polu, co-founder of Dust and former Stripe and OpenAI engineer, discusses how his company uses Qdrant’s open-source, Rust-based vector database for its strong performance and control, enabling efficient enterprise data management and the deployment of AI-powered productivity assistants across SaaS platforms. He emphasizes prioritizing product-layer innovation—focusing on practical integration of large language models through tailored AI assistants and workflows—over model training to better augment and streamline daily work.
- [Announcing Qdrant's $28M Series A Funding Round](https://qdrant.tech/blog/series-a-funding-round/): Qdrant has raised $28M in Series A funding led by Spark Capital to accelerate its mission of building a highly efficient, scalable, and high-performance vector database, already demonstrated by rapid user growth and over 5 million downloads. With advanced features like custom search algorithms, flexible deployment options, and a strong focus on performance, Qdrant aims to empower enterprises to build cutting-edge AI applications using its open-source platform.
- [Introducing Qdrant Cloud on Microsoft Azure](https://qdrant.tech/blog/qdrant-cloud-on-microsoft-azure/): Qdrant Cloud, the managed vector database, is now available on Microsoft Azure, enabling users to quickly deploy scalable clusters and manage billion-vector datasets with ease. This expansion supports rapid application development and enterprise-scale AI solutions on Azure’s infrastructure.
- [Qdrant Updated Benchmarks 2024](https://qdrant.tech/blog/qdrant-benchmarks-2024/): Qdrant has updated its open-source benchmark to better compare vector search engines, featuring notable performance improvements across all engines, a new dataset with 1 million OpenAI embeddings, and separate evaluations for latency and requests-per-second. The benchmarking principles remain unchanged, focusing on real-world production features and unbiased, transparent testing of open-source vector databases.
- [Navigating challenges and innovations in search technologies](https://qdrant.tech/blog/navigating-challenges-innovations/): The podcast discussed retrieval-augmented generation (RAG) as an innovative approach in search technologies that combines information retrieval with language generation to improve context understanding and output accuracy in NLP tasks. It also emphasized the importance and challenges of evaluating RAG and LLM-based applications, including domain-specific model performance, data processing, retrieval quality, and generation evaluation.
- [Optimizing an Open Source Vector Database with Andrey Vasnetsov](https://qdrant.tech/blog/open-source-vector-search-engine-vector-database/): Andrey Vasnetsov, CTO at Qdrant, emphasizes that for vector search systems like Qdrant, scalability and search performance are more important than transactional consistency, advocating a search engine approach over a traditional database mindset. He outlines strategies such as in-place filtering during graph traversal, utilizing subgraphs and overlapping intervals, tuning beam size in HNSW indices, and combining vector with relational search to optimize precision and efficiency at scale.
- [Vector Search Complexities: Insights from Projects in Image Search and](https://qdrant.tech/blog/vector-image-search-rag/): Noé Achache, Lead Data Scientist at Sicara, emphasizes the readiness and value of image embedding models like Dino V2 for advancing text and image search projects, highlighting their strong out-of-the-box performance without the need for fine-tuning. In a podcast episode, he shares practical insights on deploying vector search across diverse applications such as image search, real estate data deduplication, document retrieval, and handling sensitive medical information, while also addressing challenges like data safety and the importance of new model development.
- [How to Superpower Your Semantic Search Using a Vector Database](https://qdrant.tech/blog/semantic-search-vector-database/): Nicolas Mauti and the team at Malt improved their freelancer matching platform by adopting a retriever-ranker architecture with multilingual transformer-based models and transitioning to the Qdrant vector database, which dramatically increased performance—reducing search latency from 10 to 1 second—and enabled precise, scalable semantic search with geospatial filtering. Their choice balanced performance and accuracy better than Elasticsearch, significantly enhancing Malt's ability to connect freelancers and projects.
- [Building LLM Powered Applications in Production - Hamza Farooq | Vector](https://qdrant.tech/blog/llm-complex-search-copilot/): Hamza Farooq’s experience at Google and Walmart Labs, combined with his expertise in machine learning, has shaped his practical approach to building LLM-powered applications by focusing on real-world business problems, efficient open-source implementations, and user-centered conversational search and recommendation solutions. At Traversaal.ai, he leverages this background to deliver domain-specific AI products that bridge the gap between innovative technology and seamless user experiences.
- [Building a High-Performance Entity Matching Solution with Qdrant -](https://qdrant.tech/blog/entity-matching-qdrant/): Rishabh Bhardwaj and his team built a high-performance hotel matching solution, initially experimenting with Postgres but ultimately choosing Qdrant for its superior speed and recall. The HNSW algorithm in Qdrant enhanced their solution by enabling fast and accurate entity matching, effectively handling large-scale, multilingual, and inconsistent hotel data.
- [FastEmbed: Fast & Lightweight Embedding Generation - Nirant Kasliwal |](https://qdrant.tech/blog/fast-embed-models/): Nirant Kasliwal, creator of FastEmbed and contributor to OpenAI's Finetuning Cookbook, shares practical tips to improve and speed up embedding creation—highlighting FastEmbed’s efficiency, support for CPU/GPU quantization, and solutions for common production challenges in Natural Language Processing (NLP). In this Vector Space Talks episode, Nirant discusses model selection, optimizing embeddings, and the library's ability to deliver fast, lightweight, and domain-adaptable embedding workflows.
- [When music just doesn't match our vibe, can AI help? - Filip Makraduli |](https://qdrant.tech/blog/human-language-ai-models/): Filip Makraduli, a data scientist with expertise in causal ML and generative AI, has developed an innovative music recommendation system that uses language-based song descriptions and AI models to match music to users’ moods or vibes. By leveraging sentence embeddings and vector similarity, his approach enables more personalized and context-aware song recommendations, demonstrated through real-time Spotify playlist creation.
- [Binary Quantization - Andrey Vasnetsov | Vector Space Talks](https://qdrant.tech/blog/binary-quantization/): Andrey Vasnetsov, CTO of Qdrant, explains that binary quantization dramatically improves vector search efficiency—reducing storage size and boosting speed by up to 30x—though compatibility varies across models, with OpenAI models performing best. The episode highlights the importance of quantization for handling large vector indexes, the simplicity and effectiveness of binary quantization, and practical strategies like oversampling to optimize search precision in real time.
- [Loading Unstructured.io Data into Qdrant from the Terminal](https://qdrant.tech/blog/qdrant-unstructured/): The blog post explains how to use Unstructured.io’s CLI to extract data from Discord channels, process it into structured form, and ingest it into a Qdrant vector database, including necessary prerequisites and example commands for each step. It highlights that this workflow simplifies data ingestion from Discord and over 20 other data sources directly into Qdrant, supporting customization via the CLI.
- [Chat with a codebase using Qdrant and N8N](https://qdrant.tech/blog/qdrant-n8n/): n8n allows you to visually build AI-powered workflows that connect apps with APIs and manipulate data with minimal coding; using the Qdrant node and OpenAI integration, you can ingest a GitHub repository into a vector database and create a chat service to interact with the codebase. The process involves setting up necessary accounts, configuring nodes for data ingestion and retrieval, and optionally embedding the resulting chat into applications using the @n8n/chat package.
- [Vector search and applications" by Andrey Vasnetsov, CTO at Qdrant](https://qdrant.tech/blog/vector-search-and-applications-record/): Andrey Vasnetsov, Co-founder and CTO at Qdrant, discussed vector search and its applications with Learn NLP Academy, covering topics such as the Qdrant engine, Quaterion similarity learning, multimodal similarity, comparison with Elastic search, support for multiple embeddings, fundraising, and the future of vector search. He also addressed finetuning models for out-of-domain scenarios.
- [From Content Quality to Compression: The Evolution of Embedding Models](https://qdrant.tech/blog/cohere-embedding-v3/): Nils Reimers, Cohere’s Head of Machine Learning, discussed advancements in embeddings including content quality estimation, compression-aware training, reinforcement learning from human feedback, and the importance of evaluating embedding quality contextually, while also sharing upcoming features and noting current limitations in distinguishing truthfulness. He highlighted that these innovations enhance model usefulness and efficiency, but content quality models still struggle to discern true from fake statements due to reliance on pretraining data.
- [Pienso & Qdrant: Future Proofing Generative AI for Enterprise-Level Customers](https://qdrant.tech/blog/case-study-pienso/): Pienso and Qdrant are partnering to deliver scalable, efficient, and user-friendly interactive deep learning solutions by combining Pienso’s low-code platform with Qdrant’s high-performance vector storage and retrieval capabilities, enabling organizations to build and improve large language models while maintaining data sovereignty and model autonomy. This collaboration will enhance LLM performance through efficient context management, fast training and inference, and reliable, cost-effective storage, making it ideal for enterprise-scale AI-driven applications.
- [Powering Bloop semantic code search](https://qdrant.tech/blog/case-study-bloop/): bloop is a fast, local code-search engine founded in 2021 that enables semantic search of large codebases, helping developers understand and reuse code more efficiently; it relies on Qdrant, a high-performance, open-source vector search database, for fast, accurate semantic search at scale. The integration of Qdrant enables bloop to deliver reliable, instantaneous semantic search results even on very large codebases.
- [Qdrant supports ARM architecture!](https://qdrant.tech/blog/qdrant-supports-arm-architecture/): ARM-based processors, such as those used in AWS Graviton2 instances, offer better energy efficiency and are about 20% cheaper than x86 counterparts, though they are roughly 10-20% slower in vector search tasks but with more consistent performance. This makes ARM64 a cost-effective and reliable option for hosting vector search applications like Qdrant, despite a slight performance tradeoff.
- [Qdrant has joined NVIDIA Inception Program](https://qdrant.tech/blog/qdrant-joined-nvidia-inception-program/): We have joined the NVIDIA Inception program, which supports technology startups with advanced resources and connections. We are especially excited about gaining GPU support, a key feature in Qdrant's roadmap.
- [Bulk Upload Vectors](https://qdrant.tech/documentation/database-tutorials/bulk-upload/): To efficiently bulk upload vectors to a Qdrant collection, use the fastest available client (preferably Rust) and consider parallelizing uploads, while optimizing indexing settings: you can disable or defer HNSW indexing during ingestion (e.g., set `m:0` or `indexing_threshold:0`) for faster uploads and reduced memory usage, then re-enable or adjust indexing (e.g., set `m:16`) after data import for fast search performance. Choose the configuration that best balances upload speed, memory usage, and index availability for your needs.
- [Qdrant Fundamentals](https://qdrant.tech/documentation/faq/qdrant-fundamentals/): Qdrant supports dense vectors up to 65,535 dimensions, multiple vectors per data point, and flexible, real-time search and filtering features, with recommendations to optimize metadata and collection usage for performance. Compatibility requirements include careful version upgrades, primary CPU-based operation (with some GPU support), and a preference for multitenancy in single collections rather than many small collections.
- [Reranking in Semantic Search](https://qdrant.tech/documentation/search-precision/reranking-semantic-search/): Reranking in Retrieval-Augmented Generation (RAG) systems, especially with Qdrant, significantly improves the relevance and accuracy of search results by reordering top candidate documents based on relevance using specialized models such as cross-encoders, multi-vector rerankers like ColBERT, or large language models. This process enhances search precision, reduces information overload, and balances speed with relevance by first retrieving a broad set of results (for recall) and then refining them through reranking to surface the most contextually appropriate documents.
- [Role Management](https://qdrant.tech/documentation/cloud-rbac/role-management/): Qdrant Cloud's Role Management allows fine-grained access control through built-in and custom roles, where each role is defined by a set of permissions dictating user actions. Built-in roles cover common needs and cannot be modified, while custom roles can be created, edited, renamed, deleted, or duplicated to tailor access for specific resources.
- [Semantic Search 101](https://qdrant.tech/documentation/beginner-tutorials/search-beginners/): This beginner-friendly tutorial guides you through building a semantic search engine for science fiction books in just 5 minutes, using Python, Sentence Transformers for text embeddings, and Qdrant as a vector database for storage and querying. By following simple steps—installing required libraries, preparing your dataset, setting up storage, uploading data, and querying—you can create a search engine that delivers semantically relevant book recommendations beyond simple keyword matching.
- [Setup Hybrid Cloud](https://qdrant.tech/documentation/hybrid-cloud/hybrid-cloud-setup/): To set up a Qdrant cluster in a Hybrid Cloud Environment, you need a compliant Kubernetes cluster with CSI block storage, enough resources, cluster-admin permissions, required CLI tools (kubectl and helm), and ensure the agent can connect to Qdrant Cloud. Installation involves configuring the environment via the Qdrant Cloud Console, generating and running an installation script, and optionally mirroring necessary container images and Helm charts to your own registry for advanced use cases.
- [Setup Private Cloud](https://qdrant.tech/documentation/private-cloud/private-cloud-setup/): Qdrant Private Cloud requires a standard-compliant Kubernetes cluster with CSI block storage (not NFS/S3), `cluster-admin` permissions, and CLI tools `kubectl` and `helm`; installation involves configuring your registry for required container images and charts, then deploying and managing via Helm commands, with operator permissions scoped by namespace. Uninstallation removes both the software and all managed Qdrant clusters.
- [Understanding Vector Search in Qdrant](https://qdrant.tech/documentation/overview/vector-search/): Traditional search relies on inverted indexes and sparse vectors based on keyword matching, but modern vector search uses dense embeddings from deep language models to capture semantic meaning and handle synonyms and multiple languages automatically. Qdrant is a vector database optimized for efficient and scalable similarity search by using advanced indexing (like HNSW graphs), enabling fast retrieval of semantically similar documents without calculating distances to every item.
- [Vector Quantization](https://qdrant.tech/documentation/): Qdrant is an open-source, AI-native vector database and semantic search engine that enables efficient extraction and search of information from unstructured data, featuring cloud and local deployment options, advanced filtering, hybrid search, multitenancy, sharding, access control, and performance-enhancing features like quantization and multivector support. The platform offers comprehensive guidebooks and quickstart resources to help developers rapidly build and scale search solutions.
- [Automate filtering with LLMs](https://qdrant.tech/documentation/search-precision/automate-filtering-with-llms/): LLMs can automate the generation of structured filters for vector search applications (like Qdrant), allowing conversational or voice interfaces to create precise search queries without traditional UI elements. By using libraries such as Instructor and providing information about available and indexed fields (including their types), you can restrict and validate LLM-generated filters to match your collection schema, improving accuracy and reliability.
- [Build a Neural Search Service](https://qdrant.tech/documentation/beginner-tutorials/neural-search/): This beginner tutorial guides you through building a neural search service that uses Sentence Transformers to encode company descriptions from startups-list.com into vectors, stores them in Qdrant (a vector database), and serves search queries via a FastAPI-based API. Main steps include data preparation and encoding, vector storage using Qdrant (run with Docker), data upload, and API implementation, with all necessary code and prerequisites provided.
- [Configuration](https://qdrant.tech/documentation/private-cloud/configuration/): The Qdrant Private Cloud Helm chart offers extensive customization for deploying and managing the Qdrant operator and database clusters in Kubernetes, including options for replica counts, image sources, security contexts, networking, metrics, and resource allocation. Key features like cluster management, scheduling, network policies, and integration with services such as metrics scraping are configurable via YAML parameters.
- [Create & Restore Snapshots](https://qdrant.tech/documentation/database-tutorials/create-snapshot/): Qdrant collections, which store vectors and related data, can be efficiently backed up and restored using snapshots that capture all necessary data and structures; this process is especially important for large or distributed clusters and is done via HTTP endpoints on each cluster node. This tutorial provides step-by-step instructions for creating, downloading, and restoring collection snapshots, noting that the Python SDK does not support snapshot operations in local mode and requires either Qdrant via Docker or Qdrant Cloud.
- [Create a Cluster](https://qdrant.tech/documentation/hybrid-cloud/hybrid-cloud-cluster-creation/): To create a Qdrant cluster in a Hybrid Cloud environment, follow the standard cluster creation process, ensuring you select your Hybrid Cloud Environment and configure Kubernetes settings such as node selectors, tolerations, service types, and authentication using Kubernetes secrets. You can expose the cluster internally or externally via service types or ingress, configure API keys and TLS for security, and modify these settings anytime via the cluster detail page.
- [Data Ingestion for Beginners](https://qdrant.tech/documentation/data-ingestion-beginners/): This tutorial demonstrates how to build a data ingestion pipeline that extracts unstructured data (text and images) from an AWS S3 bucket, processes it with LangChain to generate embeddings using NLP models, and stores the embeddings in the Qdrant vector database for efficient semantic search. Prerequisites include an AWS and Qdrant account, a structured sample dataset, a Python environment with specific libraries, and secure storage of access keys, following step-by-step instructions for loading, processing, and embedding data.
- [Database Optimization](https://qdrant.tech/documentation/faq/database-optimization/): To optimize database performance and reduce memory usage, configure quantization or on-disk vector storage based on your needs, and choose machine configurations according to whether you prioritize speed (more RAM) or storage cost (faster disks). For better performance, ensure proper payload indexing, use fast local SSDs for on-disk storage, monitor query parameters, and set memory limits with Docker or Kubernetes if necessary.
- [How to Use Multivector Representations with Qdrant Effectively](https://qdrant.tech/documentation/advanced-tutorials/using-multivector-representations/): Multivector representations in Qdrant allow each document to be stored as multiple token-level vectors, enabling more precise query-document matching and accurate reranking, especially with models like ColBERT; however, to avoid high RAM and slow inserts, it's crucial to disable HNSW indexing for these multivectors and use them only during the reranking stage after initial dense vector retrieval. By combining Qdrant and FastEmbed, you can efficiently set up a scalable, production-ready ColBERT-style retrieval pipeline with fast retrieval and fine-grained reranking in a single workflow.
- [Large Scale Search](https://qdrant.tech/documentation/database-tutorials/large-scale-search/): This tutorial demonstrates how to efficiently upload, index, and search the massive LAION-400M dataset (400 million 512-dimensional vectors) on Qdrant using minimal hardware (8 CPUs, 64GB RAM, 650GB disk) by employing resource-saving configurations like FLOAT16 vectors, binary quantization, and optimized index parameters, achieving reasonable latency, accuracy, and throughput. Key considerations include memory usage breakdown, the necessity of high network bandwidth, and techniques ensuring vectors and indexes fit in RAM while leveraging on-disk storage and query-time optimizations.
- [Quickstart](https://qdrant.tech/documentation/fastembed/fastembed-quickstart/): To generate text embeddings with FastEmbed, install the package, load the default BAAI/bge-small-en-v1.5 model, and provide your documents as a list of strings; then, use the model’s `.embed()` method to obtain 384-dimensional vector representations of each document. You can print or visualize these embeddings, which are NumPy arrays, for further analysis.
- [Reranking in Hybrid Search](https://qdrant.tech/documentation/advanced-tutorials/reranking-hybrid-search/): Hybrid search in Qdrant combines dense, sparse (BM25), and late interaction (ColBERT) embeddings to deliver highly relevant search results, further refined through reranking for maximum relevance. Using the FastEmbed library, you can efficiently index documents with multiple embedding types and set up a multi-vector collection in Qdrant to support hybrid retrieval and advanced reranking workflows.
- [Search Through Your Codebase](https://qdrant.tech/documentation/advanced-tutorials/code-search/): This tutorial explains how to use Qdrant semantic search, along with NLP and code-specific embeddings, to navigate and find relevant code snippets within a codebase by chunking, preprocessing, and embedding code for effective semantic querying. It details codebase parsing, chunk extraction, conversion of code to natural language for general embeddings, and setting up an ingestion pipeline for semantic search using Qdrant.
- [Simple Agentic RAG System](https://qdrant.tech/documentation/agentic-rag-crewai-zoom/): This tutorial demonstrates how to build an Agentic RAG system using Qdrant for vector search and CrewAI for orchestrating modular agents, enabling deep analysis and insight extraction from meeting transcripts via a chat-like Streamlit interface. The workflow involves storing meeting data as vector embeddings in Qdrant, querying and analyzing this data with CrewAI agents and Anthropic Claude, and providing users with natural language insights, all supported by provided setup scripts and a ready-to-use GitHub repository.
- [User Management](https://qdrant.tech/documentation/cloud-rbac/user-management/): Users can be invited and assigned roles via the User Management section, with the option to select specific roles or use the default minimal-permission Base role; invites can be revoked before acceptance. Authorized users can update roles or remove active users through User Management after invitations have been accepted.
- [Agentic RAG With LangGraph](https://qdrant.tech/documentation/agentic-rag-langgraph/): Agentic RAG enhances traditional RAG systems by using AI agents—powered by LangGraph for workflow management and Qdrant for vector search—to dynamically select between multiple data sources or web search, enabling complex, multi-step information retrieval. The proposed system leverages OpenAI models, two Qdrant vector databases, and Brave Search, orchestrated via LangGraph, to flexibly answer questions about Hugging Face and Transformers documentation.
- [Build a Recommendation System with Collaborative Filtering](https://qdrant.tech/documentation/advanced-tutorials/collaborative-filtering/): This guide explains how to build a movie recommendation system using collaborative filtering and Qdrant, leveraging user ratings as sparse vectors for efficient similarity search without the need for traditional model training. The process involves preparing normalized user-movie rating data, uploading it as sparse vectors to Qdrant, and retrieving personalized movie recommendations by finding users with similar preferences.
- [Configure, Scale & Update Clusters](https://qdrant.tech/documentation/hybrid-cloud/configure-scale-upgrade/): Qdrant Hybrid Cloud Clusters allow you to configure advanced settings, scale clusters both vertically and horizontally with features like automatic shard rebalancing and resharding, and update cluster versions at any time. For more details, refer to the specific documentation links provided.
- [FastEmbed & Qdrant](https://qdrant.tech/documentation/fastembed/fastembed-semantic-search/): FastEmbed can be seamlessly integrated with Qdrant for local, in-memory vector search by installing the Qdrant client, setting up a collection with model-specific vector parameters, uploading documents (with semantic embeddings handled implicitly), and querying for semantically similar results. This allows efficient semantic search, as shown by retrieving the most relevant document based on the query.
- [Load a HuggingFace Dataset](https://qdrant.tech/documentation/database-tutorials/huggingface-datasets/): Hugging Face offers the "arxiv-titles-instructorxl-embeddings" dataset, containing 2.25 million precomputed vector embeddings of paper titles with associated metadata, which can be loaded, streamed, and efficiently imported into Qdrant for semantic search applications. The dataset is large (over 16 GB), and Qdrant provides tools and batching strategies to facilitate its integration and use for building AI-powered search systems.
- [Managing a Cluster](https://qdrant.tech/documentation/private-cloud/qdrant-cluster-management/): To manage a Qdrant cluster, configure the QdrantCluster resource with unique IDs, required labels, resource settings, and optionally expose the service and enable security features like API key authentication and TLS, referencing Kubernetes secrets as needed. Scaling, upgrading, and secure communication (including inter-node TLS) are managed by editing cluster specs, while ensuring proper network controls and following upgrade/versioning guidelines.
- [Permission Reference](https://qdrant.tech/documentation/cloud-rbac/permission-reference/): The document lists and describes all permissions available in Qdrant Cloud across areas such as Identity and Access Management, Clusters (including API Keys, backups, and schedules), Hybrid Cloud, Payment & Billing, Account Management, and Profile. Enabling any `write:*` permission automatically enables the corresponding `read:*`, ensuring users maintain required access when creating or updating resources.
- [Setup Hybrid Search with FastEmbed](https://qdrant.tech/documentation/beginner-tutorials/hybrid-search-fastembed/): This beginner-level tutorial demonstrates how to build and deploy a hybrid search service using FastEmbed for embedding generation, Qdrant as a vector database, and FastAPI for serving an API, enabling searches through a startup dataset with both dense (BERT-based) and sparse (full-text) embeddings. Key steps include preparing the dataset, running Qdrant in Docker, encoding and uploading data with FastEmbed, and creating a hybrid search API for querying similar startups.
- [What is Qdrant?](https://qdrant.tech/documentation/overview/): Vector databases, such as Qdrant, efficiently store and query high-dimensional vector representations (embeddings) generated by machine learning models, enabling fast similarity search for applications like recommendation systems and semantic search. Qdrant is a production-ready vector database offering flexible storage, efficient indexing, and support for multiple distance metrics, making it well-suited for handling large-scale, unstructured data in AI and machine learning workflows.
- [Agentic RAG Discord Bot with CAMEL-AI](https://qdrant.tech/documentation/agentic-rag-camelai-discord/): This tutorial guides you through building an agentic Retrieval-Augmented Generation (RAG) chatbot for Discord using CAMEL-AI, Qdrant, and OpenAI, which not only retrieves but actively reasons over relevant context for user queries. The workflow involves setting up the environment and APIs, scraping and embedding documentation into Qdrant, configuring intelligent multi-step reasoning with CAMEL-AI, and deploying the chatbot to Discord for dynamic, context-aware conversations.
- [Backups](https://qdrant.tech/documentation/private-cloud/backups/): To create backups in Qdrant, use the `QdrantClusterSnapshot` resource for one-time backups or `QdrantClusterScheduledSnapshot` for recurring backups, and restore using the `QdrantClusterRestore` resource. Ensure the `cluster-id` and `customer-id` labels on all resources match the associated QdrantCluster.
- [Build With Async API](https://qdrant.tech/documentation/database-tutorials/async-api/): Asynchronous programming in Python, supported by frameworks like FastAPI and tools such as Qdrant’s AsyncQdrantClient, enables efficient handling of IO-bound operations—like database interactions—by allowing multiple concurrent user requests without blocking server threads. To leverage the async API, use `async/await` syntax in an async context (e.g., with `asyncio.run`), and simply replace the synchronous client with `AsyncQdrantClient` while adding `await` to method calls, especially in multi-user web services.
- [Cloud Quickstart](https://qdrant.tech/documentation/cloud-quickstart/): To get started with Qdrant Cloud, sign up for an account, create a cluster to receive your API key, and access the Cluster UI dashboard. You can then authenticate and interact with your Qdrant instance using official SDKs or the interactive tutorial sandbox.
- [Configure the Qdrant Operator](https://qdrant.tech/documentation/hybrid-cloud/operator-configuration/): The Qdrant Operator offers a range of advanced configuration options for deployment in hybrid cloud environments, allowing customization of resource usage, scheduling, cluster management, storage, network policies, logging, and security through a structured YAML file. Key features include adjustable cluster and backup management, logging levels, pod scheduling constraints, ingress options, and support for monitoring and resource limits.
- [Logging & Monitoring](https://qdrant.tech/documentation/private-cloud/logging-monitoring/): Qdrant Private Cloud logging can be accessed via kubectl or any Kubernetes log management tool, with configurable log levels set in the QdrantCluster spec, and can be integrated into external log systems without special configuration. Monitoring is available through the Qdrant Cloud console or by scraping Prometheus/OpenTelemetry-compatible metrics from specific endpoints, with Grafana dashboards provided for visualization.
- [Measure Search Quality](https://qdrant.tech/documentation/beginner-tutorials/retrieval-quality/): Semantic search quality depends on both embedding quality and the performance of the Approximate Nearest Neighbors (ANN) algorithm. This tutorial demonstrates how to measure and improve retrieval quality in Qdrant by evaluating embeddings with benchmarks and comparing ANN results against exact kNN search using metrics like precision@k.
- [Networking, Logging & Monitoring](https://qdrant.tech/documentation/hybrid-cloud/networking-logging-monitoring/): Qdrant Hybrid Cloud provides configurable network policies for database clusters, allowing fine-grained ingress control, and supports flexible logging—accessible via kubectl or integrated log management systems—and customizable log levels. Monitoring is available through the Qdrant Cloud console and Prometheus/OpenTelemetry-compatible endpoints in each pod, with Grafana dashboards offered for visualization.
- [Scaling PDF Retrieval with Qdrant](https://qdrant.tech/documentation/advanced-tutorials/pdf-retrieval-at-scale/): The tutorial demonstrates a scalable approach to PDF retrieval using Qdrant and Vision Large Language Models (VLLMs) like ColPali and ColQwen2 by applying mean pooling to reduce multivector representations, enabling much faster indexing and retrieval while maintaining high retrieval quality. The process involves first-stage retrieval using mean-pooled vectors and reranking results with original multivectors, thus overcoming the computational challenges of large-scale PDF search.
- [Working with miniCOIL](https://qdrant.tech/documentation/fastembed/fastembed-minicoil/): miniCOIL is a sparse neural retriever that enhances BM25 keyword retrieval by leveraging the contextual meaning of keywords for scoring, improving search result relevance when exact keyword matches are required but with semantic awareness. It is integrated with Qdrant and FastEmbed, and outperforms BM25 by ranking documents with contextually appropriate keyword use, as shown in a sample query where miniCOIL surfaces medically relevant vector documents while BM25 does not.
- [API & SDKs](https://qdrant.tech/documentation/interfaces/): Qdrant provides official client libraries for Python, JavaScript/TypeScript, Rust, Go, .NET, and Java, or you can interact with its REST or gRPC APIs, with REST recommended for newcomers and prototyping, while gRPC is suited for advanced users needing optimal performance. The gRPC interface mirrors REST endpoints, requires explicit port exposure (default 6334), and is more complex but faster than REST.
- [API Reference](https://qdrant.tech/documentation/private-cloud/api-reference/): The qdrant.io/v1 API package defines schemas for managing Qdrant clusters, regions, entities, and related resources, including their phases, statuses, configurations, and GPU settings. It standardizes resource types and operational states to support orchestration and automated management of Qdrant infrastructure components.
- [Changelog](https://qdrant.tech/documentation/private-cloud/changelog/): This changelog details progressive updates to Qdrant Kubernetes components, including new features such as metrics exporting, automatic shard balancing, enhanced performance and stability, support for GPU instances, configurable settings, improved scaling, and better peer management. Key additions include OpenTelemetry/Prometheus integration, P2P TLS, and automatic shard replication, as well as numerous bug fixes and optimizations across releases.
- [Deployment Platforms](https://qdrant.tech/documentation/hybrid-cloud/platform-deployment-options/): This page outlines how to deploy Qdrant Hybrid Cloud on various managed Kubernetes platforms—including Akamai (Linode), AWS, Civo, DigitalOcean, Gcore, and GCP—by following provider-specific prerequisites and Qdrant’s setup guide. It highlights recommended instance types, storage configurations, and the need for appropriate Kubernetes storage drivers and backup solutions per platform.
- [GraphRAG with Qdrant and Neo4j](https://qdrant.tech/documentation/examples/graphrag-qdrant-neo4j/): GraphRAG combines knowledge graphs and vector search to address the limitations of traditional Retrieval-Augmented Generation (RAG) by enabling large language models to connect complex relationships and provide more accurate, context-rich responses. This tutorial demonstrates building a GraphRAG system using Neo4j and Qdrant, outlining an architecture that leverages both graph and vector databases for efficient data ingestion, semantic search, and contextual response generation.
- [Installation](https://qdrant.tech/documentation/guides/installation/): Qdrant requires a 64-bit CPU (x86_64/amd64 or arm64), sufficient RAM and SSD/NVMe storage with block-level POSIX compatibility, and open network ports 6333, 6334, and 6335; it does not support 32-bit systems, NFS or object storage. Installation options include fully managed Qdrant Cloud for production, Kubernetes deployments (with Helm chart or Enterprise Operator), Docker for development, and careful configuration of storage, security, and backups is essential for self-hosted production setups.
- [Multilingual & Multimodal RAG with LlamaIndex](https://qdrant.tech/documentation/multimodal-search/): This tutorial demonstrates how to build a multilingual and multimodal (text and image) semantic search application using LlamaIndex’s vdr-2b-multi-v1 embedding model and Qdrant vector database, enabling effective text-to-image, image-to-text, and cross-lingual searches without OCR or complex preprocessing. By uploading shared embeddings of images and captions into Qdrant, users can perform flexible and accurate searches for various applications, such as e-commerce and media management, simply by querying with either text or images in multiple languages.
- [Working with SPLADE](https://qdrant.tech/documentation/fastembed/fastembed-splade/): SPLADE is a method for generating efficient, interpretable sparse text vectors useful for information retrieval, with practical guidance provided on setup, embedding documents, and interpreting token weights using the FastEmbed library. The approach enables contextual term expansion and token importance ranking, with design features like unnormalized weights and flexible handling of vocabulary and typos.
- [5 Minute RAG with Qdrant and DeepSeek](https://qdrant.tech/documentation/rag-deepseek/): This tutorial provides a quick guide to building a Retrieval-Augmented Generation (RAG) pipeline by storing vector embeddings in Qdrant and enriching Large Language Model prompts with DeepSeek, enhancing response accuracy through context retrieval. It covers setting up the environment, ingesting documents as embeddings in Qdrant, and using DeepSeek to test and improve LLM responses with retrieved knowledge.
- [Qdrant Web UI](https://qdrant.tech/documentation/web-ui/): The Qdrant Web UI provides an intuitive interface to manage both local and cloud Qdrant deployments, offering features such as REST API interaction, collection management, and an interactive tutorial. Access is via http://localhost:6333/dashboard for local setups or by appending :6333/dashboard to your cloud cluster URL.
- [Working with ColBERT](https://qdrant.tech/documentation/fastembed/fastembed-colbert/): ColBERT is an embedding model that generates multivector representations—one vector per token—which capture nuanced semantics for strong reranking performance, though at increased computational cost; it is best used for reranking smaller candidate sets rather than for initial large-scale retrieval. Using the `fastembed` library, you can generate these multivectors and upload them to Qdrant, which natively supports multivector storage and retrieval, for efficient experimentation on text datasets.
- [Automating Processes with Qdrant and n8n](https://qdrant.tech/documentation/qdrant-n8n/): This tutorial demonstrates how to integrate Qdrant with the n8n low-code automation platform to enable advanced workflows such as recommendations and big data analysis beyond basic RAG, including use cases like movie recommendation systems and large-scale unstructured data analysis leveraging vector search. It explains how to set up both cloud and local Qdrant instances in n8n, and showcases workflows for recommendation and anomaly detection, while highlighting the newly available official Qdrant n8n node for simplified integration.
- [Reranking with FastEmbed](https://qdrant.tech/documentation/fastembed/fastembed-rerankers/): A reranker is a precise but resource-intensive model used to improve search result relevance by reordering a small subset of documents retrieved with a faster method; FastEmbed supports multiple cross-encoder reranker models, including Jina Reranker v2, which can be used after initial retrieval (e.g., with all-MiniLM-L6-v2) for more accurate ranking. The tutorial demonstrates how to set up FastEmbed with Qdrant, index data, and apply reranking models on a sample movie description dataset.
- [Administration](https://qdrant.tech/documentation/guides/administration/): Qdrant provides administration tools to modify instance behavior at runtime, including a locking API to restrict write operations (not persistent across restarts and acting per node), a recovery mode for resolving out-of-memory issues by allowing only collection deletion while loading minimal metadata, and strict mode to dynamically limit inefficient or resource-heavy operations for better stability and predictability. These features can be enabled or configured via API or environment variables and are useful for managing access, recovery, and performance in distributed and unpredictable environments.
- [Running with GPU](https://qdrant.tech/documentation/guides/running-with-gpu/): Starting with v1.13.0, Qdrant supports GPU acceleration via dedicated Docker images for NVIDIA or AMD GPUs, which require specific drivers and container toolkits; GPU usage is configured through various settings, and activation for indexing needs to be explicitly enabled. Limitations include Linux x86_64-only Docker support and a 16GB per-GPU vector data size cap per indexing iteration.
- [Capacity Planning](https://qdrant.tech/documentation/guides/capacity-planning/): Capacity planning for your cluster involves balancing RAM and disk storage based on vector count, dimensions, payload data, and indexing needs, with estimated memory requirements calculated using rough formulas that add 50% for metadata and indexing overhead. For performance, store frequently accessed and indexed data in RAM, use disk for less-used payloads, and consider scaling disk space via the cluster UI as needed, always validating with real data for accuracy.
- [FastEmbed](https://qdrant.tech/documentation/fastembed/): FastEmbed is a lightweight Python library for fast and accurate embedding generation, supporting a variety of popular and multilingual models with easy integration into Qdrant for multimodal search. It is optimized for performance with minimal dependencies, making it suitable for serverless environments, and offers beginner-to-advanced guides for tasks like semantic search, sparse and multivector embeddings, and reranking.
- [Optimize Performance](https://qdrant.tech/documentation/guides/optimize/): Qdrant performance can be optimized in three main ways: use vector quantization for high-speed search with low memory usage, store vectors and the HNSW index on disk for high precision with low memory, or keep all data in RAM and use quantization with re-scoring for both high precision and high speed. Additionally, latency and throughput can be balanced by adjusting the number and size of segments based on workload priorities.
- [Getting Started](https://qdrant.tech/documentation/cloud-getting-started/): To get started with Qdrant Managed Cloud, create an account (with required payment details), plan your cluster resources, and follow the provided guides to deploy and manage clusters, including production-ready multi-node setups. Automation options are available via the Qdrant Cloud API and Terraform Provider.
- [Multitenancy](https://qdrant.tech/documentation/guides/multiple-partitions/): For most users, efficient multitenancy is achieved by using a single collection with payload-based partitioning (e.g., by `group_id`), though multiple collections may be used for greater user isolation if needed. To maximize performance, customize your HNSW index to bypass global indexing and organize data by tenant, but note that global (cross-tenant) queries will be slower.
- [Account Setup](https://qdrant.tech/documentation/cloud-account-setup/): Qdrant Cloud accounts can be registered via email, Google, GitHub, or enterprise SSO, with functionalities for inviting and managing users, switching and creating multiple accounts, and configuring settings such as permissions, light/dark mode, and account details. Enterprise SSO is available for Premium Tier customers, supporting various identity providers.
- [Cloud RBAC](https://qdrant.tech/documentation/cloud-rbac/): Cloud RBAC in Qdrant Cloud allows precise management of user permissions for key areas like billing, identity, clusters (currently all clusters together), hybrid cloud, and account configuration via the console. Access is available through "Access Management > User & Role Management," with more detailed permission controls planned for future releases.
- [Managed Cloud](https://qdrant.tech/documentation/cloud/): Qdrant Managed Cloud is a SaaS offering that provides managed Qdrant database clusters for fast and reliable similarity search without infrastructure maintenance, accessible via a Qdrant Cloud account and API key. It supports high availability, scaling, monitoring, backups, disaster recovery, and can operate natively on major cloud providers or as a Hybrid Cloud with your own infrastructure.
- [Hybrid Cloud](https://qdrant.tech/documentation/hybrid-cloud/): Qdrant Hybrid Cloud allows you to manage vector databases across cloud, on-premises, or edge environments using your own Kubernetes clusters, ensuring data stays within your infrastructure for maximum security, privacy, and cost efficiency. It operates through a Kubernetes Operator and Cloud Agent that manage Qdrant databases locally while only sending telemetry (not user data) to Qdrant Cloud, with no need to expose your cluster or provide external credentials.
- [Vector Search Basics](https://qdrant.tech/documentation/beginner-tutorials/): The Beginner Tutorials section offers step-by-step guides on building semantic, neural, and hybrid search engines using Qdrant and related tools, as well as measuring and improving retrieval quality. These tutorials are designed to help newcomers quickly get started with advanced search technologies.
- [Advanced Retrieval](https://qdrant.tech/documentation/advanced-tutorials/): The Advanced Tutorials section offers guides on building a movie recommendation system with collaborative filtering, developing a text/image multimodal search with Qdrant and FastEmbed, navigating codebases using semantic search, and optimizing large-scale PDF retrieval with ColPali/ColQwen integration. Each tutorial demonstrates advanced applications of Qdrant for various AI-powered search and retrieval tasks.
- [Private Cloud](https://qdrant.tech/documentation/private-cloud/): Qdrant Private Cloud enables easy deployment and management of Qdrant database clusters on any Kubernetes infrastructure, offering features like zero-downtime upgrades, flexible scaling, multi-AZ support, backup and disaster recovery, extended telemetry, and enterprise support. It operates independently from the Qdrant Cloud Management Console, providing full control within your own environment.
- [Billing & Payments](https://qdrant.tech/documentation/cloud-pricing-payments/): Qdrant Cloud clusters are billed monthly based on CPU, memory, and disk usage, with payment options via credit card (processed by Stripe) or through AWS, GCP, or Azure Marketplace subscriptions—all with identical pricing and managed through the Qdrant Cloud Console. Regardless of payment method, clusters are deployed on Qdrant-owned infrastructure; those seeking deployment in their own environment should use the Hybrid Cloud solution.
- [Building a Chain-of-Thought Medical Chatbot with Qdrant and DSPy](https://qdrant.tech/documentation/examples/qdrant-dspy-medicalbot/): The article outlines building a reliable medical chatbot using Qdrant for vector search and DSPy for reasoning, leveraging Retrieval-Augmented Generation (RAG) to ground answers in current, specialty-filtered medical literature, and employing guardrails to ensure only medical queries are answered. By utilizing both dense and ColBERT embeddings for document retrieval and reranking, the system provides accurate, context-aware, and up-to-date responses while minimizing hallucinations and irrelevant answers.
- [Data Management](https://qdrant.tech/documentation/data-management/): This content lists key data management integrations, summarizing each tool’s core function: they cover ETL/ELT processing, workflow orchestration, real-time and batch data streaming, AI memory integration, and large-scale analytics. Platforms like Airbyte, Airflow, Spark, and others enable efficient data ingestion, transformation, and processing across diverse sources.
- [Multitenancy with LlamaIndex](https://qdrant.tech/documentation/examples/llama-index-multitenancy/): To implement multitenancy with LlamaIndex and Qdrant, use a single vector collection partitioned by user-specific metadata (payload-based partitioning) and set up the vector store, embedding, and chunking configuration accordingly. Index documents with tenant-specific metadata, optimize search performance with payload indexing, and use metadata constraints during retrieval to ensure users only access their own data.
- [Using the Database](https://qdrant.tech/documentation/database-tutorials/): The database tutorials cover key topics such as bulk uploading vectors, large scale search, backup and restore via snapshots, integration with Hugging Face datasets, efficient Python usage with Qdrant’s async API, migration guidance, and insights on static embeddings. These resources provide practical instructions for managing and optimizing Qdrant collections.
- [Embeddings](https://qdrant.tech/documentation/embeddings/): Qdrant supports a wide range of text and multimodal dense vector embedding models and services without limitations, including popular providers like OpenAI, AWS Bedrock, Cohere, and any open-source embeddings from HuggingFace. Users can easily integrate and utilize numerous embedding models tailored for different languages, modalities, and environments.
- [Premium Tier](https://qdrant.tech/documentation/cloud-premium/): Qdrant Cloud Premium offers enhanced features such as 24/7 priority support, a 99.9% uptime SLA, Single Sign-On, VPC Private Links, and storage encryption with user-provided keys. These benefits are designed for customers needing advanced support, improved security, and higher reliability compared to the standard tier.
- [Create a Cluster](https://qdrant.tech/documentation/cloud/create-cluster/): Qdrant Cloud offers Free (single-node, limited resources, good for testing) and Standard (multi-node, dedicated resources, high availability, disaster recovery) clusters, which can be easily created and configured through the Cloud Console by specifying provider, region, and resources. For production-ready clusters, use at least 3 nodes with replication, enable regular backups, and configure sufficient sharding for scalability; clusters can be managed, scaled, or deleted as needed via the console.
- [Frameworks](https://qdrant.tech/documentation/frameworks/): A wide range of frameworks exist to support building, deploying, and orchestrating AI and LLM-powered applications, offering capabilities such as multi-agent workflows, data orchestration, evaluation, memory, security, and domain-specific solutions in various programming languages. These tools enable rapid development, customization, testing, and production readiness for modern AI-driven systems.
- [Observability](https://qdrant.tech/documentation/observability/): The document lists observability integrations, including OpenLIT for OpenTelemetry-native observability and evaluations for LLMs and vector databases, OpenLLMetry as a set of OpenTelemetry extensions for LLM applications, and Datadog as a cloud-based monitoring and analytics platform. These tools enhance monitoring, evaluation, and analytics for large language model systems.
- [Platforms](https://qdrant.tech/documentation/platforms/): The Platform Integrations section lists various platforms specializing in web scraping, workflow automation, API and backend creation, data operations, RAG (Retrieval Augmented Generation) interfaces, and privacy-focused document querying. These tools enable users to automate tasks, integrate applications, manage data, and build business solutions with low-code or customizable options.
- [Private Chatbot for Interactive Learning](https://qdrant.tech/documentation/examples/rag-chatbot-red-hat-openshift-haystack/): This tutorial describes how to build a secure, private chatbot for corporate training using a Retrieval-Augmented Generation (RAG) pipeline with open-source tools on Red Hat OpenShift, integrating Mistral-7B-Instruct for LLM, BAAI embedding model, Qdrant for vector storage, and Haystack for orchestration—all within a closed infrastructure to protect sensitive data. It details setting up the environment, deploying and connecting each component, and constructing indexing and search pipelines to enable fast, private retrieval and question-answering over proprietary learning materials.
- [Implement Cohere RAG connector](https://qdrant.tech/documentation/examples/cohere-rag-connector/): This tutorial explains how to implement a custom HTTP-based connector for Cohere's Retrieval Augmented Generation (RAG) using Qdrant as the vector database, enabling semantic search over custom data (such as personal notes) and leveraging features like inline citations. It provides step-by-step guidance on setting up a Qdrant collection, embedding data with Cohere's API, and exposing the data to Cohere's LLM through a FastAPI service.
- [Send Data to Qdrant](https://qdrant.tech/documentation/send-data/): You can send data to a Qdrant cluster using various methods, such as streaming with Kafka, integrating with Databricks via Spark, or orchestrating data pipelines with Airflow and Astronomer. For migration from other vector databases, refer to the dedicated Migration Guide.
- [Build Prototypes](https://qdrant.tech/documentation/examples/): The provided content lists a variety of end-to-end code samples and interactive notebooks demonstrating how to build systems like chatbots, search engines, recommendation engines, and information extraction tools using Qdrant combined with frameworks such as LlamaIndex, Cohere, LangChain, and others. These examples cover use cases including multitenancy, customer support, document search, contract management, media recommendations, and medical chatbots, and are supported by detailed notebooks for hands-on learning and experimentation.
- [Question-Answering System for AI Customer Support](https://qdrant.tech/documentation/examples/rag-customer-support-cohere-airbyte-aws/): This tutorial explains how to build a fully private, AI-powered customer support system using Cohere models deployed on AWS, Qdrant as a knowledge base, and Airbyte for continuous data ingestion from sources like Excel files, enabling efficient and automated responses to customer queries via a Retrieval Augmented Generation (RAG) approach. The system ensures secure handling of proprietary support data and leverages connectors for seamless integration, allowing for scalable, multilingual, and accurate query resolution.
- [Chat With Product PDF Manuals Using Hybrid Search](https://qdrant.tech/documentation/examples/hybrid-search-llamaindex-jinaai/): This tutorial explains how to build an advanced RAG-based chatbot that uses hybrid search to answer queries from product PDF manuals, combining components like Jina Embeddings, Qdrant vector database, Mixtral LLM, LlamaIndex, and LlamaParse for data ingestion, retrieval, and natural language response. It provides step-by-step instructions for deploying necessary infrastructure, preparing and parsing documents, configuring embeddings and language models, storing vectors in Qdrant, and assembling prompts for effective hybrid search and customer support applications.
- [Qdrant Cloud API](https://qdrant.tech/documentation/cloud-api/): The Qdrant Cloud API enables automation of the Qdrant Cloud platform through both a high-performance gRPC API and a flexible REST/JSON API, allowing management of accounts, clusters, and related resources; authentication is handled via management API keys. A Terraform provider is also available, and users are advised to transition from the deprecated OpenAPI endpoint, which will be removed after November 1st, 2025.
- [Infrastructure Tools](https://qdrant.tech/documentation/cloud-tools/): Pulumi and Terraform are infrastructure as code tools that enable users to create, deploy, and manage cloud infrastructure using human-readable configuration files. Both tools streamline the process of defining and automating cloud resources.
- [Region-Specific Contract Management System](https://qdrant.tech/documentation/examples/rag-contract-management-stackit-aleph-alpha/): The tutorial outlines how to build a secure, region-specific contract management system using Retrieval Augmented Generation (RAG), leveraging Aleph Alpha embeddings, Qdrant Hybrid Cloud, and the German STACKIT cloud to ensure data residency and access controls compliant with regulations like GDPR. The process covers secure document ingestion, vector storage with per-user access permissions, and streamlined semantic search capabilities, all integrated with business workflows using LangChain for efficient and compliant contract management.
- [Practice Datasets](https://qdrant.tech/documentation/datasets/): Qdrant provides ready-made datasets in snapshot format with pre-computed embeddings (e.g., Arxiv.org titles and abstracts, Wolt food descriptions) for easy import and use in semantic search and machine learning projects, with datasets available for download or from Hugging Face. These datasets save resources by eliminating the need to generate your own embeddings and are suitable for non-commercial and academic purposes.
- [Authentication](https://qdrant.tech/documentation/cloud/authentication/): Qdrant Managed Cloud allows you to create Database API keys with granular access control (available in clusters v1.11.0+), which can be configured, rotated, and used to securely authenticate access to your cluster via REST, gRPC, or official SDKs. It is recommended to use these keys over legacy Admin keys for improved security, and to always include the API key in your request headers when connecting to your cluster.
- [RAG System for Employee Onboarding](https://qdrant.tech/documentation/examples/natural-language-search-oracle-cloud-infrastructure-cohere-langchain/): The document outlines how to build a Retrieval-Augmented Generation (RAG) system for employee onboarding that enables natural language search and chat with company documentation by integrating Cohere language models, Qdrant vector database, and LangChain, all deployed on Oracle Cloud Infrastructure to ensure confidentiality and scalability. It provides step-by-step instructions for setting up the environment, indexing website content, and preparing the system for semantic search, focusing specifically on leveraging Oracle Cloud services and open-source tools.
- [Private RAG Information Extraction Engine](https://qdrant.tech/documentation/examples/rag-chatbot-vultr-dspy-ollama/): This tutorial demonstrates how to build a private, on-premise RAG-based information extraction engine using DSPy, running on Vultr Kubernetes with Ollama-hosted LLMs and Qdrant as a knowledge base, enabling secure processing and structuring of unstructured documents without exposing data to external services. Key components include DSPy for extraction, Qdrant Hybrid Cloud for retrieval, FastEmbed for embeddings, and the integration is suited for regulated industries requiring strict data privacy.
- [Movie Recommendation System](https://qdrant.tech/documentation/examples/recommendation-system-ovhcloud/): This tutorial demonstrates how to build a movie recommendation system using the MovieLens dataset, leveraging collaborative filtering and sparse vectors stored in the Qdrant vector database deployed on OVHcloud's managed Kubernetes for secure, scalable, and efficient similarity searches. Users' movie ratings are normalized and represented as sparse vectors, which are indexed to enable personalized recommendations by comparing user preferences and suggesting films liked by similar users.
- [Blog-Reading Chatbot with GPT-4o](https://qdrant.tech/documentation/examples/rag-chatbot-scaleway/): This tutorial guides users through building a Retrieval-Augmented Generation (RAG) chatbot that leverages GPT-4o, Qdrant Hybrid Cloud, and LangChain to enable semantic search and question-answering over blog content, with deployment instructions focused on maintaining data privacy on Scaleway-managed Kubernetes. Key steps include ingesting and chunking HTML blog data, indexing embeddings in Qdrant, and integrating GPT-4o via LangChain for answer generation.
- [Cluster Access](https://qdrant.tech/documentation/cloud/cluster-access/): After creating a Qdrant Cloud cluster and setting up an API key, you can access your cluster via the Cluster UI, REST API (port 6333), or GRPC API (port 6334), with traffic automatically load balanced across all healthy nodes. Node-specific endpoints are also available for monitoring or shard management and can be found in the cluster detail page.
- [Support](https://qdrant.tech/documentation/support/): Qdrant Cloud offers community support via Discord and dedicated support for paying customers through a Jira Service Management portal, where users can submit tickets detailing their issues and select severity levels to prioritize responses. Customers are encouraged to provide detailed information and use the support bundle script for efficient troubleshooting, with response times determined by their support tier and the severity of the issue.
- [Using Cloud Inference to Build Hybrid Search](https://qdrant.tech/documentation/tutorials-and-examples/cloud-inference-hybrid-search/): This tutorial demonstrates how to build a hybrid semantic search engine with Qdrant Cloud, using cloud inference to embed data, integrating dense semantic embeddings with sparse BM25 keywords, and leveraging Reciprocal Rank Fusion (RRF) for effective hybrid search and reranking. Key steps include installing the Qdrant client, initializing it with cloud inference, creating a collection with both dense and sparse vectors, uploading data, and running a vector search to retrieve and rank relevant results.
- [Monitoring Hybrid/Private Cloud with Prometheus and Grafana](https://qdrant.tech/documentation/tutorials-and-examples/hybrid-cloud-prometheus/): This tutorial explains how to set up Prometheus and Grafana in a Kubernetes cluster for monitoring Qdrant databases in hybrid or private cloud environments, including installation with Helm, configuring Prometheus scraping with ServiceMonitors, accessing Grafana, and importing a pre-built Qdrant dashboard for visualizing metrics. It assumes you have a running Kubernetes cluster with Qdrant deployed and provides step-by-step commands and links to example resources.
- [Qdrant on Databricks](https://qdrant.tech/documentation/send-data/databricks/): This tutorial demonstrates how to use Databricks with Qdrant by first vectorizing a dataset into dense and sparse embeddings using FastEmbed, then storing these embeddings in a Qdrant collection via the Qdrant Spark connector. The step-by-step process includes setting up the environment, preparing data, generating embeddings, constructing a new Spark DataFrame, and uploading the results to Qdrant for efficient vector search and analytics.
- [Semantic Querying with Airflow and Astronomer](https://qdrant.tech/documentation/send-data/qdrant-airflow-astronomer/): This tutorial demonstrates how to use Apache Airflow with the Astronomer platform and Qdrant provider to build a Python-based data pipeline that generates embeddings in parallel from a book dataset and performs semantic retrieval for recommendations. It guides users through setting up the project environment, configuring credentials, ingesting data, and writing a DAG to enable vector search and personalized book suggestions.
- [Tutorials & Examples](https://qdrant.tech/documentation/tutorials-and-examples/): The page provides a tutorial on using cloud inference to implement hybrid search. It includes an example demonstrating this approach.
- [How to Setup Seamless Data Streaming with Kafka and Qdrant](https://qdrant.tech/documentation/send-data/data-streaming-kafka-qdrant/): The guide provides step-by-step instructions for setting up a real-time data streaming pipeline using Kafka (via Confluent), MongoDB, Azure Blob Storage, and Qdrant, with a focus on installing and configuring each component—including the Qdrant Kafka Sink Connector—for seamless integration and efficient vector data ingestion suited for semantic search and Retrieval-Augmented Generation (RAG) applications. By following this setup, users can achieve a scalable, fault-tolerant architecture that supports real-time data capture, processing, and high-performance vector search.
- [Scale Clusters](https://qdrant.tech/documentation/cloud/cluster-scaling/): Qdrant Cloud clusters can be scaled vertically (by increasing resources like CPU, memory, and storage for each node, which may require downtime) or horizontally (by adding nodes and distributing data using shards), and support resharding, which allows you to adjust the number of shards in a collection without downtime to optimize resource utilization as your needs change. The cloud platform automates shard balancing during horizontal scaling and supports transparent resharding on multi-node clusters, though performance may be temporarily reduced during the process.
- [Configure Clusters](https://qdrant.tech/documentation/cloud/configure-cluster/): Qdrant Cloud clusters can be customized with default collection settings, advanced performance options, client IP restrictions, configurable restart modes during maintenance, and automated or manual shard rebalancing strategies to optimize performance and security. These configurations are accessible from the Cluster Details page and allow users to tailor cluster behavior according to their specific requirements.
- [Monitor Clusters](https://qdrant.tech/documentation/cloud/cluster-monitoring/): Qdrant Cloud offers comprehensive cluster monitoring through easily accessible metrics, logs, and automated alerts in its console, while also supporting Prometheus-compatible endpoints (`/metrics` and `/sys_metrics`) for advanced monitoring and integration with tools like Grafana. Authentication via API keys is required for accessing node and system metrics, which include both database and operational infrastructure data, allowing for detailed cluster health and performance tracking.
- [Update Clusters](https://qdrant.tech/documentation/cloud/cluster-upgrades/): To update your Qdrant Cloud cluster to a new version, select the desired version on the Cluster Details page and click "Update." Multi-node clusters with a replication factor of 2 or more update with zero downtime, while single-node clusters or those with a replication factor of 1 will experience brief downtime.
- [Backup Clusters](https://qdrant.tech/documentation/cloud/backups/): Qdrant Cloud Clusters support both automatic and manual backups via the Cloud Dashboard or Snapshot API, allowing users to schedule, manage, and restore backups to protect against data loss or failure. Restoring from a backup resets the cluster to its previous state, and backup costs and mechanisms differ by cloud provider, with incremental backups for AWS/GCP and disk-usage-based costs for Azure.
- [Inference](https://qdrant.tech/documentation/cloud/inference/): Qdrant Managed Cloud offers built-in inference capabilities for generating vector embeddings from text and images using various machine learning models, with usage and billing managed through the cloud console; this feature is currently available only for paid clusters in US regions, and can be enabled or disabled per cluster. Inference can be accessed via Qdrant SDKs or APIs using special input objects, and the Python SDK supports seamless switching between local and cloud inference modes.
- [Distributed Deployment](https://qdrant.tech/documentation/guides/distributed_deployment/): Qdrant supports distributed deployment from v0.8.0, allowing multiple nodes to share data for improved scalability and resilience, with recommended production clusters having three or more nodes and replicated shards for optimal uptime and recovery. Distributed mode can be enabled via configuration (self-hosted) or automatically in Qdrant Cloud, but new nodes require manual collection replication or shard rebalancing, as data is not automatically redistributed.
- [Quantization](https://qdrant.tech/documentation/guides/quantization/): Quantization in Qdrant is an optional feature that reduces memory usage and accelerates search by compressing high-dimensional vectors using methods like scalar, binary, 1.5/2-bit, asymmetric, and product quantization, with each offering tradeoffs between speed, storage efficiency, and search accuracy. Scalar and binary quantization drastically compress vector data and speed up comparisons, while newer techniques (1.5/2-bit and asymmetric) further balance precision and performance, though all methods entail some loss of accuracy depending on parameters and data characteristics.
- [Monitoring & Telemetry](https://qdrant.tech/documentation/guides/monitoring/): Qdrant provides Prometheus/OpenMetrics-compatible endpoints for monitoring, including `/metrics` for node-level metrics and `/sys_metrics` for additional cloud-specific data, and recommends scraping each node individually in clusters to ensure metric consistency. It also offers a `/telemetry` endpoint for database state and Kubernetes health endpoints (`/healthz`, `/livez`, `/readyz`) for server status checks.
- [Configuration](https://qdrant.tech/documentation/guides/configuration/): Qdrant uses default configuration settings suitable for most users, but these can be customized using configuration files in various locations, environment-specific files, or environment variables (which have the highest priority). Configuration sources are loaded and merged in a specific order—with validation at startup—and settings from later sources override earlier ones; however, configuration changes are not allowed on Qdrant Cloud.
- [Security](https://qdrant.tech/documentation/guides/security/): Qdrant is unsecured by default and requires enabling security measures before production use, including API key authentication (with support for both full and read-only keys) and, for advanced control, granular access management via JWT-based RBAC, both of which should be combined with TLS for secure communication. Internal communication ports are not protected by these methods and must be restricted at the network level.
- [Usage Statistics](https://qdrant.tech/documentation/guides/usage-statistics/): Qdrant's open-source container image collects anonymized system, performance, and critical error usage statistics by default to improve the engine, but users can disable this telemetry at any time and request deletion of collected data. No personally identifiable information, user data, or collection names are ever gathered, and several anonymization techniques are used; in Qdrant Cloud, telemetry is disabled by default.
- [Troubleshooting](https://qdrant.tech/documentation/guides/common-errors/): To resolve “Too many files open (OS error 24),” increase the open file limit using `ulimit` or Docker’s `--ulimit` flag. For collection WAL errors or gRPC issues with Python's `multiprocessing`, ensure each Qdrant node has its own storage and use the `forkserver` or `spawn` process start methods, or switch to the REST API or async client.
- [Migration to Qdrant](https://qdrant.tech/documentation/database-tutorials/migration/): The Qdrant Migration Tool streamlines data migration between Qdrant instances and from other vector databases (like Pinecone), supporting features such as live batch streaming, resuming interrupted migrations, and collection reconfiguration, and can be run via Docker without requiring direct connectivity. Unlike native snapshotting, it offers greater flexibility and reliability for cross-platform and cross-region migrations.
- [Static Embeddings. Should you pay attention?](https://qdrant.tech/documentation/database-tutorials/static-embeddings/): Static embeddings are experiencing a resurgence due to new techniques that significantly speed up vector creation and reduce model size with minimal quality loss—achieving up to 400-500x faster performance compared to transformer-based models—making them ideal for resource-constrained environments like mobile apps and IoT devices. While retrieval speed in vector databases like Qdrant remains the same, static embeddings offer substantial benefits in encoding speed and cost-efficiency, with further potential gains when combined with quantization methods.
- [OpenLLMetry](https://qdrant.tech/documentation/observability/openllmetry/): OpenLLMetry by Traceloop is a set of OpenTelemetry-based extensions that provide comprehensive observability for LLM applications, specifically supporting instrumentation of the `qdrant_client` Python library and trace export to various platforms. Users can enable tracing by installing and initializing either the Traceloop SDK or the standalone OpenTelemetry instrumentation package for Qdrant.
- [OpenLIT](https://qdrant.tech/documentation/observability/openlit/): OpenLIT is an OpenTelemetry-native observability tool for LLM applications that provides auto-instrumentation to monitor Qdrant, offering insights to improve database operations and application performance. Installation and integration require just two lines of code, with options to forward telemetry data to various backends, and further data visualization and integration are available through its documentation.
- [Airbyte](https://qdrant.tech/documentation/data-management/airbyte/): Airbyte is an open-source data integration platform that enables easy data replication between systems and can be used to ingest data into Qdrant for search engine functionality, supporting various sync modes and embedding configurations. To use Airbyte with Qdrant, you need running instances of both platforms, after which you can set up Qdrant as a destination within Airbyte, configure text processing and embeddings, and create source-to-destination connections through the UI.
- [Aleph Alpha](https://qdrant.tech/documentation/embeddings/aleph-alpha/): Aleph Alpha provides multimodal and multilingual embeddings for text and images in a shared latent space, accessible via their official Python client, which supports both synchronous and asynchronous usage. These embeddings can be generated from text or images and stored in Qdrant, with specific methods for each input type (`Prompt.from_image` for images and `Prompt.from_text` for text).
- [Apache Airflow](https://qdrant.tech/documentation/data-management/airflow/): Apache Airflow is an open-source Python platform for managing workflows, and it offers a Qdrant provider that enables easy integration with Qdrant databases through hooks and operators for tasks like data ingestion. To use this, install the `apache-airflow-providers-qdrant` package, configure a Qdrant connection in Airflow, and leverage the provided hook and operator classes for interacting with Qdrant within your workflows.
- [Apache Spark](https://qdrant.tech/documentation/data-management/spark/): Apache Spark is a distributed computing framework for big data processing, and the Qdrant-Spark connector allows Spark to use Qdrant as a storage destination by supporting various vector formats and payloads via configurable options. Installation involves downloading or building the JAR file, configuring it in your Spark or Databricks environment, and specifying the appropriate options when writing data.
- [Apify](https://qdrant.tech/documentation/platforms/apify/): Apify is a web scraping and browser automation platform with over 1,500 pre-built "Actors," including the Website Content Crawler, which can cleanly extract website data for AI and RAG applications and integrate it directly with Qdrant for vector storage. Programmatic usage via the Apify Python SDK allows users to automate data extraction and upload cleaned, chunked content as embeddings to a Qdrant collection for further AI model training or retrieval.
- [Autogen](https://qdrant.tech/documentation/frameworks/autogen/): Microsoft AutoGen is an open-source framework for building customizable, collaborative AI agent workflows—including multi-agent conversations and human participation—with support for integrating Qdrant for enhanced retrieval capabilities. It provides flexible installation and usage options, allowing developers to configure agents, connect to vector databases like Qdrant, and run sophisticated agent interactions for complex tasks.
- [AWS Bedrock](https://qdrant.tech/documentation/embeddings/bedrock/): You can integrate AWS Bedrock embeddings, such as the Titan Embeddings G1 - Text model, with Qdrant by configuring AWS credentials (region, access key ID, and secret key) and using sample Python or JavaScript code to generate and upload 1536-dimensional embeddings to your Qdrant collection. Detailed steps and code samples for both languages are provided, requiring the appropriate AWS and Qdrant client libraries.
- [AWS Lakechain](https://qdrant.tech/documentation/frameworks/lakechain/): AWS Lakechain is a CDK-based framework for building scalable, modular document processing pipelines on AWS, offering over 60 components and supporting integration with Qdrant for storing vector embeddings. The Qdrant storage connector can be easily configured via code to upload embeddings and, optionally, document text to a specified Qdrant collection, using an API key managed in AWS Secrets Manager.
- [BuildShip](https://qdrant.tech/documentation/platforms/buildship/): BuildShip is a low-code visual builder for creating APIs, scheduled jobs, and backend workflows with AI assistance, featuring integration with Qdrant for semantic search capabilities. The Qdrant integration provides nodes for adding, retrieving, deleting, and searching points within workflows.
- [CamelAI](https://qdrant.tech/documentation/frameworks/camel/): Camel is a Python framework for building and using LLM-based agents, supporting integration with Qdrant as a vector database for efficient data storage and retrieval via its `QdrantStorage` class and compatible retrievers. Users can configure Camel to ingest, store, and query semantically similar data, automate workflows with Auto Retriever, and find end-to-end usage examples in its documentation and cookbooks.
- [Cheshire Cat](https://qdrant.tech/documentation/frameworks/cheshire-cat/): Cheshire Cat is an open-source framework for building intelligent agents on Large Language Models, leveraging Qdrant as its default vector memory for efficient document retrieval and management. It is easily launched via Docker, configurable through a web-based admin portal, and offers extensive documentation and community support for further development.
- [CocoIndex](https://qdrant.tech/documentation/data-management/cocoindex/): CocoIndex is a high-performance, stateful ETL framework for AI data transformation with real-time incremental processing, using Postgres for metadata tracking and Qdrant as a built-in vector database for embeddings storage. Users can install CocoIndex via pip and set up Postgres with Docker Compose, and must configure collection details before exporting data to Qdrant.
- [cognee](https://qdrant.tech/documentation/data-management/cognee/): Cognee is a Python-based memory management tool for AI apps and agents that integrates Qdrant as a built-in vector database for storing and retrieving embeddings. It supports Python 3.8–3.12, is installable via common package managers, and enables users to add, generate, and query knowledge graphs with simple code and environment setup.
- [Cohere](https://qdrant.tech/documentation/embeddings/cohere/): Qdrant integrates with Cohere's co.embed API, allowing users to generate and store embeddings in Qdrant via Cohere's Python SDK, with special support for the advanced Embed v3 model that requires specifying an input type for tasks like search or classification. For semantic search applications, documents and queries should be embedded separately using appropriate input types, and v3 models support multiple similarity metrics that yield identical rankings.
- [Confluent Kafka](https://qdrant.tech/documentation/data-management/confluent/): Confluent Cloud, developed by the original creators of Apache Kafka, is a fully managed, cloud-native data streaming platform with robust features and integrations, including the Qdrant-Kafka Sink Connector that enables seamless streaming of data from Confluent Cloud to Qdrant. The connector supports various vector message formats and can be easily set up on Confluent Cloud for real-time data ingestion into Qdrant.
- [CrewAI](https://qdrant.tech/documentation/frameworks/crewai/): CrewAI is a framework that enables collaborative, role-playing AI agents to work on complex tasks using advanced memory systems—including short-term and entity memory—that can be integrated and enhanced with Qdrant for vector-based storage and retrieval. The guide explains how to set up CrewAI with Qdrant to manage agent memories using custom storage classes, allowing for more effective information recall and relationship mapping during agent workflows.
- [Dagster](https://qdrant.tech/documentation/frameworks/dagster/): Dagster is a Python framework for data orchestration that offers integrated lineage, observability, and strong testability. The `dagster-qdrant` library allows seamless integration with Qdrant's vector database to build and manage AI-driven data pipelines within Dagster, including vector searches and data management.
- [Datadog](https://qdrant.tech/documentation/observability/datadog/): Datadog is a cloud-based monitoring platform that provides real-time visibility and analytics for servers, databases, and applications, enabling businesses to detect and mitigate issues proactively. The Qdrant integration allows users to monitor Qdrant deployments in Datadog by collecting key performance and readiness metrics via the Datadog Agent, configurable through YAML settings and supporting options like authentication.
- [DeepEval](https://qdrant.tech/documentation/frameworks/deepeval/): DeepEval is an open-source framework by Confident AI for testing large language model (LLM) outputs using customizable metrics like answer relevancy, faithfulness, hallucination, and G-Eval, with support for both single-turn and conversational evaluation through test cases. It integrates with tools like Qdrant for evaluating RAG pipelines, and results can be monitored on the Confident AI Dashboard for scalable, dataset-driven assessments.
- [DLT](https://qdrant.tech/documentation/data-management/dlt/): DLT is an open-source Python library that simplifies loading messy data from various sources into live, structured datasets, now supporting Qdrant as a destination with features like schema inference, automated maintenance, and easy configuration. To use, install the library with Qdrant support, configure your credentials, define your data and pipeline, and run it to load data into Qdrant, enabling vector search and supporting all write dispositions.
- [Dynamiq](https://qdrant.tech/documentation/frameworks/dynamiq/): Dynamiq is a Gen AI framework that simplifies the development of AI-powered applications by orchestrating retrieval-augmented generation (RAG) and large language model (LLM) agents, with built-in support for Qdrant as a vector database for document storage and retrieval. It provides easy-to-use retriever and writer nodes for managing documents, along with tutorials and documentation for further guidance.
- [Feast](https://qdrant.tech/documentation/frameworks/feast/): Feast is an open-source feature store for managing and serving production ML features at scale, and it now supports Qdrant as an online vector store by installing Feast with the `qdrant` extra. Users can configure Qdrant in their Feast setup and retrieve document vectors for entity keys, with further details available in the Feast documentation.
- [FiftyOne](https://qdrant.tech/documentation/frameworks/fifty-one/): FiftyOne is an open-source Python toolkit that improves computer vision workflows by optimizing dataset quality and providing insights into models, now featuring native integration with Qdrant for image and text similarity search using vector embeddings. It can be installed via pip, and further information on Qdrant integration is available in the documentation.
- [Firebase Genkit](https://qdrant.tech/documentation/frameworks/genkit/): Firebase Genkit is a framework for building and deploying AI-powered applications, supporting features like semantic search and content generation, with server-side JavaScript/TypeScript support. The Qdrant-Genkit plugin enables integration of Qdrant for semantic data retrieval, allowing configuration of indexing and retrieval within Genkit apps.
- [Gemini](https://qdrant.tech/documentation/embeddings/gemini/): Qdrant is compatible with Google's Gemini Embedding Models, allowing users to embed, index, and search documents using various `task_type` settings for optimal semantic retrieval, and supports advanced features like Binary Quantization to significantly reduce storage requirements while maintaining high recall. The guide provides step-by-step instructions for integrating Gemini embeddings into Qdrant and highlights best practices for semantic search tasks.
- [Haystack](https://qdrant.tech/documentation/frameworks/haystack/): Haystack is a modular NLP framework for building advanced AI applications, and it now integrates with the externally maintained QdrantDocumentStore for efficient vector-based text storage and retrieval. The QdrantDocumentStore is available as a separate installable package, supports all Qdrant Python client configuration options, and enables custom collection settings, such as enabling scalar quantization.
- [HoneyHive](https://qdrant.tech/documentation/frameworks/honeyhive/): HoneyHive is an AI observability platform that equips developers with tools to debug, monitor, and evaluate Generative AI and Retrieval-Augmented Generation (RAG) pipelines, including integration with vector databases like Qdrant for tracing, performance evaluation, and parameter optimization. The platform offers Python-based tracing for embedding generation, document insertion, retrieval, and response generation in RAG workflows, helping teams iterate efficiently and deliver robust AI products.
- [InfinyOn Fluvio](https://qdrant.tech/documentation/data-management/fluvio/): Fluvio is an open-source, cloud-native data streaming platform written in Rust, enabling high-speed real-time processing across diverse infrastructures. Using the Qdrant Fluvio Connector, users can stream data from Fluvio topics to Qdrant collections with strong delivery guarantees and support for various vector formats, after installing both platforms and configuring the connector.
- [Jina Embeddings](https://qdrant.tech/documentation/embeddings/jina-embeddings/): Qdrant integrates seamlessly with Jina AI’s multilingual embedding models—including the recommended jina-embeddings-v4—which support various tasks (retrieval, code, text-matching) and offer configurable embedding dimensions and features like Matryoshka Representation Learning and Late Chunking; users can obtain a 10% discount on Jina AI APIs using the code QDRANT. Example code demonstrates how to generate and use both text and image embeddings from Jina API with Qdrant for retrieval tasks.
- [Keboola](https://qdrant.tech/documentation/platforms/keboola/): Keboola is a data operations platform that enables seamless integration, transformation, and management of data, including vector embeddings, through its Qdrant component for advanced AI and semantic search applications. To use this integration, you need accounts for both Keboola and Qdrant, after which you can orchestrate workflows such as building Retrieval Augmented Generation (RAG) systems directly within Keboola.
- [Kotaemon](https://qdrant.tech/documentation/platforms/kotaemon/): Kotaemon is an open-source, customizable RAG UI for document chat, supporting Qdrant as a vectorstore for document ingestion and retrieval. To use Qdrant, update the `flowsettings.py` configuration with your Qdrant details and restart Kotaemon.
- [Langchain](https://qdrant.tech/documentation/frameworks/langchain/): Langchain is a library that simplifies building LLM-based applications by unifying access to major embedding providers and vector stores like Qdrant, supporting dense, sparse, and hybrid document retrieval modes. Its Qdrant integration allows for easy storage and search of text embeddings, flexible deployment (in-memory, on-disk, or server), and multiple search strategies, minimizing boilerplate and focusing on business value.
- [Langchain4J](https://qdrant.tech/documentation/frameworks/langchain4j/): LangChain for Java (Langchain4J) is a community-created port of Langchain that enables building context-aware AI applications in Java, with support for using Qdrant as a vector store through the `langchain4j-qdrant` module. To use it, add the appropriate dependency to your project and configure the Qdrant connection with your collection name, host URL, and API key.
- [LangGraph](https://qdrant.tech/documentation/frameworks/langgraph/): LangGraph is a library for building stateful, multi-actor applications and agentic workflows, offering flexible flow control, cycles, persistence, and seamless integration with LangChain for advanced retrieval and memory features. It supports Python and JavaScript, allowing easy creation of retrieval nodes using tools like Qdrant and enabling integration into agent workflows.
- [LlamaIndex](https://qdrant.tech/documentation/frameworks/llama-index/): LlamaIndex serves as an interface that connects external data to Large Language Models, enabling users to augment LLMs with private data and simplifying data ingestion and indexing by integrating vector databases like Qdrant. Installation requires separate setup of both LlamaIndex and its Qdrant integration, after which LlamaIndex can interact with a Qdrant server via a `QdrantClient` instance.
- [Make.com](https://qdrant.tech/documentation/platforms/make/): Make.com is a no-code platform for automating tasks and workflows, including integration with Qdrant via specialized modules. To use Qdrant with Make, you need accounts on both services, set up a connection, and can then trigger actions and map data between modules within your workflows.
- [Mastra](https://qdrant.tech/documentation/frameworks/mastra/): Mastra is a TypeScript framework for building AI applications quickly, providing primitives such as workflows, agents, RAG, and integrations, with support for running locally or deploying to the cloud. It integrates with Qdrant as a vector store, offering functions for creating and managing indexes, upserting and querying vectors, and retrieving results with customizable options.
- [Mem0](https://qdrant.tech/documentation/frameworks/mem0/): Mem0 is a self-improving memory layer for LLM applications that personalizes AI by remembering user preferences, supporting multiple vector stores like Qdrant for efficient data management. It can be easily installed via pip and offers flexible configuration and advanced query capabilities for AI systems.
- [Microsoft NLWeb](https://qdrant.tech/documentation/frameworks/nlweb/): Microsoft's NLWeb is a framework for enabling natural language interfaces on websites using Schema.org, RSS, and the MCP protocol, supporting Qdrant as a vector store backend for embedding storage and retrieval. It includes default Qdrant integration, can be easily installed and configured, and allows users to load content via RSS feeds and query it using a web UI or REST API.
- [Mistral](https://qdrant.tech/documentation/embeddings/mistral/): Qdrant now supports integration with the Mistral Embedding models via the official Python SDK, allowing users to easily embed documents, store, and search them using Qdrant’s vector database. Additionally, Mistral Embeddings are compatible with binary quantization, significantly reducing storage size while maintaining high retrieval quality.
- [MixedBread](https://qdrant.tech/documentation/embeddings/mixedbread/): MixedBread provides versatile, state-of-the-art embedding models suitable for various search and Retrieval Augmented Generation (RAG) tasks, which can be easily integrated with Qdrant to enhance search solutions. Installation is straightforward via pip, and embeddings can be generated and stored in Qdrant collections using a simple Python workflow.
- [Mixpeek](https://qdrant.tech/documentation/embeddings/mixpeek/): Mixpeek enables chunking and embedding of videos for semantic search, with Qdrant providing storage and efficient retrieval of these embeddings; users process videos, create embeddings for each chunk, insert them into a Qdrant collection, and can then perform similarity searches using text or video queries. Required setup includes installing the Mixpeek and Qdrant clients, setting API keys, and ensuring compatibility with Python 3.7+.
- [N8N](https://qdrant.tech/documentation/platforms/n8n/): N8N is an automation platform that supports deep data integration, and with the official Qdrant node, users can add semantic search capabilities to their workflows by connecting to a Qdrant instance. The Qdrant node allows a wide range of operations on collections, points, vectors, searches, and payloads, facilitating advanced data handling and automation within N8N.
- [Neo4j GraphRAG](https://qdrant.tech/documentation/frameworks/neo4j-graphrag/): Neo4j GraphRAG is a Python package for building graph retrieval augmented generation (GraphRAG) applications with Neo4j, offering robust features, high performance, and native integration with Qdrant for vector search. It supports both direct vector queries and integration with Langchain embedding providers for automatic text vectorization.
- [Nomic](https://qdrant.tech/documentation/embeddings/nomic/): The `nomic-embed-text-v1` is an open-source text encoder model with an 8192-context length, accessible via the Nomic API/SDK, FastEmbed, or direct HTTP requests, and can be used for creating and searching vector embeddings in databases like Qdrant. The model requires setting the `task_type` parameter (`search_document` for documents, `search_query` for queries) and proper API token configuration to generate embeddings for upserts and searches.
- [Nvidia](https://qdrant.tech/documentation/embeddings/nvidia/): Qdrant integrates with Nvidia embeddings, allowing users to generate and search high-dimensional vector representations of text using the NV-Embed-QA model and an Nvidia API key. The process involves embedding documents, storing them as points in Qdrant, and performing similarity searches using the generated vectors.
- [Ollama](https://qdrant.tech/documentation/embeddings/ollama/): Ollama offers specialized embedding models suitable for niche RAG applications and can be integrated with Qdrant by generating embeddings from text and storing them as vectors in a Qdrant collection. The integration requires installing both `ollama` and `qdrant-client` packages, and a sample code demonstrates connecting to local servers, creating collections, and uploading embedded data.
- [OpenAI](https://qdrant.tech/documentation/embeddings/openai/): Qdrant integrates seamlessly with OpenAI's embedding models, allowing users to generate, store, and search high-dimensional document embeddings using an official Python client and simple code setup. Additionally, OpenAI embeddings can be effectively compressed with Qdrant's Binary Quantization, significantly reducing embedding size while maintaining high recall in search tasks.
- [Pipedream](https://qdrant.tech/documentation/platforms/pipedream/): Pipedream is a platform for building automated workflows by connecting various apps, data sources, and APIs, offering code-level customization; it can integrate with Qdrant via the Qdrant app to add vector search capabilities to workflows. To use Qdrant in Pipedream, you need a Qdrant instance and a Pipedream project, after which you can connect, authenticate, and use pre-built actions within your workflows.
- [Power Apps](https://qdrant.tech/documentation/platforms/powerapps/): Microsoft Power Apps is a rapid development platform for building custom business apps that can connect to various data sources, including using the Qdrant Connector for adding vector search capabilities. To use the Qdrant Connector, you need a Qdrant instance and a Power Apps account; once set up, you can integrate Qdrant actions into your Power Apps flows.
- [Prem AI](https://qdrant.tech/documentation/embeddings/premai/): PremAI is a unified platform for developing, fine-tuning, deploying, and monitoring generative AI models, compatible with Qdrant for vector storage and search. The documentation provides code examples in Python and TypeScript for installing SDKs, setting up clients, generating embeddings, storing them in Qdrant, and performing semantic searches.
- [PrivateGPT](https://qdrant.tech/documentation/platforms/privategpt/): PrivateGPT is a production-ready AI project enabling users to query their documents with Large Language Models (LLMs) offline, utilizing Qdrant as the default vectorstore for document ingestion and retrieval. Qdrant settings can be customized in the `settings.yaml` file, allowing configuration of connection details such as URL, API key, ports, and other options.
- [Pulumi](https://qdrant.tech/documentation/cloud-tools/pulumi/): Pulumi is an open source infrastructure as code tool that supports using Qdrant through a generated SDK, allowing users to configure, deploy, and manage Qdrant cloud infrastructure via Pulumi in various programming languages. Supported resources include managing clusters and API keys, as well as querying package, key, and cluster information, with setup requiring Pulumi installation and a Qdrant cloud API key.
- [Redpanda Connect](https://qdrant.tech/documentation/data-management/redpanda/): Redpanda Connect is a declarative, data-agnostic streaming service that enables efficient, resilient, and stateless data processing with at-least-once delivery, configured via a YAML file organizing various input, processor, and output components. The Qdrant Output component allows streaming of vector data into Qdrant collections within Redpanda pipelines, and supports flexible batching, authentication, and payload mapping options.
- [Rig-rs](https://qdrant.tech/documentation/frameworks/rig-rs/): Rig-rs is a Rust library designed to simplify the creation of scalable, modular LLM-powered applications, with full support for language model completion, embeddings, and integration with Qdrant for semantic document storage and retrieval. It enables developers to ingest documents and perform vector-based search with minimal code using OpenAI embeddings and the Qdrant vector store.
- [Salesforce Mulesoft](https://qdrant.tech/documentation/platforms/mulesoft/): Salesforce MuleSoft Anypoint is an integration platform for connecting applications, data, and devices, while the open-source MAC Project adds AI capabilities (like LLMs and vector databases such as Qdrant) into the MuleSoft ecosystem via specialized connectors. The Mulesoft Vectors connector allows users to configure and perform operations such as adding, listing, querying, and removing documents in Qdrant collections within their MuleSoft projects.
- [Semantic-Router](https://qdrant.tech/documentation/frameworks/semantic-router/): Semantic-Router is a library for building decision-making layers in LLMs and agents using vector embeddings to semantically route tool-use decisions, with support for Qdrant as an index for route data storage and retrieval. Installation involves `pip install semantic-router[qdrant]`, and with minimal setup, routes can be defined and efficiently managed for different conversational contexts.
- [SmolAgents](https://qdrant.tech/documentation/frameworks/smolagents/): SmolAgents is a HuggingFace Python library for creating AI agents that use code-based tool orchestration via LLMs, shown to be more efficient and perform better on benchmarks than traditional dictionary-based approaches. It can be integrated with Qdrant for semantic search, as demonstrated with a movie recommendation agent that queries a vector database to retrieve relevant movie information.
- [Snowflake Models](https://qdrant.tech/documentation/embeddings/snowflake/): Qdrant supports integration with Snowflake’s text embedding models (available on HuggingFace), enabling users to generate and store vector embeddings of text documents for efficient search and retrieval. The provided examples show how to set up, embed, store, and search documents using the `snowflake-arctic-embed-s` model in both Python and TypeScript.
- [Spring AI](https://qdrant.tech/documentation/frameworks/spring-ai/): Spring AI is a Java framework offering Spring-style APIs for AI application development, supporting integration with Qdrant as a vector database through straightforward configuration in Spring Boot or Java code. The Qdrant vector store can be set up via application properties or a config bean, enabling use with any supported Spring AI embedding provider, and collections are auto-created if not pre-existing.
- [Stanford DSPy](https://qdrant.tech/documentation/frameworks/dspy/): Stanford DSPy is a Python framework that simplifies building advanced language model applications by integrating prompting, fine-tuning, reasoning, and retrieval (including with Qdrant as a retriever). It offers declarative modules, an automatic compiler, and a straightforward API for configuring and using retrieval-augmented generation (RAG) and other LM-powered workflows.
- [Sycamore](https://qdrant.tech/documentation/frameworks/sycamore/): Sycamore is an LLM-powered system for preparing, processing, and analyzing complex, unstructured documents, and it provides connectors to read from and write to Qdrant collections for managing document data. Writing and reading operations use the `docset.write.qdrant()` and `docset.read.qdrant()` functions, which accept various configuration parameters for customized interaction with Qdrant.
- [Terraform](https://qdrant.tech/documentation/cloud-tools/terraform/): HashiCorp Terraform is an infrastructure as code tool for managing both cloud and on-prem resources, and with the Qdrant Terraform Provider, users can automate the provisioning and management of Qdrant cloud clusters and related resources. To use the provider, you need a Terraform installation and a Qdrant API key, and it supports managing clusters, API keys, and retrieving related information.
- [Testcontainers](https://qdrant.tech/documentation/frameworks/testcontainers/): Testcontainers is a library that enables integration testing with real services in Docker containers, offering easy-to-use APIs in multiple languages. The Qdrant module allows developers to quickly spin up a Qdrant instance for end-to-end testing, with options to customize container configurations.
- [ToolJet](https://qdrant.tech/documentation/platforms/tooljet/): ToolJet is a low-code platform for building business applications that can connect to various data sources, including Qdrant, via a plugin in its marketplace. By configuring the Qdrant plugin, users can perform operations such as listing collections, viewing collection info, upserting, retrieving, deleting, and querying points within their Qdrant instance directly from ToolJet.
- [Twelve Labs](https://qdrant.tech/documentation/embeddings/twelvelabs/): Twelve Labs provides an Embed API that generates unified vector embeddings for videos, text, images, and audio, enabling cross-modal semantic search and advanced applications like sentiment analysis and recommendation systems. The guide demonstrates how to use the API with Python and Node SDKs to embed various content types, store and query them in Qdrant, and perform searches across different modalities.
- [txtai](https://qdrant.tech/documentation/frameworks/txtai/): txtai enables building semantic search applications using neural embeddings, and Qdrant can be used as an embedding backend by installing the additional qdrant-txtai package. More information and examples are available in the qdrant-txtai repository.
- [Unstructured](https://qdrant.tech/documentation/data-management/unstructured/): Unstructured is a library for preprocessing and structuring unstructured text documents for machine learning, supporting integration with Qdrant as an ingestion destination. It can be installed with Qdrant support and used via CLI or programmatically, allowing users to configure document processing, embedding, and upload workflows.
- [Upstage](https://qdrant.tech/documentation/embeddings/upstage/): Qdrant integrates with Upstage's Solar Embeddings API, allowing users to generate and store high-dimensional text embeddings (size 4096) for both documents and queries using dual models, and perform vector similarity search within a unified vector space. Authentication is performed via API key, and examples are provided for embedding, storing, and searching text data in both Python and TypeScript.
- [Vanna.AI](https://qdrant.tech/documentation/frameworks/vanna-ai/): Vanna.AI is a Python package that leverages retrieval-augmented generation (RAG) and LLMs to generate accurate SQL queries for your database, supporting integration with Qdrant as a vector store and various SQL databases. Users train the model with schema, documentation, and example queries, then ask natural language questions to receive relevant SQL statements.
- [VectaX - Mirror Security](https://qdrant.tech/documentation/frameworks/mirror-security/): VectaX by Mirror Security is an AI-driven access control and encryption system that secures vector embeddings through similarity-preserving encryption and fine-grained RBAC, enabling protected storage, search, and operations in vector databases like Qdrant. It allows integration via the Mirror SDK to encrypt vectors, define role-based policies, generate access keys, and ensure that only authorized users can decrypt and access specific vector data.
- [Vectorize.io](https://qdrant.tech/documentation/platforms/vectorize/): Vectorize.io is a SaaS platform that automates data extraction from multiple sources and enables rapid deployment and evaluation of real-time RAG pipelines for unstructured data, integrating seamlessly with Qdrant for vector storage and immediate processing of source updates. Setting up requires accounts with Vectorize and Qdrant, after which users can configure pipelines by connecting their vector database, selecting an embeddings provider, and choosing a data source.
- [VoltAgent](https://qdrant.tech/documentation/frameworks/voltagent/): VoltAgent is an open-source TypeScript framework for building AI agents with modular tool integration, LLM coordination, adaptable multi-agent architectures, and features an observability dashboard for monitoring and debugging. It supports seamless integration with Qdrant for vector search, enables embedding generation via OpenAI, and provides example implementations for retrieval-augmented generation workflows.
- [Voyage AI](https://qdrant.tech/documentation/embeddings/voyage/): Qdrant integrates with Voyage AI embeddings, allowing users to generate sentence embeddings (e.g., with the "voyage-large-2" model), store them as vectors in Qdrant collections, and perform vector searches for relevant documents. Setup involves obtaining a Voyage AI API key, embedding texts, converting embeddings to Qdrant points, inserting them into collections, and querying for similar documents.
- [How vector search should be benchmarked?](https://qdrant.tech/benchmarks/benchmarks-intro/): Qdrant prioritizes efficient resource usage to deliver fast, accurate, and cost-effective vector database performance, backing its design choices with comparative, reproducible, and open-sourced benchmarks against other vector search engines. Benchmark scenarios include single node upload & search, filtered search, with plans for memory consumption and cluster mode tests.
- [Single node benchmarks](https://qdrant.tech/benchmarks/single-node-speed-benchmark/): Qdrant outperformed other vector search engines by achieving the highest requests-per-second (RPS) and lowest latencies across almost all scenarios, while Elasticsearch improved speed but remained slow in indexing, Milvus excelled at indexing speed but lagged in RPS and latency for larger or high-dimensional datasets, Redis performed well at lower precision with latency increasing under more parallel requests, and Weaviate showed the least improvement since the last benchmark. Benchmarks were run fairly with each engine limited to 25GB RAM, using multiple datasets to evaluate both parallel (RPS) and single-request (latency) performance under varying precision thresholds.
- [Single node benchmarks (2022)](https://qdrant.tech/benchmarks/single-node-speed-benchmark-2022/): This is an archived version of Single node benchmarks. For the latest information, please refer to the new version at /benchmarks/single-node-speed-benchmark/.
- [Filtered search benchmark](https://qdrant.tech/benchmarks/filtered-search-intro/): Filtered search introduces significant complexity to search engines because most ANN algorithms struggle to efficiently incorporate filters, with existing solutions (pre-filtering and post-filtering) having scalability and accuracy issues. The Qdrant team has created benchmark datasets to test engines in this scenario and proposes an alternative approach that avoids the limitations of pre- and post-filtering while maintaining accuracy.
- [Benchmarks F.A.Q.](https://qdrant.tech/benchmarks/benchmark-faq/): The Qdrant team acknowledges potential bias in their open-source vector database benchmarks but strives for fairness by using consistent hardware, focusing on search precision, speed, and resource use at standardized precision thresholds, and testing primarily with official Python clients for reproducibility. They only test open-source databases (not closed SaaS platforms) and encourage community contributions and improvements via their public GitHub repository.
- [Bug Bounty Program](https://qdrant.tech/security/bug-bounty-program/): Qdrant’s Bug Bounty Program invites responsible disclosure of security vulnerabilities in its cloud application and website, rewarding researchers based on the impact and quality of their reports, while strictly prohibiting social engineering, unauthorized account access, and issues deemed out-of-scope. Submissions must be detailed and reproducible, follow ethical guidelines, and abide by disclosure confidentiality; all vulnerability reports are assessed using the CVSS v4 framework, and safe harbor protections apply when the program’s rules are followed.
- [Credits](https://qdrant.tech/legal/credits/): Icons are designed by srip and available on flaticon.com. The email marketing vector was created by storyset and can be found on freepik.com.
- [Impressum](https://qdrant.tech/legal/impressum/): Qdrant Solutions GmbH, represented by André Zayarni and based in Berlin, provides company and legal information, outlines data privacy practices, and disclaims liability for external links and content accuracy on their website. The site uses Google Analytics and warns users about potential data security risks, stating personal data is only collected voluntarily and not shared without consent.
- [Privacy Policy](https://qdrant.tech/legal/privacy-policy/): Qdrant Solutions GmbH collects and processes personal data on its website, cloud panel, and social media for purposes including service provision, customer support, marketing, and legal compliance, based on relevant GDPR legal frameworks, while implementing safeguards for international data transfers and allowing data subjects rights such as access, correction, objection, and deletion. Data is retained only as necessary, and users can contact Qdrant for privacy concerns or to withdraw consent at any time.
- [Terms and Conditions](https://qdrant.tech/legal/terms_and_conditions/): By using the Qdrant website or service, users agree to be bound by these Terms and Conditions, which define the rights and obligations of all parties, limit the Company's liability, and provide services "as is" without warranties; users must be over 18, comply with the Privacy Policy, and acknowledge the Company is not responsible for third-party sites. Violation of these terms may result in immediate termination of access, and disputes should be resolved by contacting the Company first, with local laws applying as relevant.
