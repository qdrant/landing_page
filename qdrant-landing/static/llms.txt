# https://qdrant.tech/ llms.txt
## Overall Summary
> Qdrant is a cutting-edge platform focused on delivering exceptional performance and efficiency in vector similarity search. As a robust vector database, it specializes in managing, searching, and retrieving high-dimensional vector data, essential for enhancing AI applications, machine learning, and modern search engines. With a suite of powerful features such as state-of-the-art hybrid search capabilities, retrieval-augmented generation (RAG) applications, and dense and sparse vector support, Qdrant stands out as an industry leader. Its offerings include managed cloud services, enabling users to harness the robust functionality of Qdrant without the burden of maintaining infrastructure. The platform supports advanced data security measures and seamless integrations with popular platforms and frameworks, catering to diverse data handling and analytic needs. Additionally, Qdrant offers comprehensive solutions for complex searching requirements through its innovative Query API and multivector representations, allowing for precise matching and enhanced retrieval quality. With its commitment to open-source principles and continuous innovation, Qdrant tailors solutions to meet both small-scale projects and enterprise-level demands efficiently, helping organizations unlock profound insights from their unstructured data and optimize their AI capabilities.

## Page Links
- [Concepts](https://qdrant.tech/documentation/concepts/): Qdrant's glossary introduces key AI concepts, such as Collections (sets of searchable points), Payloads (associated vector data), and Points (vector records). It also covers tools like Search, Explore, Filtering, Hybrid Queries, and processes like Indexing, Optimization, and Snapshots for efficient storage and retrieval in vector databases.
- [Collections](https://qdrant.tech/documentation/concepts/collections/): A collection in Qdrant is a structured set of points (vectors) that support searching, with specific dimensionality and distance metrics like Dot product, Cosine similarity, and Euclidean distance. Collections can be customized with various configurations (e.g., multitenancy, on-disk storage, sparse/dense vectors, and indexing options), allowing flexibility in structure, storage, and performance tuning, including features like multiple named vectors and uint8 embeddings.
- [Points](https://qdrant.tech/documentation/concepts/points/): Qdrant uses "points" as its core data structure, which consist of vectors and optional payloads, and these are searchable within collections via vector similarity. It supports dense, sparse, and named vectors, allows batch uploads with idempotent APIs, and provides methods for modifying points, including updating and deleting vectors or payloads, while ensuring data consistency with a write-ahead-log mechanism.
- [Vectors](https://qdrant.tech/documentation/concepts/vectors/): Qdrant's vector search engine uses vectors (or embeddings) to represent object similarity in vector space, with dense vectors (fixed-length numerical arrays) as the most common type, while also supporting sparse vectors, multivectors, and named vectors for diverse use cases. By offering options like Float32, Float16, and Uint8 data types, Qdrant optimizes memory use and precision, enabling tasks like similarity search, clustering, and cross-modal representation of data.
- [Payload](https://qdrant.tech/documentation/concepts/payload/): Qdrant enables storing additional metadata, called "payloads," alongside vectors in JSON format and supports various data types like integers, floats, keywords, geo-coordinates, and datetimes. It offers flexible methods for payload management, including setting, overwriting, clearing, deleting, and indexing payloads, which enhance customization and search performance in dynamic datasets.
- [Search](https://qdrant.tech/documentation/concepts/search/): Similarity search utilizes vector-based methods to find objects close in meaning or attributes (like texts, images, or music) in vector space, with Qdrant's `Query API` supporting various search types such as k-NN, filtering, recommendations, and hybrid queries. Key features include customizable metrics (e.g., cosine similarity), filtering by conditions like scores or payloads, efficient approximate and exact search options, and capabilities for handling both dense and sparse vectors, with batch search available for multiple queries in one request.
- [Explore](https://qdrant.tech/documentation/concepts/explore/): Qdrant offers advanced APIs for data exploration, including the **Recommendation API** and **Discovery API**, which support flexible search strategies like average vector, best score, and sum scores, allowing for recommendations based on positive/negative examples or dissimilarity to outliers. These tools, along with multi-vector support, batch processing, and inter-collection lookup, enable powerful use cases such as recommendations, data cleaning, and embedding-agnostic searches optimized by strategies tailored to specific needs and performance trade-offs.
- [Hybrid Queries #required](https://qdrant.tech/documentation/concepts/hybrid-queries/): Qdrant's `Query API` enables advanced search techniques like hybrid and multi-stage queries by combining multiple queries or stages for more precise results. Key features include prefetching for nested queries, result fusion methods (`rrf` and `dbsf`), multi-stage re-scoring, diversity-focused Maximal Marginal Relevance (MMR), and customizable score boosting to incorporate business logic.
- [Filtering](https://qdrant.tech/documentation/concepts/filtering/): Qdrant enables advanced filtering for vector search by allowing the use of logical clauses like `AND`, `OR`, and `NOT`, alongside conditions such as `Match`, `Match Any`, `Match Except`, and nested filters, to refine searches based on payload attributes or point IDs. These filters facilitate complex queries addressing both business-specific requirements, like availability or pricing, and structured JSON data, such as nested fields or arrays.
- [Optimizer](https://qdrant.tech/documentation/concepts/optimizer/): Qdrant employs three key optimizers to enhance data storage and retrieval efficiency: Vacuum Optimizer removes accumulated deleted records to free memory and maintain performance, Merge Optimizer consolidates small segments to improve search efficiency while adhering to size thresholds, and Indexing Optimizer dynamically enables indexes and memmap storage based on data size, optimizing resource usage. All optimizer behaviors are configurable through a configuration file or dynamically adjustable at the collection level.
- [Storage](https://qdrant.tech/documentation/concepts/storage/): Qdrant stores data in segments, which handle vector and payload storage along with indexing, and can be either appendable (supporting additions, deletions, and queries) or non-appendable (read-only). Storage options include in-memory (high speed but RAM-intensive) and memmap (disk-based with flexible memory use), with configurable thresholds for optimization, while payload storage can be in-memory for speed or on-disk for lower RAM usage, supported by indexing to reduce latency during queries.
- [Indexing](https://qdrant.tech/documentation/concepts/indexing/): Qdrant combines vector and traditional indexes to optimize vector search with filters. It offers payload indexes for various data types, parameterized and on-disk options for performance and memory efficiency, and specialized indexing (tenant and principal indexes) to enhance multi-tenant and time-based searches.
- [Snapshots](https://qdrant.tech/documentation/concepts/snapshots/): Snapshots in Qdrant are `tar` archive files capturing the data and configuration of a specific collection on a specific node, requiring separate snapshots for each node in a distributed setup. They facilitate data archiving, replication, and recovery, with multiple restoration methods available, such as via URL, file upload, or start-up parameter, and support full storage snapshots for single-node deployments.
- [Local Quickstart](https://qdrant.tech/documentation/quickstart/): To get started with Qdrant locally, install and run its Docker container, then use the supported client libraries like Python, JavaScript, or others to initialize a connection, create a collection, and add vectors with associated payloads. Ensure security measures are in place as Qdrant runs without encryption or authentication by default.
- [Distance-based data exploration](https://qdrant.tech/articles/distance-based-exploration/): To uncover hidden structures in unstructured high-dimensional datasets, tools like Qdrant’s Distance Matrix API simplify distance calculations, which can then be used with methods such as UMAP for dimensionality reduction or KMeans for clustering. These approaches enable visualizing data relationships and patterns effectively, supporting deeper data exploration without requiring computationally intensive operations.
- [Modern Sparse Neural Retrieval: From Theory to Practice](https://qdrant.tech/articles/modern-sparse-neural-retrieval/): The article explores modern sparse neural retrieval methods that aim to combine the precision of keyword-based approaches like BM25 with the semantic understanding of dense retrievers. It compares key models like DeepCT, DeepImpact, and TILDEv2, detailing their evolution, strengths, and limitations, while providing practical guidance on using SPLADE++ in Qdrant for optimized retrieval solutions.
- [Qdrant Summer of Code 2024 - ONNX Cross Encoders in Python](https://qdrant.tech/articles/cross-encoder-integration-gsoc/): Huong (Celine) Hoang's Summer of Code 2024 internship at Qdrant focused on integrating cross-encoders into the lightweight FastEmbed library to enhance re-ranking capabilities for more context-aware search applications. Through overcoming challenges like tokenization, ONNX model integration, and testing, she successfully expanded FastEmbed's functionality while ensuring user-friendliness and scalability, with future improvements aimed at model support and optimization.
- [An Introduction to Vector Databases](https://qdrant.tech/articles/what-is-a-vector-database/): A **vector database** is a specialized system designed to manage unstructured data by converting it into vectors—numerical representations that capture context and semantics—enabling advanced similarity searches and analysis that traditional databases cannot perform. Unlike conventional structured databases, vector databases excel in applications such as recommendation systems, anomaly detection, and semantic search, leveraging metadata and filtering for more refined results.
- [What is Vector Quantization?](https://qdrant.tech/articles/what-is-vector-quantization/): Vector quantization is a data compression technique that reduces memory usage for high-dimensional data while retaining essential information, enabling faster and more efficient storage and search operations, particularly in large datasets. Methods like Scalar and Binary Quantization significantly shrink vector sizes (e.g., Binary Quantization reduces memory by 32x), with Binary Quantization providing the most substantial speed gains for distance computations using optimized binary operations like XOR and Popcount.
- [Vector Search Resource Optimization Guide](https://qdrant.tech/articles/vector-search-resource-optimization/): This guide provides strategies for optimizing vector database performance and resource efficiency, focusing on techniques like indexing (using HNSW), parameter tuning for balancing speed and accuracy, and data compression via quantization (e.g., reducing 32-bit float vectors to 8-bit representations). These methods aim to help users scale applications cost-effectively while tailoring performance to specific use cases.
- [A Complete Guide to Filtering in Vector Search](https://qdrant.tech/articles/vector-search-filtering/): Qdrant's filtered vector search combines semantic similarity and metadata constraints to improve search precision and efficiency, particularly in applications like e-commerce. By using methods like pre-filtering, post-filtering, and Qdrant's novel filterable HNSW index, it addresses inefficiencies in traditional search approaches, balancing accuracy and computational resources for datasets of varying sizes and metadata cardinality.
- [Qdrant Internals: Immutable Data Structures](https://qdrant.tech/articles/immutable-data-structures/): The article discusses the trade-offs and performance considerations of different data structures based on hardware efficiency, highlighting that no single structure is universally optimal. It emphasizes the benefits of immutability, such as optimized memory allocation and faster access, and introduces advanced techniques like minimal perfect hash functions to reduce hash collisions and improve performance in real-world systems like vector databases (e.g., Qdrant).
- [miniCOIL: on the Road to Usable Sparse Neural Retrieval](https://qdrant.tech/articles/minicoil/): Sparse neural retrieval bridges the gap between term-based and dense retrieval by combining lightweight, explainable methods like BM25 with the ability to capture word meanings, but prior models faced challenges in domain adaptability and efficiency. To address this, Qdrant developed **miniCOIL**, a sparse neural retriever inspired by Contextualized Inverted Lists (COIL), designed to produce lightweight, semantically-aware representations while maintaining robust out-of-domain performance.
- [Relevance Feedback in Informational Retrieval](https://qdrant.tech/articles/search-feedback-loop/): A well-defined search query significantly aids information retrieval, but users often struggle to articulate precise queries, making relevance feedback a critical tool for refining results iteratively. Despite decades of research, integrating relevance feedback into modern vector search systems remains underutilized due to challenges like user reluctance, computational costs, and the early-stage development of neural search techniques, prompting a need to explore approaches like query and scoring refinement for better adoption of these methods.
- [Built for Vector Search](https://qdrant.tech/articles/dedicated-vector-search/): Vector databases are specialized tools designed for efficient high-dimensional data searches, where general-purpose databases struggle with scalability and performance due to vectors' heavy storage and computational demands. With BASE-oriented architectures prioritizing availability and scalability, vector databases leverage fixed-size embeddings for fast indexing and retrieval, making them ideal for applications requiring high-throughput, low-latency search, unlike ACID-compliant systems built for strict consistency.
- [Any* Embedding Model Can Become a Late Interaction Model... If You Give It a Chance!](https://qdrant.tech/articles/late-interaction-models/): Qdrant 1.10's multi-vector support allows traditional dense embedding models to be adapted for late interaction retrieval by leveraging output token embeddings, achieving competitive or superior performance compared to specialized models like ColBERT while maintaining efficiency. Experimental results show that using output token embeddings enhances retrieval quality across multiple datasets, suggesting significant optimization potential for advanced search systems.
- [Optimizing Memory for Bulk Uploads](https://qdrant.tech/articles/indexing-optimization/): Efficient memory management during bulk ingestion in Qdrant can be achieved by temporarily disabling dense vector indexing (`m=0`), deferring HNSW graph construction, and enabling on-disk storage (`on_disk: true`) for vectors and indexes to reduce RAM usage. These practices help avoid out-of-memory errors and maintain performance, particularly in large-scale datasets or memory-constrained environments.
- [Introducing Gridstore: Qdrant's Custom Key-Value Store](https://qdrant.tech/articles/gridstore-key-value-storage/): Qdrant built its own storage engine, Gridstore, after encountering limitations with RocksDB, such as compaction-induced latency and configuration complexity, as well as a need for tailored features like efficient handling of sequential keys and variable-sized data. Gridstore's architecture consists of three layers—Data, Mask, and Gaps—that optimize fast retrieval, space reuse, and scalable updates, while ensuring data integrity through a Write-Ahead Log for crash recovery.
- [What is Agentic RAG? Building Agents with Qdrant](https://qdrant.tech/articles/agentic-rag/): Standard Retrieval-Augmented Generation (RAG) follows a linear flow of retrieving documents and generating responses, while Agentic RAG integrates agents, which use large language models (LLMs) to make decisions and take non-linear, multi-step actions, such as querying databases, improving queries, or self-correcting. While tools like LangGraph facilitate building such agentic systems by enabling cyclic workflows and decision-making, their practical applications often depend on use cases, with limitations like higher cost and latency compared to simpler RAG systems.
- [Hybrid Search Revamped - Building with Qdrant's Query API](https://qdrant.tech/articles/hybrid-search/): Qdrant 1.10 introduces a powerful new Query API that simplifies building hybrid search systems by enabling server-side integration of dense, sparse, and multivector search strategies, as well as supporting complex search pipelines and late interaction models like ColBERT for efficient reranking. Unlike simple linear combinations of scores, Qdrant employs advanced methods such as Reciprocal Rank Fusion and layered reranking to improve retrieval quality, emphasizing data-driven experimentation and effectiveness evaluation.
- [What is RAG: Understanding Retrieval-Augmented Generation](https://qdrant.tech/articles/what-is-rag-in-ai/): Retrieval-augmented generation (RAG) enhances Large Language Models (LLMs) by integrating external information retrieval, enabling them to access and generate responses based on relevant data from a vector database rather than relying solely on pre-trained knowledge. The RAG architecture includes a "retriever" using vector similarity techniques (dense or sparse) to identify relevant data and a "generator" to create accurate, non-hallucinated outputs, with hybrid search approaches often improving results further.
- [BM42: New Baseline for Hybrid Search](https://qdrant.tech/articles/bm42/): BM25 has remained a robust and widely used search algorithm due to its simplicity and focus on term importance within a corpus (via IDF), though its limitations in handling modern Retrieval-Augmented Generation (RAG) use cases with shorter, fixed-length documents have reduced its effectiveness. Despite alternatives like SPLADE showing promise, challenges such as inefficient token expansion, domain dependency, and high computational costs persist, motivating efforts like Qdrant's approach to combine BM25's interpretability with transformer-based semantic insights.
- [Qdrant 1.8.0: Enhanced Search Capabilities for Better Results](https://qdrant.tech/articles/qdrant-1.8.x/): Qdrant 1.8.0 introduces significant performance enhancements, including up to 16x faster hybrid search with sparse vectors, dynamic CPU resource allocation for optimized indexing, and improved text data indexing that reduces RAM usage by 10%. Additional features include ordering search results by metadata, datetime payload support, collection existence checks, and enhanced nested field modification capabilities, offering a more efficient and scalable search experience.
